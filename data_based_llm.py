"""
ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œ
"""
import json
import pandas as pd
import os
from typing import Dict, List, Any, Optional
from llm_manager import LLMManager
from naver_search import IssueResearchService
try:
    from enhanced_research_service import EnhancedResearchService
    ENHANCED_RESEARCH_AVAILABLE = True
except ImportError:
    ENHANCED_RESEARCH_AVAILABLE = False

try:
    from optimized_web_research import OptimizedWebResearchService
    OPTIMIZED_RESEARCH_AVAILABLE = True
except ImportError:
    OPTIMIZED_RESEARCH_AVAILABLE = False

try:
    from quality_enhancer import QualityEnhancer
    QUALITY_ENHANCER_AVAILABLE = True
except ImportError:
    QUALITY_ENHANCER_AVAILABLE = False

class DataBasedLLM:
    """ë°ì´í„° íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” LLM í´ë˜ìŠ¤"""
    
    def __init__(self, data_folder: str = "data", model: str = "gpt-4"):
        """
        ë°ì´í„° ê¸°ë°˜ LLM ì´ˆê¸°í™”
        
        Args:
            data_folder (str): ë°ì´í„° íŒŒì¼ë“¤ì´ ìœ„ì¹˜í•œ í´ë” ê²½ë¡œ
            model (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸
        """
        self.data_folder = data_folder
        self.llm = LLMManager(model, data_folder)  # data_folder ì „ë‹¬
        self.master_data = None
        self.media_response_data = None
        self.research_service = None
        
        # ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        try:
            self.research_service = IssueResearchService()
            print("INIT: ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"WARNING: ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.enhanced_research = None
        if ENHANCED_RESEARCH_AVAILABLE:
            try:
                self.enhanced_research = EnhancedResearchService()
                print("INIT: ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"WARNING: ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # ìµœì í™”ëœ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.optimized_research = None
        if OPTIMIZED_RESEARCH_AVAILABLE:
            try:
                self.optimized_research = OptimizedWebResearchService()
                print("INIT: ìµœì í™”ëœ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"WARNING: ìµœì í™”ëœ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # í’ˆì§ˆ ê°œì„  ëª¨ë“ˆ ì´ˆê¸°í™”
        self.quality_enhancer = None
        if QUALITY_ENHANCER_AVAILABLE:
            try:
                self.quality_enhancer = QualityEnhancer()
                print("INIT: í’ˆì§ˆ ê°œì„  ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"WARNING: í’ˆì§ˆ ê°œì„  ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # ë°ì´í„° ë¡œë“œ
        self._load_data()
    
    def _load_data(self):
        """ë°ì´í„° íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        try:
            # master_data.json ë¡œë“œ
            master_path = os.path.join(self.data_folder, "master_data.json")
            if os.path.exists(master_path):
                with open(master_path, 'r', encoding='utf-8') as f:
                    self.master_data = json.load(f)
                print(f"LOAD: master_data.json ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âœ— master_data.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {master_path}")
            
            # ì–¸ë¡ ëŒ€ì‘ë‚´ì—­.csv ë¡œë“œ
            csv_path = os.path.join(self.data_folder, "ì–¸ë¡ ëŒ€ì‘ë‚´ì—­.csv")
            if os.path.exists(csv_path):
                self.media_response_data = pd.read_csv(csv_path, encoding='utf-8')
                print(f"LOAD: ì–¸ë¡ ëŒ€ì‘ë‚´ì—­.csv ë¡œë“œ ì™„ë£Œ ({len(self.media_response_data)}ê±´)")
            else:
                print(f"âœ— ì–¸ë¡ ëŒ€ì‘ë‚´ì—­.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
                
        except Exception as e:
            print(f"âœ— ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def get_relevant_departments(self, query: str, use_ai_enhancement: bool = True) -> List[Dict]:
        """ê°œì„ ëœ AI ê¸°ë°˜ ë¶€ì„œ-ì´ìŠˆ ë§¤í•‘ ì‹œìŠ¤í…œ"""
        if not self.master_data or 'departments' not in self.master_data:
            return []
        
        # 1ë‹¨ê³„: ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­
        relevant_depts = self._basic_department_matching(query)
        
        # 2ë‹¨ê³„: AI ê¸°ë°˜ ì‹œë§¨í‹± ë§¤ì¹­ (ì„ íƒì )
        if use_ai_enhancement and len(relevant_depts) <= 1:
            ai_enhanced_depts = self._ai_enhanced_department_matching(query)
            # AI ê²°ê³¼ì™€ ê¸°ë³¸ ê²°ê³¼ í†µí•©
            relevant_depts = self._merge_department_results(relevant_depts, ai_enhanced_depts)
        
        # 3ë‹¨ê³„: ìµœì¢… ì •ë ¬ ë° í•„í„°ë§
        final_depts = self._finalize_department_selection(relevant_depts, query)
        
        return final_depts[:3]  # ìƒìœ„ 3ê°œ ë¶€ì„œ ë°˜í™˜
    
    def _basic_department_matching(self, query: str) -> List[Dict]:
        """1ë‹¨ê³„: ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶€ì„œ ë§¤ì¹­ (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)"""
        relevant_depts = []
        query_lower = query.lower()
        
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬ - ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        key_terms = self._extract_key_terms(query_lower)
        
        for dept_name, dept_info in self.master_data['departments'].items():
            if not dept_info.get('í™œì„±ìƒíƒœ', True):
                continue
                
            relevance_score = 0
            matched_items = []
            
            # 1) ë¶€ì„œëª… ì§ì ‘ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 8)
            if dept_name.lower() in query_lower:
                relevance_score += 8
                matched_items.append(f"ë¶€ì„œëª…:{dept_name}")
            
            # 2) ë‹´ë‹¹ì´ìŠˆ ì •ë°€ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 6)
            issues = dept_info.get('ë‹´ë‹¹ì´ìŠˆ', [])
            for issue in issues:
                issue_lower = issue.lower()
                # ì™„ì „ ë§¤ì¹­
                if issue_lower in query_lower:
                    relevance_score += 6
                    matched_items.append(f"ì´ìŠˆ:{issue}")
                # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œ í¬í•¨)
                elif any(term in issue_lower for term in key_terms):
                    relevance_score += 3
                    matched_items.append(f"ì´ìŠˆ(ë¶€ë¶„):{issue}")
            
            # 3) í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 4)
            keywords = dept_info.get('í‚¤ì›Œë“œ', '').split(', ')
            for keyword in keywords:
                if keyword.strip():
                    keyword_lower = keyword.strip().lower()
                    if keyword_lower in query_lower:
                        relevance_score += 4
                        matched_items.append(f"í‚¤ì›Œë“œ:{keyword}")
                    elif any(term in keyword_lower for term in key_terms):
                        relevance_score += 2
                        matched_items.append(f"í‚¤ì›Œë“œ(ë¶€ë¶„):{keyword}")
            
            # 4) ì‹œë§¨í‹± ê´€ë ¨ì„± ë³´ë„ˆìŠ¤
            semantic_bonus = self._calculate_semantic_bonus(query_lower, dept_info)
            relevance_score += semantic_bonus
            
            if relevance_score > 0:
                dept_info_copy = dept_info.copy()
                dept_info_copy['ë¶€ì„œëª…'] = dept_name
                dept_info_copy['ê´€ë ¨ì„±ì ìˆ˜'] = relevance_score
                dept_info_copy['ë§¤ì¹­í•­ëª©'] = matched_items
                relevant_depts.append(dept_info_copy)
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        relevant_depts.sort(key=lambda x: (-x['ê´€ë ¨ì„±ì ìˆ˜'], x.get('ìš°ì„ ìˆœìœ„', 999)))
        return relevant_depts
    
    def _extract_key_terms(self, query_lower: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = ['ì˜', 'ì—', 'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ê³¼', 'ì™€', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ê´€ë ¨', 'ëŒ€í•œ', 'ì—ê²Œ', 'í•œí…Œ']
        
        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´ ì •ì˜
        important_patterns = [
            '2ì°¨ì „ì§€', 'ë°°í„°ë¦¬', 'ë¦¬íŠ¬', 'ì „ê¸°ì°¨', 'esg', 'í™˜ê²½', 'ë¯¸ì–€ë§ˆ', 'ê°€ìŠ¤ì „', 'lng', 'í„°ë¯¸ë„',
            'íˆ¬ì', 'ì‹¤ì ', 'ì£¼ê°€', 'ë°°ë‹¹', 'í™ë³´', 'ir', 'ì¸ì‚¬', 'ì±„ìš©', 'ë…¸ì‚¬', 'ì „ëµ', 'ceo', 'ê²½ì˜ì§„',
            'ì†Œì¬', 'ë°”ì´ì˜¤', 'ê³¡ë¬¼', 'íŒœìœ ', 'ì •ì±…', 'ê·œì œ', 'ì •ë¶€', 'ì—ë„ˆì§€', 'ì‹ ì¬ìƒ', 'ì„íƒ„', 'ë°œì „',
            'ëª¨í„°ì½”ì–´', 'ìë™ì°¨', 'ë¯¸ë˜ì°¨', 'ì†”ë£¨ì…˜', 'ëª¨ë¹Œë¦¬í‹°', 'ë²•ì¸', 'ì§€ì‚¬', 'í•´ì™¸', 'í„°í‚¤'
        ]
        
        # ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” íŒ¨í„´ ì¶”ì¶œ
        key_terms = []
        for pattern in important_patterns:
            if pattern in query_lower:
                key_terms.append(pattern)
        
        # ì¼ë°˜ì ì¸ ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        words = query_lower.split()
        for word in words:
            if len(word) >= 2 and word not in stop_words:
                if word not in key_terms:  # ì¤‘ë³µ ì œê±°
                    key_terms.append(word)
        
        return key_terms[:5]  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
    
    def _calculate_semantic_bonus(self, query_lower: str, dept_info: Dict) -> float:
        """ë¶€ì„œì™€ ì¿¼ë¦¬ ê°„ ì‹œë§¨í‹± ê´€ë ¨ì„± ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        bonus = 0
        
        # ë¶€ì„œë³„ íŠ¹í™” ë„ë©”ì¸ ì •ì˜
        dept_domains = {
            'IRê·¸ë£¹': ['ì£¼ì‹', 'íˆ¬ìì', 'ì¬ë¬´', 'ê³µì‹œ', 'ë°°ë‹¹', 'ìˆ˜ìµ', 'ì£¼ì£¼', 'í€ë”ë©˜í„¸'],
            'ì—ë„ˆì§€ì •ì±…ê·¸ë£¹': ['ì „ë ¥', 'ê°€ìŠ¤', 'ì„ìœ ', 'ì¬ìƒì—ë„ˆì§€', 'ì •ì±…', 'ê·œì œ'],
            'ê°€ìŠ¤ì‚¬ì—…ìš´ì˜ì„¹ì…˜': ['íƒì‚¬', 'ìƒì‚°', 'ê´‘êµ¬', 'e&p', 'ì„ìœ ê°€ìŠ¤'],
            'ì†Œì¬ë°”ì´ì˜¤ì‚¬ì—…ìš´ì˜ì„¹ì…˜': ['ì‹ ì†Œì¬', 'ì²¨ë‹¨ì†Œì¬', 'ë°”ì´ì˜¤ë§¤ìŠ¤', 'ì‹í’ˆ', 'ë†ì—…'],
            'ì§€ì†ê°€ëŠ¥ê²½ì˜ê·¸ë£¹': ['ì§€ì†ê°€ëŠ¥ì„±', 'íƒ„ì†Œì¤‘ë¦½', 'ì¹œí™˜ê²½', 'ì‚¬íšŒì±…ì„'],
            'í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜': ['ìë™ì°¨ì‚°ì—…', 'ë¶€í’ˆ', 'ì™„ì„±ì°¨', 'oem']
        }
        
        dept_name = dept_info.get('ë¶€ì„œëª…', '')
        if dept_name in dept_domains:
            domain_keywords = dept_domains[dept_name]
            for keyword in domain_keywords:
                if keyword in query_lower:
                    bonus += 1.5
        
        return bonus
    
    def _ai_enhanced_department_matching(self, query: str) -> List[Dict]:
        """2ë‹¨ê³„: AI ê¸°ë°˜ ì‹œë§¨í‹± ë¶€ì„œ ë§¤í•‘"""
        
        # ë¶€ì„œ ì •ë³´ë¥¼ AIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ êµ¬ì„±
        dept_descriptions = []
        for dept_name, dept_info in self.master_data['departments'].items():
            if not dept_info.get('í™œì„±ìƒíƒœ', True):
                continue
                
            description = f"""
ë¶€ì„œ: {dept_name}
ë‹´ë‹¹ì: {dept_info.get('ë‹´ë‹¹ì', 'N/A')}
ë‹´ë‹¹ì´ìŠˆ: {', '.join(dept_info.get('ë‹´ë‹¹ì´ìŠˆ', []))}
í‚¤ì›Œë“œ: {dept_info.get('í‚¤ì›Œë“œ', 'N/A')}
ìš°ì„ ìˆœìœ„: {dept_info.get('ìš°ì„ ìˆœìœ„', 'N/A')}
            """.strip()
            dept_descriptions.append(description)
        
        # AI ê¸°ë°˜ ë¶€ì„œ ì¶”ì²œ í”„ë¡¬í”„íŠ¸
        ai_prompt = f"""
        ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ ë‹´ë‹¹ ë¶€ì„œ 2-3ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”:
        
        ì´ìŠˆ: {query}
        
        === ë¶€ì„œ ì •ë³´ ===
        {chr(10).join(dept_descriptions)}
        
        ì¶”ì²œ ê¸°ì¤€:
        1. ë‹´ë‹¹ ì´ìŠˆì™€ì˜ ì§ì ‘ì  ì—°ê´€ì„±
        2. ë¶€ì„œì˜ ì „ë¬¸ì„±ê³¼ ì±…ì„ ë²”ìœ„  
        3. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ì²˜ë¦¬ ê²½í—˜
        
        ì‘ë‹µ í˜•ì‹: ë¶€ì„œëª…1, ë¶€ì„œëª…2, ë¶€ì„œëª…3 (ì¶”ì²œ ìˆœì„œëŒ€ë¡œ)
        ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ê³  ë¶€ì„œëª…ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        try:
            ai_response = self.llm.chat(
                ai_prompt,
                system_prompt="ë‹¹ì‹ ì€ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ì˜ ì¡°ì§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ìŠˆì˜ ì„±ê²©ì„ íŒŒì•…í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë‹´ë‹¹ ë¶€ì„œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
                temperature=0.2
            )
            
            # AI ì‘ë‹µì—ì„œ ë¶€ì„œëª… ì¶”ì¶œ
            recommended_depts = self._extract_departments_from_ai_response(ai_response)
            return recommended_depts
            
        except Exception as e:
            print(f"AI ê¸°ë°˜ ë¶€ì„œ ë§¤í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _extract_departments_from_ai_response(self, response: str) -> List[Dict]:
        """AI ì‘ë‹µì—ì„œ ë¶€ì„œëª… ì¶”ì¶œí•˜ì—¬ ë¶€ì„œ ì •ë³´ ë°˜í™˜"""
        recommended_depts = []
        
        if not response:
            return recommended_depts
        
        # ì‘ë‹µì—ì„œ ë¶€ì„œëª… ì¶”ì¶œ
        dept_names = [name.strip() for name in response.split(',')]
        
        for i, dept_name in enumerate(dept_names[:3]):  # ìµœëŒ€ 3ê°œ
            if dept_name in self.master_data['departments']:
                dept_info = self.master_data['departments'][dept_name].copy()
                dept_info['ë¶€ì„œëª…'] = dept_name
                dept_info['ê´€ë ¨ì„±ì ìˆ˜'] = 10 - i * 2  # AI ì¶”ì²œ ìˆœì„œì— ë”°ë¥¸ ì ìˆ˜
                dept_info['ë§¤ì¹­í•­ëª©'] = ['AIì¶”ì²œ']
                recommended_depts.append(dept_info)
        
        return recommended_depts
    
    def _merge_department_results(self, basic_results: List[Dict], ai_results: List[Dict]) -> List[Dict]:
        """ê¸°ë³¸ ë§¤ì¹­ê³¼ AI ë§¤ì¹­ ê²°ê³¼ í†µí•©"""
        merged = {}
        
        # ê¸°ë³¸ ê²°ê³¼ ì¶”ê°€
        for dept in basic_results:
            dept_name = dept['ë¶€ì„œëª…']
            merged[dept_name] = dept
        
        # AI ê²°ê³¼ í†µí•© (ê¸°ì¡´ ë¶€ì„œëŠ” ì ìˆ˜ ë³´ì •, ìƒˆ ë¶€ì„œëŠ” ì¶”ê°€)
        for dept in ai_results:
            dept_name = dept['ë¶€ì„œëª…']
            if dept_name in merged:
                # ê¸°ì¡´ ë¶€ì„œì˜ ì ìˆ˜ ë³´ì • (AI ì¶”ì²œ ë³´ë„ˆìŠ¤)
                merged[dept_name]['ê´€ë ¨ì„±ì ìˆ˜'] += 3
                merged[dept_name]['ë§¤ì¹­í•­ëª©'].append('AIì¶”ì²œ')
            else:
                # ìƒˆ ë¶€ì„œ ì¶”ê°€
                merged[dept_name] = dept
        
        return list(merged.values())
    
    def _finalize_department_selection(self, departments: List[Dict], query: str) -> List[Dict]:
        """ìµœì¢… ë¶€ì„œ ì„ ì • ë° ì •ë ¬"""
        
        if not departments:
            # ê¸°ë³¸ ë¶€ì„œ ì œê³µ
            default_depts = ['IRê·¸ë£¹', 'í™ë³´ê·¸ë£¹', 'ê²½ì˜ì „ëµê·¸ë£¹']
            for dept_name in default_depts:
                if dept_name in self.master_data['departments']:
                    dept_info = self.master_data['departments'][dept_name].copy()
                    dept_info['ë¶€ì„œëª…'] = dept_name
                    dept_info['ê´€ë ¨ì„±ì ìˆ˜'] = 1
                    dept_info['ë§¤ì¹­í•­ëª©'] = ['ê¸°ë³¸ë¶€ì„œ']
                    departments.append(dept_info)
                    break
        
        # ìµœì¢… ì •ë ¬: ê´€ë ¨ì„±ì ìˆ˜ > ìš°ì„ ìˆœìœ„ > ë¶€ì„œëª…
        departments.sort(key=lambda x: (
            -x['ê´€ë ¨ì„±ì ìˆ˜'], 
            x.get('ìš°ì„ ìˆœìœ„', 999),
            x['ë¶€ì„œëª…']
        ))
        
        return departments
    
    def search_media_responses(self, query: str, limit: int = 10) -> pd.DataFrame:
        """ì–¸ë¡ ëŒ€ì‘ë‚´ì—­ì—ì„œ ê´€ë ¨ ì¼€ì´ìŠ¤ ê²€ìƒ‰"""
        if self.media_response_data is None:
            return pd.DataFrame()
        
        query_lower = query.lower()
        
        # ì´ìŠˆ ë°œìƒ ë³´ê³  ì»¬ëŸ¼ì—ì„œ ê²€ìƒ‰
        mask = self.media_response_data['ì´ìŠˆ ë°œìƒ ë³´ê³ '].str.lower().str.contains(
            query_lower, na=False, regex=False
        )
        
        relevant_cases = self.media_response_data[mask].head(limit)
        
        return relevant_cases
    
    def get_recent_cases_by_type(self, case_type: str = None, limit: int = 5) -> pd.DataFrame:
        """ìµœê·¼ ì‚¬ë¡€ë“¤ì„ ìœ í˜•ë³„ë¡œ ê²€ìƒ‰"""
        if self.media_response_data is None:
            return pd.DataFrame()
        
        df = self.media_response_data.copy()
        
        if case_type:
            df = df[df['ë°œìƒ ìœ í˜•'] == case_type]
        
        # ìµœê·¼ ë°ì´í„°ë¶€í„° ì •ë ¬ (ë°œìƒ ì¼ì‹œ ê¸°ì¤€)
        df = df.sort_values('ë°œìƒ ì¼ì‹œ', ascending=False)
        
        return df.head(limit)
    
    def generate_data_based_response(self, query: str) -> str:
        """ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        # 1. ê´€ë ¨ ë¶€ì„œ ì •ë³´ ìˆ˜ì§‘
        relevant_depts = self.get_relevant_departments(query)
        
        # 2. ê´€ë ¨ ì–¸ë¡ ëŒ€ì‘ ì‚¬ë¡€ ìˆ˜ì§‘
        relevant_cases = self.search_media_responses(query)
        
        # 3. ìµœê·¼ ì‚¬ë¡€ë“¤ ìˆ˜ì§‘
        recent_cases = self.get_recent_cases_by_type(limit=3)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(query, relevant_depts, relevant_cases, recent_cases)
        
        # 5. LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì´ê´„ í”„ë¡¬í”„íŠ¸ ìë™ ì‚¬ìš©)
        prompt = f"""
        ì§ˆë¬¸: {query}
        
        ê´€ë ¨ ë°ì´í„°:
        {context}
        
        ìœ„ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        
        ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:
        1. ê´€ë ¨ ë¶€ì„œ ì •ë³´ì™€ ê° ë¶€ì„œì˜ êµ¬ì²´ì  ì—­í•  ë° ì±…ì„
        2. ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ì˜ ëŒ€ì‘ ë°©ë²•ê³¼ ê²°ê³¼ ë¶„ì„
        3. ë‹¨ê³„ë³„ êµ¬ì²´ì ì¸ ì¡°ì¹˜ì‚¬í•­ê³¼ ì‹¤í–‰ ê³„íš
        4. ì˜ˆìƒë˜ëŠ” ë¦¬ìŠ¤í¬ì™€ êµ¬ì²´ì ì¸ ì™„í™” ë°©ì•ˆ
        5. ì¤‘ë¦½ì ì´ê³  ê°ê´€ì ì¸ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë°©í–¥ì„±
        6. ì‹¤í–‰ ê°€ëŠ¥í•œ íƒ€ì„ë¼ì¸ê³¼ ì„±ê³¼ ì¸¡ì • ë°©ë²•
        
        ë‹µë³€ì€ êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        """
        
        return self.llm.chat(prompt)  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œê±°
    
    def generate_enhanced_response_with_fact_check(self, query: str) -> str:
        """ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì‚¬ì‹¤ê²€ì¦ê³¼ ëŒ€ì‘ë°©ì•ˆ í¬í•¨ ë‹µë³€ ìƒì„±"""
        
        if not self.enhanced_research:
            print("WARNING: ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            return self.generate_data_based_response(query)
        
        print("START: ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì¢…í•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        try:
            # 1. ì¢…í•© ì´ìŠˆ ë¶„ì„ ìˆ˜í–‰
            comprehensive_analysis = self.enhanced_research.comprehensive_issue_analysis(query)
            
            # 2. ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ì •ë³´ë„ í•¨ê»˜ ìˆ˜ì§‘
            relevant_depts = self.get_relevant_departments(query)
            relevant_cases = self.search_media_responses(query)
            
            # 3. ê°•í™”ëœ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            enhanced_prompt = self._build_enhanced_response_prompt(
                query, comprehensive_analysis, relevant_depts, relevant_cases
            )
            
            # 4. LLMì„ í†µí•œ ìµœì¢… ë‹µë³€ ìƒì„±
            final_response = self.llm.chat(enhanced_prompt)
            
            return final_response
            
        except Exception as e:
            print(f"ê°•í™”ëœ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self.generate_data_based_response(query)
    
    def _build_enhanced_response_prompt(self, query: str, analysis: Dict, 
                                      relevant_depts: List[Dict], relevant_cases) -> str:
        """ê°•í™”ëœ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        fact_verification = analysis.get('fact_verification')
        response_strategy = analysis.get('response_strategy')
        research_data = analysis.get('research_data', {})
        
        # ì‚¬ì‹¤ê²€ì¦ ì •ë³´ ìš”ì•½
        fact_summary = f"""
=== ì‚¬ì‹¤ê²€ì¦ ê²°ê³¼ ===
í™•ì¸ ìƒíƒœ: {fact_verification.fact_status}
ì‹ ë¢°ë„: {fact_verification.confidence_score:.2f}
ê²€ì¦ ê·¼ê±°: {len(fact_verification.evidence_sources)}ê°œ ì†ŒìŠ¤
ëª¨ìˆœì‚¬í•­: {', '.join(fact_verification.contradictions) if fact_verification.contradictions else 'ì—†ìŒ'}
ì¶”ê°€ ê²€ì¦ì‚¬í•­: {fact_verification.verification_notes}
        """.strip()
        
        # ëŒ€ì‘ì „ëµ ìš”ì•½
        strategy_summary = f"""
=== ê¶Œê³  ëŒ€ì‘ì „ëµ ===
ìœ„ê¸°ìˆ˜ì¤€: {response_strategy.crisis_level}
ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í†¤: {response_strategy.communication_tone}

ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ (24ì‹œê°„ ë‚´):
{chr(10).join([f"â€¢ {action}" for action in response_strategy.immediate_actions[:5]])}

ë‹¨ê¸° ì „ëµ (1ì£¼ì¼ ë‚´):
{chr(10).join([f"â€¢ {action}" for action in response_strategy.short_term_strategy[:5]])}

ì¥ê¸° ì „ëµ (1ê°œì›” ë‚´):
{chr(10).join([f"â€¢ {action}" for action in response_strategy.long_term_strategy[:3]])}

í•µì‹¬ ì´í•´ê´€ê³„ì: {', '.join(response_strategy.stakeholders)}
ì„±ê³¼ ì¸¡ì •ì§€í‘œ: {', '.join(response_strategy.success_metrics)}
        """.strip()
        
        # ê´€ë ¨ ë¶€ì„œ ì •ë³´
        dept_info = ""
        if relevant_depts:
            dept_info = "=== ê´€ë ¨ ë¶€ì„œ ì •ë³´ ===\n"
            for dept in relevant_depts[:3]:
                dept_info += f"""
ë¶€ì„œ: {dept.get('ë¶€ì„œëª…', 'N/A')}
ë‹´ë‹¹ì: {dept.get('ë‹´ë‹¹ì', 'N/A')}
ì—°ë½ì²˜: {dept.get('ì—°ë½ì²˜', 'N/A')}
ì´ë©”ì¼: {dept.get('ì´ë©”ì¼', 'N/A')}
ë‹´ë‹¹ì˜ì—­: {', '.join(dept.get('ë‹´ë‹¹ì´ìŠˆ', []))}
                """.strip() + "\n"
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        web_research_summary = f"""
=== ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ===
ë‰´ìŠ¤ ê¸°ì‚¬: {len(research_data.get('news_results', []))}ê±´
ë¸”ë¡œê·¸ ê²Œì‹œë¬¼: {len(research_data.get('blog_results', []))}ê±´
ë³´ë„ ìˆ˜ì¤€: {research_data.get('analysis_summary', {}).get('coverage_level', 'N/A')}
        """.strip()
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        enhanced_prompt = f"""
        ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•´ ì‚¬ì‹¤ê²€ì¦ ê²°ê³¼ì™€ ì „ëµì  ëŒ€ì‘ë°©ì•ˆì„ í¬í•¨í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        ì§ˆë¬¸: {query}
        
        {fact_summary}
        
        {strategy_summary}
        
        {dept_info}
        
        {web_research_summary}
        
        ë‹¤ìŒ êµ¬ì¡°ë¡œ ì „ë¬¸ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        1. **ì´ìŠˆ ê°œìš” ë° í˜„í™©**
           - ì´ìŠˆì˜ ì •í™•í•œ ì •ì˜ì™€ í˜„ì¬ ìƒí™©
           - ì‚¬ì‹¤ê´€ê³„ í™•ì¸ ê²°ê³¼ ë° ì‹ ë¢°ë„
        
        2. **ì‚¬ì‹¤ ê²€ì¦ ë° ë¶„ì„**
           - í™•ì¸ëœ ì‚¬ì‹¤ê³¼ ë¯¸í™•ì¸ ì •ë³´ êµ¬ë¶„
           - ìƒì¶©ë˜ëŠ” ì •ë³´ë‚˜ ì£¼ì˜ì‚¬í•­
           - ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­
        
        3. **ê´€ë ¨ ë¶€ì„œ ë° ë‹´ë‹¹ì**
           - ì£¼ ë‹´ë‹¹ ë¶€ì„œì™€ ì—°ë½ì²˜
           - ê° ë¶€ì„œì˜ ì—­í• ê³¼ ì±…ì„
           - í˜‘ì—…ì´ í•„ìš”í•œ ë¶€ì„œë“¤
        
        4. **ìœ ê´€ ì˜ê²¬ (ì „ë¬¸ê°€ ê´€ì )**
           - ì—…ê³„ ì „ë¬¸ê°€ ì‹œê°
           - ìœ ì‚¬ ì‚¬ë¡€ ë²¤ì¹˜ë§ˆí‚¹
           - ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ ë¶„ì„
        
        5. **ë‹¨ê³„ë³„ ëŒ€ì‘ ë°©ì•ˆ**
           - ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ (24ì‹œê°„ ë‚´)
           - ë‹¨ê¸° ëŒ€ì‘ì „ëµ (1ì£¼ì¼ ë‚´)  
           - ì¥ê¸° ëŒ€ì‘ì „ëµ (1ê°œì›” ë‚´)
           - ê° ë‹¨ê³„ë³„ êµ¬ì²´ì  ì‹¤í–‰ê³„íš
        
        6. **ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ëµ**
           - ê¶Œê³  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í†¤ì•¤ë§¤ë„ˆ
           - í•µì‹¬ ë©”ì‹œì§€ í¬ì¸íŠ¸
           - ì´í•´ê´€ê³„ìë³„ ì†Œí†µ ë°©ì•ˆ
        
        7. **ëª¨ë‹ˆí„°ë§ ë° ì„±ê³¼ ì¸¡ì •**
           - ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ
           - ì„±ê³¼ ì¸¡ì • ë°©ë²•
           - ì˜ˆìƒ íƒ€ì„ë¼ì¸
        
        ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ê²½ì˜ì§„ì´ ì¦‰ì‹œ ì˜ì‚¬ê²°ì •í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        ì „ë¬¸ ìš©ì–´ëŠ” ì ì ˆíˆ ì‚¬ìš©í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
        """
        
        return enhanced_prompt
    
    def generate_optimized_response_with_fact_check(self, query: str, show_progress: bool = True) -> str:
        """ìµœì í™”ëœ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ê³ ì† ì‚¬ì‹¤ê²€ì¦ê³¼ ëŒ€ì‘ë°©ì•ˆ í¬í•¨ ë‹µë³€ ìƒì„±"""
        
        if not self.optimized_research:
            print("WARNING: ìµœì í™”ëœ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°•í™”ëœ ë²„ì „ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            return self.generate_enhanced_response_with_fact_check(query)
        
        # ì§„í–‰ë¥  ì½œë°± ì„¤ì •
        if show_progress:
            def progress_callback(step: str, progress: int):
                print(f"â© {step} ({progress}%)")
            self.optimized_research.set_progress_callback(progress_callback)
        
        print("START: ìµœì í™”ëœ ê³ ì† ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        try:
            import time
            start_time = time.time()
            
            # 1. ìµœì í™”ëœ ì¢…í•© ì´ìŠˆ ë¶„ì„ ìˆ˜í–‰
            comprehensive_analysis = self.optimized_research.optimized_comprehensive_analysis(query)
            
            # 2. ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ì •ë³´ë„ í•¨ê»˜ ìˆ˜ì§‘ (ìºì‹œëœ ë²„ì „)
            relevant_depts = self.get_relevant_departments(query)
            
            # 3. ìµœì í™”ëœ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            optimized_prompt = self._build_optimized_response_prompt(
                query, comprehensive_analysis, relevant_depts
            )
            
            # 4. LLMì„ í†µí•œ ìµœì¢… ë‹µë³€ ìƒì„± (ìºì‹œëœ ë²„ì „)
            final_response = self.llm.chat(optimized_prompt)
            
            processing_time = time.time() - start_time
            cache_status = comprehensive_analysis.get('cache_status', {})
            
            # ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
            performance_info = f"""
            
ğŸ“Š **ì„±ëŠ¥ ì •ë³´**
- ì´ ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ
- ë©”ëª¨ë¦¬ ìºì‹œ í•­ëª©: {cache_status.get('memory_cache_size', 0)}ê°œ
- ë¶„ì„ ì²˜ë¦¬ì‹œê°„: {comprehensive_analysis.get('processing_time', 0)}ì´ˆ
            """
            
            return final_response + performance_info
            
        except Exception as e:
            print(f"ìµœì í™”ëœ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self.generate_enhanced_response_with_fact_check(query)
    
    def _build_optimized_response_prompt(self, query: str, analysis: Dict, 
                                       relevant_depts: List[Dict]) -> str:
        """ìµœì í™”ëœ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        fact_verification = analysis.get('fact_verification', {})
        crisis_assessment = analysis.get('crisis_assessment', {})
        response_strategy = analysis.get('response_strategy', {})
        research_data = analysis.get('research_data', {})
        
        # ì‚¬ì‹¤ê²€ì¦ ì •ë³´ ìš”ì•½ (ê°„ì†Œí™”)
        fact_summary = f"""
=== ì‚¬ì‹¤ê²€ì¦ ê²°ê³¼ ===
í™•ì¸ ìƒíƒœ: {fact_verification.get('fact_status', 'ë¯¸í™•ì¸')}
ì‹ ë¢°ë„: {fact_verification.get('confidence_score', 0.5):.2f}
í•µì‹¬ ë°œê²¬: {', '.join(fact_verification.get('key_findings', ['ë¶„ì„ ì¤‘']))}
        """.strip()
        
        # ìœ„ê¸°í‰ê°€ ìš”ì•½
        crisis_summary = f"""
=== ìœ„ê¸°ìˆ˜ì¤€ í‰ê°€ ===
ìœ„ê¸°ìˆ˜ì¤€: {crisis_assessment.get('crisis_level', 'ì£¼ì˜')}
ê¸´ê¸‰ë„: {crisis_assessment.get('urgency_score', 5)}/10
ì˜í–¥í‰ê°€: {crisis_assessment.get('impact_assessment', 'ê²€í†  ì¤‘')}
        """.strip()
        
        # ëŒ€ì‘ì „ëµ ìš”ì•½ (ìµœì í™”)
        strategy_summary = f"""
=== ê¶Œê³  ëŒ€ì‘ì „ëµ ===
ì¦‰ì‹œ ì¡°ì¹˜: {', '.join(response_strategy.get('immediate_actions', ['ìƒí™© ëª¨ë‹ˆí„°ë§'])[:3])}
ë‹¨ê¸° ì „ëµ: {', '.join(response_strategy.get('short_term_strategy', ['ëŒ€ì‘ ê²€í† '])[:3])}
ì¥ê¸° ì „ëµ: {', '.join(response_strategy.get('long_term_strategy', ['ì§€ì† ê´€ì°°'])[:2])}
í•µì‹¬ ì´í•´ê´€ê³„ì: {', '.join(response_strategy.get('key_stakeholders', ['ë‚´ë¶€ ê´€ê³„ì']))}
ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í†¤: {response_strategy.get('communication_tone', 'ì‹ ì¤‘í•œ ëŒ€ì‘')}
        """.strip()
        
        # ê´€ë ¨ ë¶€ì„œ ì •ë³´ (ê°„ì†Œí™”)
        dept_info = ""
        if relevant_depts:
            dept_info = "=== ê´€ë ¨ ë¶€ì„œ ì •ë³´ ===\n"
            for dept in relevant_depts[:2]:  # ìµœëŒ€ 2ê°œ ë¶€ì„œë§Œ
                dept_info += f"{dept.get('ë¶€ì„œëª…', 'N/A')}: {dept.get('ë‹´ë‹¹ì', 'N/A')} ({dept.get('ì—°ë½ì²˜', 'N/A')})\n"
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (ê°„ì†Œí™”)
        web_summary = f"""
=== ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ===
ìˆ˜ì§‘ ì •ë³´: ë‰´ìŠ¤ {len(research_data.get('news_results', []))}ê±´, ë¸”ë¡œê·¸ {len(research_data.get('blog_results', []))}ê±´
ë³´ë„ ìˆ˜ì¤€: {research_data.get('analysis_summary', {}).get('coverage_level', 'N/A')}
        """.strip()
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ (ê°„ê²°í•˜ê³  ëª…í™•í•œ êµ¬ì¡°)
        optimized_prompt = f"""
        ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•´ ì‹ ì†í•˜ê³  ì •í™•í•œ ëŒ€ì‘ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”:
        
        ì§ˆë¬¸: {query}
        
        {fact_summary}
        
        {crisis_summary}
        
        {strategy_summary}
        
        {dept_info}
        
        {web_summary}
        
        ë‹¤ìŒ êµ¬ì¡°ë¡œ ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        ## 1. ğŸ“‹ ì´ìŠˆ ìš”ì•½
        - í˜„ì¬ ìƒí™© ë° ì‚¬ì‹¤ê´€ê³„
        - ìœ„ê¸°ìˆ˜ì¤€ê³¼ ê¸´ê¸‰ë„
        
        ## 2. ğŸ” ì‚¬ì‹¤ ê²€ì¦
        - í™•ì¸ëœ ì •ë³´ì™€ ë¯¸í™•ì¸ ì •ë³´
        - ì¶”ê°€ í™•ì¸ í•„ìš”ì‚¬í•­
        
        ## 3. ğŸ‘¥ ë‹´ë‹¹ ë¶€ì„œ
        - ì£¼ ë‹´ë‹¹ë¶€ì„œì™€ ì—°ë½ì²˜
        - í˜‘ì—… ë¶€ì„œ
        
        ## 4. ğŸ’­ ì „ë¬¸ê°€ ì˜ê²¬
        - ì—…ê³„ ê´€ì  ë° ìœ ì‚¬ì‚¬ë¡€
        - ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸
        
        ## 5. âš¡ ì¦‰ì‹œ ëŒ€ì‘ë°©ì•ˆ
        - 24ì‹œê°„ ë‚´ ì¡°ì¹˜ì‚¬í•­
        - ë‹´ë‹¹ì ë° ì‹¤í–‰ê³„íš
        
        ## 6. ğŸ“ˆ ë‹¨ê¸°/ì¥ê¸° ì „ëµ
        - 1ì£¼ì¼ ë‚´ ë‹¨ê¸°ì „ëµ
        - 1ê°œì›” ë‚´ ì¥ê¸°ì „ëµ
        
        ## 7. ğŸ“¢ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê°€ì´ë“œ
        - ê¶Œê³  ë©”ì‹œì§€ í†¤
        - ì´í•´ê´€ê³„ìë³„ ì†Œí†µë°©ì•ˆ
        
        ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±í•˜ë©°, í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ í¬í•¨í•´ì£¼ì„¸ìš”.
        """
        
        return optimized_prompt
    
    def clear_all_caches(self):
        """ëª¨ë“  ìºì‹œ ì´ˆê¸°í™”"""
        if self.optimized_research:
            self.optimized_research.clear_cache()
            print("ğŸ§¹ ìµœì í™” ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("WARNING: ìºì‹œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def _build_context(self, query: str, relevant_depts: List[Dict], 
                      relevant_cases: pd.DataFrame, recent_cases: pd.DataFrame) -> str:
        """ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        # ê´€ë ¨ ë¶€ì„œ ì •ë³´
        if relevant_depts:
            context_parts.append("=== ê´€ë ¨ ë¶€ì„œ ì •ë³´ ===")
            for dept in relevant_depts:
                dept_info = f"""
ë¶€ì„œ: {dept.get('ë¶€ì„œëª…', 'N/A')}
ë‹´ë‹¹ì: {dept.get('ë‹´ë‹¹ì', 'N/A')}
ì—°ë½ì²˜: {dept.get('ì—°ë½ì²˜', 'N/A')}
ì´ë©”ì¼: {dept.get('ì´ë©”ì¼', 'N/A')}
ë‹´ë‹¹ì´ìŠˆ: {', '.join(dept.get('ë‹´ë‹¹ì´ìŠˆ', []))}
ìš°ì„ ìˆœìœ„: {dept.get('ìš°ì„ ìˆœìœ„', 'N/A')}
                """.strip()
                context_parts.append(dept_info)
        
        # ê´€ë ¨ ì–¸ë¡ ëŒ€ì‘ ì‚¬ë¡€
        if not relevant_cases.empty:
            context_parts.append("\n=== ê´€ë ¨ ì–¸ë¡ ëŒ€ì‘ ì‚¬ë¡€ ===")
            for _, case in relevant_cases.iterrows():
                case_info = f"""
ì¼ì‹œ: {case.get('ë°œìƒ ì¼ì‹œ', 'N/A')}
ìœ í˜•: {case.get('ë°œìƒ ìœ í˜•', 'N/A')}
ë‹¨ê³„: {case.get('ë‹¨ê³„', 'N/A')}
ì´ìŠˆ: {case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ', 'N/A')}
ê²°ê³¼: {case.get('ëŒ€ì‘ ê²°ê³¼', 'N/A')}
                """.strip()
                context_parts.append(case_info)
        
        # ìµœê·¼ ì‚¬ë¡€ë“¤
        if not recent_cases.empty:
            context_parts.append("\n=== ìµœê·¼ ëŒ€ì‘ ì‚¬ë¡€ ===")
            for _, case in recent_cases.head(3).iterrows():
                case_info = f"""
ì¼ì‹œ: {case.get('ë°œìƒ ì¼ì‹œ', 'N/A')}
ìœ í˜•: {case.get('ë°œìƒ ìœ í˜•', 'N/A')}
ì´ìŠˆ: {case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ', 'N/A')}
                """.strip()
                context_parts.append(case_info)
        
        return "\n\n".join(context_parts)
    
    def analyze_issue_trends(self, days: int = 30) -> str:
        """ì´ìŠˆ íŠ¸ë Œë“œ ë¶„ì„"""
        if self.media_response_data is None:
            return "ë°ì´í„°ê°€ ì—†ì–´ íŠ¸ë Œë“œ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìµœê·¼ ë°ì´í„° í•„í„°ë§ (ê°„ë‹¨í•˜ê²Œ ìµœê·¼ Nê°œ ë ˆì½”ë“œë¡œ ëŒ€ì²´)
        recent_data = self.media_response_data.tail(days)
        
        # ìœ í˜•ë³„ ì§‘ê³„
        type_counts = recent_data['ë°œìƒ ìœ í˜•'].value_counts()
        stage_counts = recent_data['ë‹¨ê³„'].value_counts()
        
        trend_context = f"""
=== ìµœê·¼ {days}ê±´ ì´ìŠˆ íŠ¸ë Œë“œ ë¶„ì„ ===

ë°œìƒ ìœ í˜•ë³„ ë¶„í¬:
{type_counts.to_string()}

ë‹¨ê³„ë³„ ë¶„í¬:
{stage_counts.to_string()}

ì´ ê±´ìˆ˜: {len(recent_data)}ê±´
        """
        
        prompt = f"""
        ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœê·¼ ì´ìŠˆ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³  ì‹œì‚¬ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”:
        
        {trend_context}
        """
        
        return self.llm.chat(prompt, "ë‹¹ì‹ ì€ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° ì´ìŠˆ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")
    
    def recommend_response_strategy(self, issue_description: str) -> str:
        """ì´ìŠˆ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€ì‘ ì „ëµ ì¶”ì²œ"""
        
        # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
        similar_cases = self.search_media_responses(issue_description, limit=5)
        
        # ê´€ë ¨ ë¶€ì„œ ì°¾ê¸°
        relevant_depts = self.get_relevant_departments(issue_description)
        
        context = self._build_context(issue_description, relevant_depts, similar_cases, pd.DataFrame())
        
        prompt = f"""
        ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•œ ëŒ€ì‘ ì „ëµì„ ì¶”ì²œí•´ì£¼ì„¸ìš”:
        
        ì´ìŠˆ ì„¤ëª…: {issue_description}
        
        ì°¸ê³  ë°ì´í„°:
        {context}
        
        ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
        1. ì´ìŠˆì˜ ì‹¬ê°ë„ í‰ê°€ (1-5ì )
        2. ì£¼ê´€ ë¶€ì„œ ë° ë‹´ë‹¹ì ì—°ë½ì²˜
        3. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­
        4. ë‹¨ê³„ë³„ ëŒ€ì‘ ê³„íš
        5. ìœ ì‚¬ ì‚¬ë¡€ ëŒ€ì‘ ê²°ê³¼ ì°¸ì¡°
        6. ì˜ˆìƒ ë¦¬ìŠ¤í¬ ë° ì™„í™” ë°©ì•ˆ
        """
        
        return self.llm.chat(prompt)  # ì´ê´„ í”„ë¡¬í”„íŠ¸ ìë™ ì‚¬ìš©
    
    def generate_comprehensive_issue_report(self, media_name: str, reporter_name: str, issue_description: str, mode: str = "enhanced") -> str:
        """ì™„ì „í•œ 8ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ì´ìŠˆë°œìƒë³´ê³ ì„œ ìƒì„±"""
        
        # ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ
        if mode == "enhanced":
            return self._generate_enhanced_report(media_name, reporter_name, issue_description)
        
        print(f"START: ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ì´ìŠˆë°œìƒë³´ê³ ì„œ ìƒì„± ì‹œì‘: {media_name} / {reporter_name} (ëª¨ë“œ: {mode})")
        
        # 1. ì‚¬ìš©ì ì¸í’‹ ë°ì´í„° ê²€ì¦
        if not self._validate_inputs(media_name, reporter_name, issue_description):
            return "ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨"
        
        # 2. LLM ê¸°ë°˜ ì´ìŠˆ ì´ˆê¸° ë¶„ì„
        print("STEP 2: LLM ê¸°ë°˜ ì´ìŠˆ ì´ˆê¸° ë¶„ì„...")
        initial_analysis = self._analyze_issue_nature(issue_description)
        
        # 3. data í´ë” íŒŒì¼ ê¸°ë°˜ ìœ ê´€ë¶€ì„œ, ìœ„ê¸°ë‹¨ê³„ ì§€ì •
        print("STEP 3: ìœ ê´€ë¶€ì„œ ë° ìœ„ê¸°ë‹¨ê³„ ì§€ì •...")
        relevant_depts = self.get_relevant_departments_from_master_data(issue_description)
        crisis_level = self._assess_crisis_level_from_master_data(issue_description)
        media_info = self._get_media_info_from_master_data(media_name)
        
        # 4. Naver API ê¸°ë°˜ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        print("STEP 4: Naver API ì›¹ ê²€ìƒ‰ ìˆ˜í–‰...")
        web_search_results = self._conduct_web_research(issue_description, initial_analysis)
        
        # 5. ì·¨í•© ì •ë³´ ê¸°ë°˜ ë°°ê²½ì§€ì‹ ë° ì‚¬ì‹¤ í™•ì¸
        print("STEP 5: ë°°ê²½ì§€ì‹ ë° ì‚¬ì‹¤ í™•ì¸...")
        fact_verification = self._verify_facts_and_background(issue_description, web_search_results, initial_analysis)
        
        # 6. ìœ ê´€ë¶€ì„œ ì˜ê²¬ ê°€ì•ˆ ë„ì¶œ
        print("STEP 6: ìœ ê´€ë¶€ì„œ ì˜ê²¬ ê°€ì•ˆ ë„ì¶œ...")
        department_opinions = self._generate_department_opinions(relevant_depts, issue_description, web_search_results)
        
        # 7. ì–¸ë¡ í™ë³´ í˜ë¥´ì†Œë‚˜ ê´€ì  ëŒ€ì‘ë°©ì•ˆ ë§ˆë ¨
        print("STEP 7: ì–¸ë¡ í™ë³´ ì „ë¬¸ê°€ ëŒ€ì‘ë°©ì•ˆ ë§ˆë ¨...")
        pr_strategy = self._develop_pr_strategy(issue_description, crisis_level, fact_verification, department_opinions)
        
        # 8. ë³´ê³ ì„œ ê²°ê³¼ê°’ ìƒì„±
        print("STEP 8: ìµœì¢… ë³´ê³ ì„œ ìƒì„±...")
        final_report = self._generate_final_comprehensive_report(
            media_name=media_name,
            reporter_name=reporter_name,
            issue_description=issue_description,
            initial_analysis=initial_analysis,
            relevant_depts=relevant_depts,
            crisis_level=crisis_level,
            media_info=media_info,
            web_search_results=web_search_results,
            fact_verification=fact_verification,
            department_opinions=department_opinions,
            pr_strategy=pr_strategy
        )
        
        # 9. í’ˆì§ˆ ê°œì„  ì ìš© (ëª¨ë“  ëª¨ë“œì—ì„œ ì ìš©)
        if self.quality_enhancer:
            print("STEP 9: í’ˆì§ˆ ê°œì„  ì ìš©...")
            final_report = self.quality_enhancer.enhance_report_quality(
                final_report, issue_description, media_name, reporter_name
            )
            print("STEP 9 ì™„ë£Œ")
        
        print(f"COMPLETE: ì™„ì „í•œ 8ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ - ìœ„ê¸°ë‹¨ê³„: {crisis_level}, ê´€ë ¨ë¶€ì„œ: {len(relevant_depts)}ê°œ")
        return final_report
    
    def _generate_enhanced_report(self, media_name: str, reporter_name: str, issue_description: str) -> str:
        """ê°•í™”ëœ ì²˜ë¦¬ ëª¨ë“œ - ì„±ëŠ¥ ìµœì í™” ì ìš©"""
        import time
        
        start_time = time.time()
        print(f"START: ê°•í™”ëœ ëª¨ë“œ ì²˜ë¦¬ ì‹œì‘ - {media_name}")
        
        try:
            # 1. ì…ë ¥ ê²€ì¦ (ê¸°ì¡´ ë™ì¼)
            if not self._validate_inputs(media_name, reporter_name, issue_description):
                return "ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨"
            
            # 2. ìµœì í™”ëœ ì´ˆê¸° ë¶„ì„
            step_start = time.time()
            initial_analysis = self._optimized_initial_analysis(issue_description)
            print(f"  STEP 2 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            # 3. ë¹ ë¥¸ ë¶€ì„œ/ìœ„ê¸° ë§¤í•‘
            step_start = time.time()
            relevant_depts = self._fast_department_mapping(issue_description)
            crisis_level = self._fast_crisis_assessment(issue_description)
            media_info = self._get_media_info_from_master_data(media_name)
            print(f"  STEP 3 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            # 4-5. ì›¹ ê²€ìƒ‰ + ì‚¬ì‹¤ í™•ì¸ (ì¡°ê±´ë¶€ ì‹¤í–‰)
            step_start = time.time()
            if self._should_do_web_search(issue_description):
                web_search_results = self._conduct_web_research(issue_description, initial_analysis)
                fact_verification = self._verify_facts_and_background(issue_description, web_search_results, initial_analysis)
            else:
                web_search_results = {"search_summary": "ì›¹ ê²€ìƒ‰ ìƒëµ (ë¡œì»¬ ë¶„ì„ ì¶©ë¶„)"}
                fact_verification = self._local_fact_verification(issue_description, initial_analysis)
            print(f"  STEP 4-5 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            # 6. ê°„ì†Œí™”ëœ ë¶€ì„œ ì˜ê²¬
            step_start = time.time()
            department_opinions = self._simplified_department_opinions(relevant_depts, issue_description)
            print(f"  STEP 6 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            # 7. ê°„ì†Œí™”ëœ PR ì „ëµ
            step_start = time.time()
            pr_strategy = self._simplified_pr_strategy(issue_description, crisis_level, fact_verification)
            print(f"  STEP 7 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            # 8. ìµœì¢… ë³´ê³ ì„œ (êµ¬ì¡°í™”ëœ ë°©ì‹ ì‚¬ìš©)
            step_start = time.time()
            final_report = self._generate_final_comprehensive_report(
                media_name=media_name,
                reporter_name=reporter_name,
                issue_description=issue_description,
                initial_analysis=initial_analysis,
                relevant_depts=relevant_depts,
                crisis_level=crisis_level,
                media_info=media_info,
                web_search_results=web_search_results,
                fact_verification=fact_verification,
                department_opinions=department_opinions,
                pr_strategy=pr_strategy
            )
            print(f"  STEP 8 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            # 9. í’ˆì§ˆ ê°œì„  ì ìš© (Enhanced ëª¨ë“œì—ì„œë§Œ)
            if self.quality_enhancer:
                step_start = time.time()
                print("STEP 9: í’ˆì§ˆ ê°œì„  ì ìš©...")
                final_report = self.quality_enhancer.enhance_report_quality(
                    final_report, issue_description, media_name, reporter_name
                )
                print(f"  STEP 9 ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            
            total_time = time.time() - start_time
            print(f"COMPLETE: ê°•í™”ëœ ì²˜ë¦¬ ì™„ë£Œ ({total_time:.2f}ì´ˆ) - ìœ„ê¸°ë‹¨ê³„: {crisis_level}")
            
            return final_report
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"ERROR: ê°•í™”ëœ ì²˜ë¦¬ ì‹¤íŒ¨ ({error_time:.2f}ì´ˆ) - {str(e)}")
            
            # í´ë°±: ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì „í™˜
            print("FALLBACK: ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì „í™˜...")
            return self._generate_fallback_report(media_name, reporter_name, issue_description, str(e))
    
    def _optimized_initial_analysis(self, issue_description: str) -> dict:
        """ìµœì í™”ëœ ì´ˆê¸° ë¶„ì„ - ë” ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸"""
        
        prompt = f"""
ë‹¤ìŒ ì´ìŠˆë¥¼ ë¹ ë¥´ê²Œ ë¶„ì„í•˜ì„¸ìš”:

ì´ìŠˆ: {issue_description}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "category": "ì œí’ˆí’ˆì§ˆ/í™˜ê²½ì•ˆì „/ì¬ë¬´ì„±ê³¼/ì‚¬ì—…ìš´ì˜",
  "urgency": "ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ",
  "summary": "í•µì‹¬ ìš”ì•½ (30ì ì´ë‚´)"
}}
        """
        
        try:
            response = self.llm.chat(prompt)
            import json
            return json.loads(response)
        except:
            return {
                "category": "ì‚¬ì—…ìš´ì˜",
                "urgency": "ì¤‘ê°„", 
                "summary": issue_description[:30] + "..." if len(issue_description) > 30 else issue_description
            }
    
    def _fast_department_mapping(self, issue_description: str) -> list:
        """ë¹ ë¥¸ ë¶€ì„œ ë§¤í•‘ - í‚¤ì›Œë“œ ê¸°ë°˜"""
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ë§¤í•‘
        keyword_dept_map = {
            "ì² ê°•": [{"ë¶€ì„œëª…": "ì² ê°•ì‚¬ì—…ë¶€", "ë‹´ë‹¹ì": "ì² ê°•íŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5678"}],
            "ìì›": [{"ë¶€ì„œëª…": "ìì›ê°œë°œì‚¬ì—…ë¶€", "ë‹´ë‹¹ì": "ìì›íŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5679"}],
            "í™˜ê²½": [{"ë¶€ì„œëª…": "ESGê²½ì˜ì‹¤", "ë‹´ë‹¹ì": "ESGíŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5680"}],
            "í’ˆì§ˆ": [{"ë¶€ì„œëª…": "í’ˆì§ˆë³´ì¦íŒ€", "ë‹´ë‹¹ì": "í’ˆì§ˆíŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5681"}],
            "ì¬ë¬´": [{"ë¶€ì„œëª…": "ì¬ë¬´íŒ€", "ë‹´ë‹¹ì": "ì¬ë¬´íŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5682"}],
            "ê´‘ì‚°": [{"ë¶€ì„œëª…": "ìì›ê°œë°œì‚¬ì—…ë¶€", "ë‹´ë‹¹ì": "í•´ì™¸ì‚¬ì—…íŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5683"}]
        }
        
        issue_lower = issue_description.lower()
        
        for keyword, depts in keyword_dept_map.items():
            if keyword in issue_lower:
                return depts
        
        # ê¸°ë³¸ ë¶€ì„œ
        return [{"ë¶€ì„œëª…": "í™ë³´ê·¸ë£¹", "ë‹´ë‹¹ì": "í™ë³´íŒ€ì¥", "ì—°ë½ì²˜": "02-1234-5000"}]
    
    def _fast_crisis_assessment(self, issue_description: str) -> str:
        """ë¹ ë¥¸ ìœ„ê¸° ë‹¨ê³„ í‰ê°€ - í‚¤ì›Œë“œ ê¸°ë°˜"""
        
        high_risk = ["ì‚¬ê³ ", "ì¤‘ë‹¨", "íì‡„", "ì†Œì†¡", "í™˜ê²½ì˜¤ì—¼", "ëŒ€ê·œëª¨", "ê¸´ê¸‰"]
        medium_risk = ["ì§€ì—°", "ë¶ˆëŸ‰", "ë¬¸ì œ", "ìš°ë ¤", "ë…¼ë€", "ê²€í† "]
        
        issue_lower = issue_description.lower()
        
        if any(word in issue_lower for word in high_risk):
            return "3ë‹¨ê³„(ìœ„ê¸°)"
        elif any(word in issue_lower for word in medium_risk):
            return "2ë‹¨ê³„(ì£¼ì˜)"
        else:
            return "1ë‹¨ê³„(ê´€ì‹¬)"
    
    def _should_do_web_search(self, issue_description: str) -> bool:
        """ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        
        # ë³µì¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°ë§Œ ì›¹ ê²€ìƒ‰
        complex_keywords = ["í™˜ê²½", "ì†Œì†¡", "ì‚¬ê³ ", "êµ­ì œ", "ê·œì œ", "ì •ë¶€"]
        
        return any(keyword in issue_description.lower() for keyword in complex_keywords)
    
    def _local_fact_verification(self, issue_description: str, initial_analysis: dict) -> dict:
        """ë¡œì»¬ ê¸°ë°˜ ì‚¬ì‹¤ í™•ì¸ (ì›¹ ê²€ìƒ‰ ì—†ì´)"""
        
        return {
            "fact_status": "ë‚´ë¶€ê²€í† ì¤‘",
            "credibility": "ì¤‘ê°„",
            "background_context": "ê´€ë ¨ ë¶€ì„œì—ì„œ ì‚¬ì‹¤ ê´€ê³„ í™•ì¸ ì§„í–‰ ì¤‘",
            "cautions": ["ì •í™•í•œ ì •ë³´ í™•ì¸ í•„ìš”", "ì‹ ì¤‘í•œ ëŒ€ì‘ ìš”êµ¬"],
            "source_metadata": {
                "total_sources": 0,
                "official_sources_available": False,
                "credibility_level": "ë‚´ë¶€ ê²€í†  ë‹¨ê³„"
            }
        }
    
    def _simplified_department_opinions(self, relevant_depts: list, issue_description: str) -> dict:
        """ê°„ì†Œí™”ëœ ë¶€ì„œ ì˜ê²¬ ìƒì„±"""
        
        opinions = {}
        
        for dept in relevant_depts[:2]:  # ìƒìœ„ 2ê°œ ë¶€ì„œë§Œ
            dept_name = dept.get('ë¶€ì„œëª…', 'ë¯¸ìƒ')
            opinions[dept_name] = {
                "opinion": f"{dept_name}ì—ì„œ í•´ë‹¹ ì´ìŠˆì— ëŒ€í•´ ê²€í†  ì§„í–‰ ì¤‘",
                "action": "ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ ë° ëŒ€ì‘ ë°©ì•ˆ ê²€í† "
            }
        
        return opinions
    
    def _simplified_pr_strategy(self, issue_description: str, crisis_level: str, fact_verification: dict) -> dict:
        """ê°„ì†Œí™”ëœ PR ì „ëµ"""
        
        # ìœ„ê¸° ë‹¨ê³„ë³„ ê¸°ë³¸ ì „ëµ
        if "3ë‹¨ê³„" in crisis_level:
            tone = "ì ê·¹ì  íˆ¬ëª…ì„±"
            messages = ["ì‹ ì†í•œ ì‚¬ì‹¤ í™•ì¸", "íˆ¬ëª…í•œ ì •ë³´ ê³µê°œ", "ì±…ì„ê° ìˆëŠ” ëŒ€ì‘"]
            actions = ["ê¸´ê¸‰ ëŒ€ì‘íŒ€ êµ¬ì„±", "ì¦‰ì‹œ ì¡°ì‚¬ ì‹¤ì‹œ"]
        elif "2ë‹¨ê³„" in crisis_level:
            tone = "ì‹ ì¤‘í•œ íˆ¬ëª…ì„±"
            messages = ["ì •í™•í•œ ì‚¬ì‹¤ í™•ì¸", "ì„±ì‹¤í•œ ëŒ€ì‘", "ì§€ì†ì  ì†Œí†µ"]
            actions = ["ê´€ë ¨ ë¶€ì„œ í˜‘ì˜", "ì¶”ê°€ ì¡°ì‚¬ ì§„í–‰"]
        else:
            tone = "ì˜ˆë°©ì  ì†Œí†µ"
            messages = ["ì‚¬ì „ ì˜ˆë°©ì  ê´€ë¦¬", "íˆ¬ëª…í•œ ìš´ì˜", "ì§€ì†ì  ê°œì„ "]
            actions = ["ëª¨ë‹ˆí„°ë§ ê°•í™”", "ì˜ˆë°© ì¡°ì¹˜ ê²€í† "]
        
        return {
            "communication_tone": tone,
            "key_messages": messages,
            "immediate_actions": actions
        }
    
    def _generate_fallback_report(self, media_name: str, reporter_name: str, 
                                issue_description: str, error_msg: str) -> str:
        """ì—ëŸ¬ ì‹œ í´ë°± ë³´ê³ ì„œ"""
        
        current_time = self._get_current_time()
        
        return f"""<ì´ìŠˆ ë°œìƒ ë³´ê³ >

1. ë°œìƒ ì¼ì‹œ: {current_time}

2. ë°œìƒ ë‹¨ê³„: 2ë‹¨ê³„(ì£¼ì˜)

3. ë°œìƒ ë‚´ìš©:
({media_name} {reporter_name})
{issue_description}

4. ìœ ê´€ ì˜ê²¬:
- ì‚¬ì‹¤ í™•ì¸: ê´€ë ¨ ë¶€ì„œì—ì„œ ì‚¬ì‹¤ ê´€ê³„ í™•ì¸ ì§„í–‰ ì¤‘
- ì„¤ëª… ë…¼ë¦¬: ì •í™•í•œ ì •ë³´ íŒŒì•… í›„ íˆ¬ëª…í•˜ê³  ì„±ì‹¤í•œ ì†Œí†µ ì˜ˆì •
- ë©”ì‹œì§€ ë°©í–¥ì„±: ì‹ ì¤‘í•˜ê³  ì±…ì„ê° ìˆëŠ” ëŒ€ì‘

5. ëŒ€ì‘ ë°©ì•ˆ:
- ì›ë³´ì´ìŠ¤: 'ì •í™•í•œ ì‚¬ì‹¤ í™•ì¸ì„ í†µí•´ ì„±ì‹¤í•˜ê²Œ ëŒ€ì‘í•˜ê² ìŠµë‹ˆë‹¤'
- ì´í›„ ëŒ€ì‘ ë°©í–¥ì„±: 
  - ê´€ë ¨ ë¶€ì„œ ê¸´ê¸‰ íšŒì˜ ì†Œì§‘
  - ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„

6. ëŒ€ì‘ ê²°ê³¼: (ì¶”í›„ ì—…ë°ì´íŠ¸)

ì°¸ì¡°. ìµœê·¼ ìœ ì‚¬ ì‚¬ë¡€ (1ë…„ ì´ë‚´):
- ê´€ë ¨ ì‚¬ë¡€ ì¡°ì‚¬ ì¤‘

ì°¸ì¡°. ì´ìŠˆ ì •ì˜ ë° ê°œë… ì •ë¦½:
- ê°œë…: ê¸°ì—… ìš´ì˜ ê³¼ì •ì—ì„œ ë°œìƒí•œ ì´ìŠˆ ìƒí™©
- ê²½ì˜/ì‚¬íšŒ/ë²•ë¥ ì  í•¨ì˜: ê¸°ì—… ì‹ ë¢°ë„ ë° ìš´ì˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„ í•„ìš”

â€» ì£¼ì˜: ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘ ì¼ì‹œì  ë¬¸ì œë¡œ ê°„ì†Œí™”ëœ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ìƒì„¸ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì¬ì‹¤í–‰í•˜ì‹œê±°ë‚˜ í‘œì¤€ ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""
    
    def generate_issue_report(self, media_name: str, reporter_name: str, issue_description: str) -> str:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ - ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ í˜¸ì¶œ"""
        return self.generate_comprehensive_issue_report(media_name, reporter_name, issue_description)
    
    def _get_current_time(self) -> str:
        """í˜„ì¬ ì‹œê°„ì„ í•œêµ­ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
    
    def _format_department_opinions_for_report(self, relevant_depts: list, department_opinions: dict) -> str:
        """ë³´ê³ ì„œìš© ìœ ê´€ë¶€ì„œ ì˜ê²¬ í¬ë§·íŒ…"""
        if not relevant_depts:
            return "- ìœ ê´€ë¶€ì„œ: í™ë³´ê·¸ë£¹ (ê¸°ë³¸ ëŒ€ì‘)\n"
        
        formatted_opinions = []
        
        for i, dept in enumerate(relevant_depts[:3]):  # ìƒìœ„ 3ê°œ ë¶€ì„œë§Œ
            dept_name = dept.get('ë¶€ì„œëª…', '')
            dept_contact = dept.get('ë‹´ë‹¹ì', '')
            dept_phone = dept.get('ì—°ë½ì²˜', '')
            matching_score = dept.get('ë§¤ì¹­ì ìˆ˜', 0)
            matched_keywords = dept.get('ë§¤ì¹­í‚¤ì›Œë“œ', [])
            
            # ë¶€ì„œ ì •ë³´ ê¸°ë³¸ í¬ë§·
            dept_info = f"- {dept_name}"
            if dept_contact:
                dept_info += f" ({dept_contact})"
            if dept_phone:
                dept_info += f" [{dept_phone}]"
            
            # ë§¤ì¹­ ì •ë³´ ì¶”ê°€
            if matching_score > 0:
                dept_info += f" - ê´€ë ¨ë„: {matching_score}ì "
            
            if matched_keywords:
                keywords_str = ', '.join(matched_keywords[:3])  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ
                dept_info += f" (ë§¤ì¹­: {keywords_str})"
            
            # ë¶€ì„œë³„ ì˜ê²¬ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if dept_name in department_opinions:
                opinion = department_opinions[dept_name]
                if isinstance(opinion, dict):
                    opinion_text = opinion.get('summary', opinion.get('opinion', 'ì˜ê²¬ ìˆ˜ë ´ ì¤‘'))
                else:
                    opinion_text = str(opinion)[:100] + "..." if len(str(opinion)) > 100 else str(opinion)
                dept_info += f"\\n  â”” ë¶€ì„œ ì˜ê²¬: {opinion_text}"
            else:
                dept_info += "\\n  â”” ë¶€ì„œ ì˜ê²¬: ê²€í†  ì¤‘"
            
            formatted_opinions.append(dept_info)
        
        return "\\n".join(formatted_opinions) + "\\n"
    
    def get_relevant_departments_from_master_data(self, issue_description: str) -> list:
        """master_data.jsonì—ì„œ ì´ìŠˆ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶€ì„œ ë§¤í•‘ (ê°œì„ ëœ ë²„ì „)"""
        if not self.master_data:
            return []
        
        departments = self.master_data.get("departments", {})
        relevant_depts = []
        
        issue_lower = issue_description.lower()
        
        # ë™ì˜ì–´ ë§¤í•‘ í…Œì´ë¸”
        synonym_map = {
            "ì‹¤ì ": ["ìˆ˜ìµ", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ë¶„ê¸°ì‹¤ì ", "ì—°ê°„ì‹¤ì ", "ì¬ë¬´ì„±ê³¼"],
            "ê³µì‹œ": ["ë°œí‘œ", "ë³´ê³ ", "ê³µê°œ", "ë°œí‘œìë£Œ", "irìë£Œ"],
            "ë°°ë‹¹": ["ë°°ë‹¹ê¸ˆ", "ì£¼ì£¼ë°°ë‹¹", "ë°°ë‹¹ì •ì±…", "ë°°ë‹¹ìˆ˜ìµë¥ "],
            "ì£¼ê°€": ["ì£¼ì‹", "ì¦ê¶Œ", "ì‹œê°€ì´ì•¡", "ì£¼ì‹ê°€ê²©"],
            "ì—ë„ˆì§€": ["ì „ë ¥", "ë°œì „", "ì „ê¸°", "ì‹ ì¬ìƒ", "ì¬ìƒì—ë„ˆì§€", "ì¹œí™˜ê²½"],
            "lng": ["ì²œì—°ê°€ìŠ¤", "ì•¡í™”ì²œì—°ê°€ìŠ¤", "ê°€ìŠ¤"],
            "ì² ê°•": ["steel", "ì² ", "ê°•ì² ", "ì œì² ", "ì›ë£Œ", "ì½”í¬ìŠ¤", "ì² ê´‘ì„"],
            "ì„íƒ„": ["coal", "ìœ ì—°íƒ„", "ë¬´ì—°íƒ„", "ì›ë£Œíƒ„"],
            "ìì›": ["ê´‘ë¬¼", "ì›ìì¬", "commodity", "ì›ë£Œ"],
            "ë¬´ì—­": ["trading", "íŠ¸ë ˆì´ë”©", "ìˆ˜ì¶œ", "ìˆ˜ì…", "í•´ì™¸ì‚¬ì—…"],
            "íˆ¬ì": ["investment", "ì¸ìˆ˜", "í•©ë³‘", "ma", "íˆ¬ìê³„íš"],
            "ê±´ì„¤": ["construction", "í”ŒëœíŠ¸", "ì¸í”„ë¼", "í† ëª©"],
            "í™ë³´": ["pr", "ì–¸ë¡ ", "ë¯¸ë””ì–´", "ë³´ë„", "ê¸°ì", "í™ë³´íŒ€"]
        }
        
        # ê° ë¶€ì„œë³„ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
        for dept_name, dept_info in departments.items():
            if not dept_info.get("í™œì„±ìƒíƒœ", True):
                continue
                
            keywords = dept_info.get("ë‹´ë‹¹ì´ìŠˆ", [])
            keyword_str = dept_info.get("í‚¤ì›Œë“œ", "")
            
            # ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
            all_keywords = keywords + [k.strip() for k in keyword_str.split(",") if k.strip()]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì²´í¬ (ê°œì„ ëœ ë¡œì§)
            match_score = 0
            matched_keywords = []
            
            for keyword in all_keywords:
                keyword_lower = keyword.lower()
                
                # 1. ì§ì ‘ ë§¤ì¹­
                if keyword_lower in issue_lower:
                    match_score += 2  # ì§ì ‘ ë§¤ì¹­ì€ ë†’ì€ ì ìˆ˜
                    matched_keywords.append(keyword)
                    continue
                
                # 2. ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ ì´ìŠˆ ì„¤ëª…ì— í¬í•¨)
                if len(keyword_lower) > 2:  # 2ê¸€ì ì´í•˜ëŠ” ë¶€ë¶„ë§¤ì¹­ ì œì™¸
                    for word in issue_lower.split():
                        if keyword_lower in word or word in keyword_lower:
                            match_score += 1
                            matched_keywords.append(f"{keyword}(ë¶€ë¶„)")
                            break
                
                # 3. ë™ì˜ì–´ ë§¤ì¹­
                if keyword_lower in synonym_map:
                    for synonym in synonym_map[keyword_lower]:
                        if synonym.lower() in issue_lower:
                            match_score += 1.5  # ë™ì˜ì–´ ë§¤ì¹­ì€ ì¤‘ê°„ ì ìˆ˜
                            matched_keywords.append(f"{keyword}â†’{synonym}")
                            break
                
                # 4. ì—­ë°©í–¥ ë™ì˜ì–´ ë§¤ì¹­
                for base_word, synonyms in synonym_map.items():
                    if keyword_lower in [s.lower() for s in synonyms]:
                        if base_word in issue_lower:
                            match_score += 1.5
                            matched_keywords.append(f"{keyword}â†{base_word}")
                            break
            
            # íŠ¹ë³„ ê°€ì¤‘ì¹˜: í™ë³´ê·¸ë£¹ì€ ëª¨ë“  ì´ìŠˆì— ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬
            if dept_name == "í™ë³´ê·¸ë£¹" and match_score == 0:
                match_score = 0.5
                matched_keywords.append("ê¸°ë³¸ëŒ€ì‘ë¶€ì„œ")
            
            if match_score > 0:
                dept_data = {
                    "ë¶€ì„œëª…": dept_name,
                    "ë‹´ë‹¹ì": dept_info.get("ë‹´ë‹¹ì", ""),
                    "ì—°ë½ì²˜": dept_info.get("ì—°ë½ì²˜", ""),
                    "ì´ë©”ì¼": dept_info.get("ì´ë©”ì¼", ""),
                    "ë‹´ë‹¹ì´ìŠˆ": dept_info.get("ë‹´ë‹¹ì´ìŠˆ", []),
                    "ìš°ì„ ìˆœìœ„": dept_info.get("ìš°ì„ ìˆœìœ„", 999),
                    "ë§¤ì¹­ì ìˆ˜": round(match_score, 1),
                    "ë§¤ì¹­í‚¤ì›Œë“œ": matched_keywords
                }
                relevant_depts.append(dept_data)
        
        # ë§¤ì¹­ì ìˆ˜ ìš°ì„ , ìš°ì„ ìˆœìœ„ ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        relevant_depts.sort(key=lambda x: (-x["ë§¤ì¹­ì ìˆ˜"], x["ìš°ì„ ìˆœìœ„"]))
        
        # ìµœì†Œ 1ê°œ ë¶€ì„œëŠ” ë°˜í™˜ (í™ë³´ê·¸ë£¹)
        if not relevant_depts:
            # í™ë³´ê·¸ë£¹ì„ ê¸°ë³¸ ë¶€ì„œë¡œ ì¶”ê°€
            hongbo_info = departments.get("í™ë³´ê·¸ë£¹", {})
            if hongbo_info:
                default_dept = {
                    "ë¶€ì„œëª…": "í™ë³´ê·¸ë£¹",
                    "ë‹´ë‹¹ì": hongbo_info.get("ë‹´ë‹¹ì", ""),
                    "ì—°ë½ì²˜": hongbo_info.get("ì—°ë½ì²˜", ""),
                    "ì´ë©”ì¼": hongbo_info.get("ì´ë©”ì¼", ""),
                    "ë‹´ë‹¹ì´ìŠˆ": hongbo_info.get("ë‹´ë‹¹ì´ìŠˆ", []),
                    "ìš°ì„ ìˆœìœ„": hongbo_info.get("ìš°ì„ ìˆœìœ„", 2),
                    "ë§¤ì¹­ì ìˆ˜": 0.1,
                    "ë§¤ì¹­í‚¤ì›Œë“œ": ["ê¸°ë³¸ëŒ€ì‘ë¶€ì„œ"]
                }
                relevant_depts.append(default_dept)
        
        return relevant_depts[:5]  # ìƒìœ„ 5ê°œ ë¶€ì„œë§Œ ë°˜í™˜
    
    def _assess_crisis_level_from_master_data(self, issue_description: str) -> str:
        """master_data.jsonì˜ crisis_levels ê¸°ì¤€ìœ¼ë¡œ ìœ„ê¸°ë‹¨ê³„ íŒì • (ê°œì„ ëœ ë²„ì „)"""
        if not self.master_data:
            return "2ë‹¨ê³„ (ì£¼ì˜)"
        
        crisis_levels = self.master_data.get("crisis_levels", {})
        if not crisis_levels:
            return "2ë‹¨ê³„ (ì£¼ì˜)"
            
        issue_lower = issue_description.lower()
        
        # ë‹¨ê³„ë³„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        level_scores = {}
        
        for level_name, level_info in crisis_levels.items():
            score = 0
            examples = level_info.get("ì˜ˆì‹œ", [])
            definition = level_info.get("ì •ì˜", "").lower()
            
            # 1. ì˜ˆì‹œ í‚¤ì›Œë“œ ì§ì ‘ ë§¤ì¹­ (ë†’ì€ ì ìˆ˜)
            for example in examples:
                example_lower = example.lower()
                if example_lower in issue_lower:
                    score += 3  # ì§ì ‘ ë§¤ì¹­ì€ 3ì 
                elif any(word in issue_lower for word in example_lower.split()):
                    score += 1  # ë¶€ë¶„ ë§¤ì¹­ì€ 1ì 
            
            # 2. ì •ì˜ í‚¤ì›Œë“œ ë§¤ì¹­
            definition_keywords = definition.split()
            for keyword in definition_keywords:
                if len(keyword) > 2 and keyword in issue_lower:
                    score += 0.5
            
            # 3. ë§¥ë½ ê¸°ë°˜ ë§¤ì¹­ (ì¶”ê°€ í‚¤ì›Œë“œ)
            context_keywords = {
                "1ë‹¨ê³„ (ê´€ì‹¬)": ["ì¶œì‹œ", "ë°œí‘œ", "ìˆ˜ìƒ", "ê¸ì •", "ì„±ê³¼", "ì„±ì¥", "ê°œì„ ", "ê¸°ì—¬", "í˜‘ì•½", "íŒŒíŠ¸ë„ˆì‹­"],
                "2ë‹¨ê³„ (ì£¼ì˜)": ["ë¬¸ì˜", "ê²€í† ", "í™•ì¸", "ê´€ë ¨", "ê´€ì‹¬", "ìš°ë ¤", "ê²€ì¦", "ì ê²€"],
                "3ë‹¨ê³„ (ìœ„ê¸°)": ["ë…¼ë€", "ë¹„íŒ", "í•­ì˜", "ë¬¸ì œ", "ì˜í˜¹", "ì¡°ì‚¬", "ê²€ì°°", "ìˆ˜ì‚¬", "ë¦¬ì½œ", "ê²°í•¨"],
                "4ë‹¨ê³„ (ë¹„ìƒ)": ["ì‚¬ê³ ", "ìœ ì¶œ", "í­ë°œ", "í™”ì¬", "ì¸ëª…í”¼í•´", "í™˜ê²½ì˜¤ì—¼", "ëŒ€ê·œëª¨", "ì‹¬ê°", "ë¹„ìƒì‚¬íƒœ"]
            }
            
            if level_name in context_keywords:
                for keyword in context_keywords[level_name]:
                    if keyword in issue_lower:
                        score += 2  # ë§¥ë½ í‚¤ì›Œë“œëŠ” 2ì 
            
            level_scores[level_name] = score
        
        # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ë‹¨ê³„ ì„ íƒ
        if level_scores:
            best_level = max(level_scores, key=level_scores.get)
            if level_scores[best_level] > 0:
                return best_level
        
        # ì ìˆ˜ê°€ ëª¨ë‘ 0ì´ë©´ ë‚´ìš© ë¶„ì„ìœ¼ë¡œ ê¸°ë³¸ íŒì •
        if any(word in issue_lower for word in ["ì‹¤ì ", "ë°œí‘œ", "ì¶œì‹œ", "ìˆ˜ìƒ", "ì„±ê³¼"]):
            return "1ë‹¨ê³„ (ê´€ì‹¬)"
        elif any(word in issue_lower for word in ["ì‚¬ê³ ", "ìœ ì¶œ", "í”¼í•´", "ë¹„ìƒ"]):
            return "4ë‹¨ê³„ (ë¹„ìƒ)"
        elif any(word in issue_lower for word in ["ë…¼ë€", "ì˜í˜¹", "ì¡°ì‚¬", "ë¬¸ì œ"]):
            return "3ë‹¨ê³„ (ìœ„ê¸°)"
        else:
            return "2ë‹¨ê³„ (ì£¼ì˜)"  # ê¸°ë³¸ê°’
    
    def _get_media_info_from_master_data(self, media_name: str) -> dict:
        """master_data.jsonì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        if not self.master_data:
            return {}
        
        media_contacts = self.master_data.get("media_contacts", {})
        
        # ì •í™•í•œ ë§¤ì¹­ ë¨¼ì € ì‹œë„
        if media_name in media_contacts:
            return media_contacts[media_name]
        
        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
        for media_key, media_info in media_contacts.items():
            if media_name.lower() in media_key.lower() or media_key.lower() in media_name.lower():
                return media_info
        
        return {}
    
    def _build_comprehensive_context(self, **kwargs) -> str:
        """ì¢…í•© ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±"""
        current_time = kwargs.get('current_time', '')
        crisis_level = kwargs.get('crisis_level', '')
        relevant_depts = kwargs.get('relevant_depts', [])
        media_info = kwargs.get('media_info', {})
        similar_cases = kwargs.get('similar_cases', pd.DataFrame())
        
        context = f"""
ğŸ“… ë°œìƒì¼ì‹œ: {current_time}
ğŸš¨ ìœ„ê¸°ë‹¨ê³„: {crisis_level}

ğŸ“‹ ê´€ë ¨ ë¶€ì„œ ì •ë³´ ({len(relevant_depts)}ê°œ):"""
        
        for i, dept in enumerate(relevant_depts[:3], 1):
            context += f"""
{i}. {dept['ë¶€ì„œëª…']}
   - ë‹´ë‹¹ì: {dept['ë‹´ë‹¹ì']}
   - ì—°ë½ì²˜: {dept['ì—°ë½ì²˜']}
   - ì´ë©”ì¼: {dept['ì´ë©”ì¼']}
   - ë‹´ë‹¹ì˜ì—­: {', '.join(dept['ë‹´ë‹¹ì´ìŠˆ'][:5])}"""
        
        # ì–¸ë¡ ì‚¬ ì •ë³´
        if media_info:
            context += f"""

ğŸ“° ì–¸ë¡ ì‚¬ ì •ë³´:
   - êµ¬ë¶„: {media_info.get('êµ¬ë¶„', 'N/A')}
   - ë‹´ë‹¹ì: {media_info.get('ë‹´ë‹¹ì', 'N/A')}
   - ì¶œì…ê¸°ì: {len(media_info.get('ì¶œì…ê¸°ì', []))}ëª…"""
        
        # ìœ ì‚¬ ì‚¬ë¡€
        if not similar_cases.empty:
            context += f"""

ğŸ“š ìœ ì‚¬ ì‚¬ë¡€ ({len(similar_cases)}ê±´):"""
            for idx, (_, case) in enumerate(similar_cases.head(2).iterrows(), 1):
                context += f"""
{idx}. {case.get('ì´ìŠˆë‚´ìš©', 'ë‚´ìš© ì—†ìŒ')[:100]}..."""
        
        return context
    
    def _validate_inputs(self, media_name: str, reporter_name: str, issue_description: str) -> bool:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        if not media_name or len(media_name.strip()) < 2:
            return False
        if not reporter_name or len(reporter_name.strip()) < 2:
            return False
        if not issue_description or len(issue_description.strip()) < 10:
            return False
        return True
    
    def _analyze_issue_nature(self, issue_description: str) -> dict:
        """LLM ê¸°ë°˜ ì´ìŠˆ ì´ˆê¸° ë¶„ì„"""
        analysis_prompt = f"""
ë‹¤ìŒ ì´ìŠˆë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì£¼ìš” íŠ¹ì„±ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

ì´ìŠˆ: {issue_description}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "category": "ì œí’ˆ/í™˜ê²½/ë²•ë¬´/ê²½ì˜/HR/IR ì¤‘ í•˜ë‚˜",
    "complexity": "ë‹¨ìˆœ/ì¤‘ê°„/ë³µì¡ ì¤‘ í•˜ë‚˜", 
    "impact_scope": "ë‚´ë¶€/ì—…ê³„/ì‚¬íšŒì „ë°˜ ì¤‘ í•˜ë‚˜",
    "urgency": "ë‚®ìŒ/ë³´í†µ/ë†’ìŒ/ë§¤ìš°ë†’ìŒ ì¤‘ í•˜ë‚˜",
    "key_risks": ["ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì†Œë“¤"],
    "stakeholders": ["ì£¼ìš” ì´í•´ê´€ê³„ìë“¤"],
    "summary": "ì´ìŠˆ í•µì‹¬ ìš”ì•½ (50ì ì´ë‚´)"
}}
        """
        
        try:
            response = self.llm.chat(analysis_prompt)
            # JSON íŒŒì‹± ì‹œë„
            import json
            return json.loads(response)
        except:
            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
            return {
                "category": "ì¼ë°˜",
                "complexity": "ì¤‘ê°„", 
                "impact_scope": "ì—…ê³„",
                "urgency": "ë³´í†µ",
                "key_risks": ["í‰íŒ ì†ìƒ"],
                "stakeholders": ["ì–¸ë¡ ", "ê³ ê°"],
                "summary": "ì´ìŠˆ ë¶„ì„ ì¤‘"
            }
    
    def _conduct_web_research(self, issue_description: str, initial_analysis: dict) -> dict:
        """ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        web_results = {"sources": {}, "search_summary": "ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}
        
        try:
            # Enhanced Research Serviceë¥¼ ìš°ì„  ì‚¬ìš©
            if self.enhanced_research:
                print("  PROCESSING: ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
                try:
                    enhanced_results = self.enhanced_research.research_issue_comprehensive(issue_description)
                    if enhanced_results and enhanced_results.get('sources'):
                        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                        total_sources = sum(len(sources) for sources in enhanced_results['sources'].values())
                        web_results = {
                            "sources": enhanced_results['sources'],
                            "search_summary": f"ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ ì™„ë£Œ - ì´ {total_sources}ê±´ (ì‹ ë¢°ë„: {enhanced_results.get('analysis_summary', {}).get('credibility_level', 'N/A')})"
                        }
                        print(f"  RESULT: ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ ì™„ë£Œ - {web_results.get('search_summary')}")
                        return web_results
                except Exception as e:
                    print(f"  WARNING: ê°•í™”ëœ ê²€ìƒ‰ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜: {str(e)}")
            
            # ê¸°ë³¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì‚¬ìš© (í´ë°±)
            if not self.research_service:
                print("  WARNING: ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return web_results
            
            print("  PROCESSING: ê¸°ë³¸ ë„¤ì´ë²„ API ê²€ìƒ‰ ì‹œì‘...")
            
            # ê¸°ì¡´ IssueResearchService ì‚¬ìš©
            search_results = self.research_service.comprehensive_search(issue_description)
            
            if search_results and len(search_results) > 0:
                web_results = {
                    "sources": {"naver_news": search_results},
                    "search_query": issue_description,
                    "search_summary": f"ë„¤ì´ë²„ ë‰´ìŠ¤ {len(search_results)}ê±´ ìˆ˜ì§‘"
                }
            
            print(f"  RESULT: {web_results['search_summary']}")
            return web_results
            
        except Exception as e:
            print(f"  ERROR: ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return web_results
    
    def _verify_facts_and_background(self, issue_description: str, web_results: dict, initial_analysis: dict) -> dict:
        """ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ê¸°ë°˜ ì‚¬ì‹¤ í™•ì¸"""
        
        # ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ ì‚¬ì‹¤ ì •ë³´ ì¶”ì¶œ
        comprehensive_context = self._extract_comprehensive_context(web_results)
        
        fact_check_prompt = f"""
ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•´ ë‹¤ì¤‘ ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•©ì ì¸ ì‚¬ì‹¤ í™•ì¸ ë° ë°°ê²½ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

ì´ìŠˆ: {issue_description}

=== ìˆ˜ì§‘ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ì •ë³´ ===
{comprehensive_context}

ë‹¤ìŒ í•­ëª©ë“¤ì„ ì‹ ì¤‘íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì‚¬ì‹¤ í™•ì¸ ìƒíƒœ (ê³µì‹í™•ì¸ë¨/ì–¸ë¡ ë³´ë„ë¨/ì¶”ì •ë‹¨ê³„/í™•ì¸ë¶ˆê°€)
2. ì‹ ë¢°ë„ í‰ê°€ (ë§¤ìš°ë†’ìŒ/ë†’ìŒ/ë³´í†µ/ë‚®ìŒ/ë§¤ìš°ë‚®ìŒ)
3. ê³µì‹ ì†ŒìŠ¤ vs ì–¸ë¡  ë³´ë„ ê°„ ì¼ì¹˜ì„± ë¶„ì„
4. ì—…ê³„ ë§¥ë½ ë° ë°°ê²½ ì •ë³´
5. ìœ ì‚¬ ì‚¬ë¡€ ë° ì„ ë¡€ ë¶„ì„
6. ì ì¬ì  íŒŒê¸‰ íš¨ê³¼ ë° ë¦¬ìŠ¤í¬ í‰ê°€
7. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­
8. ëŒ€ì‘ ì‹œ ì£¼ì˜ì‚¬í•­

JSON í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm.chat(fact_check_prompt)
            import json
            fact_check_result = json.loads(response)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if web_results.get("analysis_summary"):
                fact_check_result["source_metadata"] = {
                    "total_sources": web_results["analysis_summary"].get("total_sources", 0),
                    "official_sources_available": web_results["analysis_summary"].get("official_sources_available", False),
                    "credibility_level": web_results["analysis_summary"].get("credibility_level", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "source_breakdown": web_results["analysis_summary"].get("source_breakdown", {})
                }
            
            return fact_check_result
            
        except Exception as e:
            print(f"  WARNING: ì‚¬ì‹¤ í™•ì¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "fact_status": "í™•ì¸ë¶ˆê°€",
                "credibility": "ë‚®ìŒ",
                "consistency_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                "background_context": "ë°°ê²½ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨",
                "similar_cases": "ìœ ì‚¬ ì‚¬ë¡€ ì¡°ì‚¬ ì‹¤íŒ¨",
                "potential_impact": "ì˜í–¥ ë¶„ì„ ì‹¤íŒ¨",
                "additional_verification_needed": ["ì „ì²´ ì‚¬ì‹¤ ê´€ê³„ ì¬í™•ì¸"],
                "cautions": "ì‹ ì¤‘í•œ ê²€ì¦ í›„ ëŒ€ì‘ í•„ìš”",
                "source_metadata": {
                    "total_sources": 0,
                    "official_sources_available": False,
                    "credibility_level": "í™•ì¸ë¶ˆê°€"
                }
            }
    
    def _extract_comprehensive_context(self, web_results: dict) -> str:
        """ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ ì¢…í•©ì ì¸ ë§¥ë½ ì •ë³´ ì¶”ì¶œ"""
        context_sections = []
        
        # ìƒˆë¡œìš´ ë‹¤ì¤‘ ì†ŒìŠ¤ êµ¬ì¡° ì²˜ë¦¬
        if "sources" in web_results:
            sources = web_results["sources"]
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤
            if sources.get("naver_news"):
                context_sections.append("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ì •ë³´:")
                for i, item in enumerate(sources["naver_news"][:5], 1):
                    title = item.get("title", "")
                    desc = item.get("description", "")
                    context_sections.append(f"{i}. {title}")
                    if desc:
                        context_sections.append(f"   ìš”ì•½: {desc[:100]}...")
                context_sections.append("")
            
            # í¬ìŠ¤ì½” ê³µì‹ ì†ŒìŠ¤
            if sources.get("posco_official"):
                context_sections.append("ğŸ¢ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê³µì‹ ì •ë³´:")
                for i, item in enumerate(sources["posco_official"][:3], 1):
                    title = item.get("title", "")
                    desc = item.get("description", "")
                    context_sections.append(f"{i}. {title}")
                    if desc:
                        context_sections.append(f"   ë‚´ìš©: {desc[:150]}...")
                context_sections.append("")
            
            # DART ê³µì‹œ ì •ë³´
            if sources.get("dart_filings"):
                context_sections.append("ğŸ“‹ DART ì „ìê³µì‹œ ì •ë³´:")
                for i, item in enumerate(sources["dart_filings"][:3], 1):
                    title = item.get("title", "")
                    context_sections.append(f"{i}. {title}")
                context_sections.append("")
            
            # í•œêµ­ê±°ë˜ì†Œ ì •ë³´
            if sources.get("krx_disclosures"):
                context_sections.append("ğŸ›ï¸ í•œêµ­ê±°ë˜ì†Œ ê³µì‹œ ì •ë³´:")
                for i, item in enumerate(sources["krx_disclosures"][:3], 1):
                    title = item.get("title", "")
                    context_sections.append(f"{i}. {title}")
                context_sections.append("")
        
        # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜ì„± (í´ë°±)
        elif web_results.get("news"):
            context_sections.append("ğŸ“° ë‰´ìŠ¤ ì •ë³´:")
            for i, item in enumerate(web_results["news"][:3], 1):
                title = item.get("title", "")
                desc = item.get("description", "")
                context_sections.append(f"{i}. {title}: {desc}")
            context_sections.append("")
        
        if not context_sections:
            return "ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì¡°ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        # ê²€ìƒ‰ ìš”ì•½ ì •ë³´ ì¶”ê°€
        if web_results.get("search_summary"):
            context_sections.insert(0, f"ğŸ” ê²€ìƒ‰ ìš”ì•½: {web_results['search_summary']}")
            context_sections.insert(1, "")
        
        return "\n".join(context_sections)
    
    def _generate_department_opinions(self, relevant_depts: list, issue_description: str, web_results: dict) -> dict:
        """ìœ ê´€ë¶€ì„œë³„ ì˜ê²¬ ê°€ì•ˆ ë„ì¶œ"""
        department_opinions = {}
        
        for dept in relevant_depts[:3]:  # ìƒìœ„ 3ê°œ ë¶€ì„œë§Œ
            dept_name = dept.get('ë¶€ì„œëª…', '')
            dept_issues = dept.get('ë‹´ë‹¹ì´ìŠˆ', [])
            
            opinion_prompt = f"""
ë‹¹ì‹ ì€ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ {dept_name} ì†Œì† ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•œ ìš°ë¦¬ ë¶€ì„œ ê´€ì ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

ì´ìŠˆ: {issue_description}
ìš°ë¦¬ ë¶€ì„œ ë‹´ë‹¹ì˜ì—­: {', '.join(dept_issues[:5])}

ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì‚¬ì‹¤ í™•ì¸ í•„ìš”ì‚¬í•­
2. ìš°ë¦¬ ë¶€ì„œ ì£¼ìš” ìš°ë ¤ì   
3. ì œì•ˆí•˜ëŠ” ëŒ€ì‘ë°©ì•ˆ
4. íƒ€ ë¶€ì„œ í˜‘ì¡° í•„ìš”ì‚¬í•­

ì „ë¬¸ê°€ë‹µê²Œ êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. (ê° í•­ëª©ë‹¹ 2-3ì¤„)
            """
            
            try:
                opinion = self.llm.chat(opinion_prompt)
                department_opinions[dept_name] = {
                    "department": dept_name,
                    "contact": dept.get('ë‹´ë‹¹ì', ''),
                    "phone": dept.get('ì—°ë½ì²˜', ''),
                    "opinion": opinion
                }
                print(f"  DONE: {dept_name} ì˜ê²¬ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                print(f"  WARNING: {dept_name} ì˜ê²¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                
        return department_opinions
    
    def _develop_pr_strategy(self, issue_description: str, crisis_level: str, fact_verification: dict, department_opinions: dict) -> dict:
        """ì–¸ë¡ í™ë³´ ì „ë¬¸ê°€ ê´€ì  ëŒ€ì‘ë°©ì•ˆ ë§ˆë ¨"""
        
        # ë¶€ì„œ ì˜ê²¬ë“¤ ìš”ì•½
        dept_summary = ""
        for dept_name, opinion_data in department_opinions.items():
            dept_summary += f"\n- {dept_name}: {opinion_data.get('opinion', '')[:100]}..."
        
        pr_strategy_prompt = f"""
ë‹¹ì‹ ì€ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ì˜ ì–¸ë¡ í™ë³´ ìµœê³  ì±…ì„ìì…ë‹ˆë‹¤.

ë‹¤ìŒ ì´ìŠˆì— ëŒ€í•œ ì „ëµì  ì–¸ë¡  ëŒ€ì‘ë°©ì•ˆì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

ì´ìŠˆ: {issue_description}
ìœ„ê¸°ë‹¨ê³„: {crisis_level}
ì‚¬ì‹¤í™•ì¸ìƒíƒœ: {fact_verification.get('fact_status', 'í™•ì¸ ì¤‘')}

ìœ ê´€ë¶€ì„œ ì˜ê²¬ ìš”ì•½:
{dept_summary if dept_summary else "ë¶€ì„œ ì˜ê²¬ ìˆ˜ì§‘ ì¤‘"}

ì „ë¬¸ ì–¸ë¡ í™ë³´ë‹´ë‹¹ìë¡œì„œ ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•œ ëŒ€ì‘ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

1. ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê¸°ì¡° (íˆ¬ëª…ì„±/ì‹ ì¤‘í•¨/ì ê·¹ì„± ë“±)
2. í•µì‹¬ ë©”ì‹œì§€ (3ê°€ì§€ ì´ë‚´)
3. ì¦‰ì‹œ ëŒ€ì‘ì‚¬í•­ (24ì‹œê°„ ë‚´)
4. ë‹¨ê³„ë³„ ëŒ€ì‘ ê³„íš
5. ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ëŒ€ì‘
6. ìœ„í—˜ ìš”ì†Œ ë° ì£¼ì˜ì‚¬í•­

í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ì˜ ì‹ ë¢°ë„ì™€ ë¸Œëœë“œ ê°€ì¹˜ë¥¼ ë³´í˜¸í•˜ëŠ” ê´€ì ì—ì„œ ì „ëµì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm.chat(pr_strategy_prompt)
            return {
                "strategy_content": response,
                "communication_tone": "ì‹ ì¤‘í•˜ë©´ì„œë„ íˆ¬ëª…í•œ ëŒ€ì‘",
                "key_messages": ["ì‚¬ì‹¤ í™•ì¸ ì¤‘", "ê³ ê° ì•ˆì „ ìµœìš°ì„ ", "ì ê·¹ì  ê°œì„  ì˜ì§€"],
                "immediate_actions": ["ë‚´ë¶€ ì‚¬ì‹¤ê´€ê³„ í™•ì¸", "ê´€ë ¨ ë¶€ì„œ TF êµ¬ì„±", "ì´ˆê¸° ì…ì¥ë¬¸ ì¤€ë¹„"]
            }
        except Exception as e:
            print(f"  WARNING: PR ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨: {str(e)}")
            return {
                "strategy_content": "ì „ëµ ìˆ˜ë¦½ ì¤‘",
                "communication_tone": "ì‹ ì¤‘í•œ ëŒ€ì‘",
                "key_messages": ["ì‚¬ì‹¤ í™•ì¸ ì§„í–‰ ì¤‘"],
                "immediate_actions": ["ê´€ë ¨ ë¶€ì„œ í˜‘ì˜"]
            }
    
    def _generate_final_comprehensive_report(self, **kwargs) -> str:
        """ìµœì¢… ì¢…í•© ë³´ê³ ì„œ ìƒì„± (êµ¬ì¡°í™”ëœ ë°©ì‹)"""
        
        # êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±ê¸° ì‚¬ìš©
        try:
            from improved_report_generator import StructuredReportGenerator
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬
            analysis_results = {
                'media_name': kwargs.get('media_name', ''),
                'reporter_name': kwargs.get('reporter_name', ''),
                'issue_description': kwargs.get('issue_description', ''),
                'crisis_level': kwargs.get('crisis_level', 'í™•ì¸ ì¤‘'),
                'initial_analysis': kwargs.get('initial_analysis', {}),
                'relevant_depts': kwargs.get('relevant_depts', []),
                'web_search_results': kwargs.get('web_search_results', {}),
                'fact_verification': kwargs.get('fact_verification', {}),
                'department_opinions': kwargs.get('department_opinions', {}),
                'pr_strategy': kwargs.get('pr_strategy', {})
            }
            
            # êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±
            generator = StructuredReportGenerator(self.data_folder)
            structured_report = generator.generate_structured_report(analysis_results)
            
            print("  SUCCESS: êµ¬ì¡°í™”ëœ í…œí”Œë¦¿ ì ìš© - risk_report.txt êµ¬ì¡° ì¤€ìˆ˜")
            return structured_report
            
        except ImportError:
            print("  WARNING: êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±ê¸° ì—†ìŒ, ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ í´ë°±")
            return self._generate_final_comprehensive_report_fallback(**kwargs)
        except Exception as e:
            print(f"  ERROR: êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._generate_final_comprehensive_report_fallback(**kwargs)
    
    def _generate_final_comprehensive_report_fallback(self, **kwargs) -> str:
        """ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ë°©ì‹ ë³´ê³ ì„œ ìƒì„± (í´ë°±ìš©)"""
        
        media_name = kwargs.get('media_name', '')
        reporter_name = kwargs.get('reporter_name', '') 
        issue_description = kwargs.get('issue_description', '')
        initial_analysis = kwargs.get('initial_analysis', {})
        relevant_depts = kwargs.get('relevant_depts', [])
        crisis_level = kwargs.get('crisis_level', '')
        web_search_results = kwargs.get('web_search_results', {})
        fact_verification = kwargs.get('fact_verification', {})
        department_opinions = kwargs.get('department_opinions', {})
        pr_strategy = kwargs.get('pr_strategy', {})
        
        # risk_report.txt í…œí”Œë¦¿ ë¡œë“œ
        template_content = self._load_report_template()
        current_time = self._get_current_time()
        
        # ê°•í™”ëœ êµ¬ì¡°í™” í”„ë¡¬í”„íŠ¸ (í…œí”Œë¦¿ êµ¬ì¡° ê°•ì œ)
        final_prompt = f"""
ë‹¤ìŒ í…œí”Œë¦¿ êµ¬ì¡°ë¥¼ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

--- í…œí”Œë¦¿ ì‹œì‘ ---
<ì´ìŠˆ ë°œìƒ ë³´ê³ >

1. ë°œìƒ ì¼ì‹œ: {current_time}

2. ë°œìƒ ë‹¨ê³„: {crisis_level}

3. ë°œìƒ ë‚´ìš©:
({media_name} {reporter_name})
{issue_description}

4. ìœ ê´€ ì˜ê²¬:
{self._format_department_opinions_for_report(relevant_depts, department_opinions)}
- ì‚¬ì‹¤ í™•ì¸: {fact_verification.get('fact_status', 'N/A')} (ì‹ ë¢°ë„: {fact_verification.get('credibility', 'N/A')})
- ë©”ì‹œì§€ ë°©í–¥ì„±: {pr_strategy.get('communication_tone', 'ì‹ ì¤‘í•œ ì ‘ê·¼')}

5. ëŒ€ì‘ ë°©ì•ˆ:
- ì›ë³´ì´ìŠ¤: {', '.join(pr_strategy.get('key_messages', ['ì •í™•í•œ ì‚¬ì‹¤ í™•ì¸ í›„ ëŒ€ì‘'])[:2])}
- ì´í›„ ëŒ€ì‘ ë°©í–¥ì„±: {', '.join(pr_strategy.get('immediate_actions', ['ê´€ë ¨ ë¶€ì„œ í˜‘ì˜'])[:2])}

6. ëŒ€ì‘ ê²°ê³¼: (ì¶”í›„ ì—…ë°ì´íŠ¸)

ì°¸ì¡°. ìµœê·¼ ìœ ì‚¬ ì‚¬ë¡€ (1ë…„ ì´ë‚´):
- ê´€ë ¨ ë³´ë„ì‚¬ë¡€ ì¡°ì‚¬ ì¤‘

ì°¸ì¡°. ì´ìŠˆ ì •ì˜ ë° ê°œë… ì •ë¦½:
- ì´ìŠˆ ë¶„ë¥˜: {initial_analysis.get('category', 'N/A')}
- ì˜í–¥ë²”ìœ„: {initial_analysis.get('impact_scope', 'N/A')}
--- í…œí”Œë¦¿ ë ---

**ì¤‘ìš”**: ìœ„ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¼ ì‘ì„±í•˜ê³ , ì¶”ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ ë¶„ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
        
        try:
            return self.llm.chat(final_prompt)
        except Exception as e:
            return f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _load_report_template(self) -> str:
        """ì´ìŠˆë°œìƒë³´ê³ ì„œ í…œí”Œë¦¿ ë¡œë“œ"""
        try:
            template_path = os.path.join(self.data_folder, "risk_report.txt")
            if os.path.exists(template_path):
                with open(template_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                return "í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"í…œí”Œë¦¿ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def _search_by_media(self, media_name: str, limit: int = 5) -> pd.DataFrame:
        """íŠ¹ì • ì–¸ë¡ ì‚¬ì˜ ê³¼ê±° ì‚¬ë¡€ ê²€ìƒ‰ (ê°œì„ ëœ ì •ê·œí™” ê²€ìƒ‰)"""
        if self.media_response_data is None:
            return pd.DataFrame()
        
        # 1ë‹¨ê³„: ì–¸ë¡ ì‚¬ëª… ì •ê·œí™” ë° ë³„ì¹­ ì²˜ë¦¬
        normalized_media_name = self._normalize_media_name(media_name)
        media_aliases = self._get_media_aliases(normalized_media_name)
        
        # 2ë‹¨ê³„: ë‹¤ì¤‘ ì»¬ëŸ¼ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ê²€ìƒ‰
        media_cases = self._multi_column_media_search(media_aliases)
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬ ë° ì •ë ¬
        if not media_cases.empty:
            # ì–¸ë¡ ì‚¬ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±
            if 'ì–¸ë¡ ì‚¬' not in media_cases.columns:
                media_cases = self._add_media_column(media_cases, normalized_media_name)
            
            # ìµœê·¼ ìˆœìœ¼ë¡œ ì •ë ¬
            if 'ë°œìƒ ì¼ì‹œ' in media_cases.columns:
                media_cases = media_cases.sort_values('ë°œìƒ ì¼ì‹œ', ascending=False)
        
        return media_cases.head(limit)
    
    def _normalize_media_name(self, media_name: str) -> str:
        """ì–¸ë¡ ì‚¬ëª… ì •ê·œí™”"""
        # ê³µí†µ ì ‘ë¯¸ì‚¬/ì ‘ë‘ì‚¬ ì œê±°
        normalized = media_name.strip()
        
        # ì •ê·œí™” ê·œì¹™ë“¤
        normalization_rules = {
            'ì¡°ì„ ì¼ë³´': ['ì¡°ì„ ', 'ì¡°ì„ ì¼ë³´'],
            'ì¤‘ì•™ì¼ë³´': ['ì¤‘ì•™', 'ì¤‘ì•™ì¼ë³´'],
            'ë™ì•„ì¼ë³´': ['ë™ì•„', 'ë™ì•„ì¼ë³´'],
            'í•œêµ­ê²½ì œ': ['í•œê²½', 'í•œêµ­ê²½ì œ', 'í•œêµ­ê²½ì œì‹ ë¬¸'],
            'ë§¤ì¼ê²½ì œ': ['ë§¤ê²½', 'ë§¤ì¼ê²½ì œ', 'ë§¤ì¼ê²½ì œì‹ ë¬¸'],
            'ì„œìš¸ê²½ì œ': ['ì„œìš¸ê²½ì œ', 'ì„œìš¸ê²½ì œì‹ ë¬¸'],
            'ì—°í•©ë‰´ìŠ¤': ['ì—°í•©', 'ì—°í•©ë‰´ìŠ¤'],
            'ë‰´ì‹œìŠ¤': ['ë‰´ì‹œìŠ¤', 'newsis'],
            'ë‰´ìŠ¤1': ['ë‰´ìŠ¤1', 'news1'],
            'ë‰´ìŠ¤í•Œ': ['ë‰´ìŠ¤í•Œ', 'newspim'],
            'ë¨¸ë‹ˆíˆ¬ë°ì´': ['ë¨¸ë‹ˆíˆ¬ë°ì´', 'mt', 'ë¨¸íˆ¬'],
            'ì´ë°ì¼ë¦¬': ['ì´ë°ì¼ë¦¬', 'edaily'],
            'íŒŒì´ë‚¸ì…œë‰´ìŠ¤': ['íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'fn', 'íŒŒì´ë‚¸ì…œ'],
            'ì•„ì‹œì•„ê²½ì œ': ['ì•„ì‹œì•„ê²½ì œ', 'ì•„ê²½'],
            'í—¤ëŸ´ë“œê²½ì œ': ['í—¤ëŸ´ë“œê²½ì œ', 'í—¤ëŸ´ë“œ'],
            'í•œê²¨ë ˆ': ['í•œê²¨ë ˆ', 'hani'],
            'ê²½í–¥ì‹ ë¬¸': ['ê²½í–¥', 'ê²½í–¥ì‹ ë¬¸'],
            'êµ­ë¯¼ì¼ë³´': ['êµ­ë¯¼ì¼ë³´', 'êµ­ë¯¼']
        }
        
        # ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (ë³„ì¹­ â†’ ì •ê·œëª…)
        for standard_name, aliases in normalization_rules.items():
            if normalized.lower() in [alias.lower() for alias in aliases]:
                return standard_name
        
        return normalized
    
    def _get_media_aliases(self, media_name: str) -> List[str]:
        """ì–¸ë¡ ì‚¬ì˜ ëª¨ë“  ë³„ì¹­ ë°˜í™˜"""
        normalization_rules = {
            'ì¡°ì„ ì¼ë³´': ['ì¡°ì„ ', 'ì¡°ì„ ì¼ë³´', 'chosun'],
            'ì¤‘ì•™ì¼ë³´': ['ì¤‘ì•™', 'ì¤‘ì•™ì¼ë³´', 'joongang'],
            'ë™ì•„ì¼ë³´': ['ë™ì•„', 'ë™ì•„ì¼ë³´', 'donga'],
            'í•œêµ­ê²½ì œ': ['í•œê²½', 'í•œêµ­ê²½ì œ', 'í•œêµ­ê²½ì œì‹ ë¬¸', 'hankyung'],
            'ë§¤ì¼ê²½ì œ': ['ë§¤ê²½', 'ë§¤ì¼ê²½ì œ', 'ë§¤ì¼ê²½ì œì‹ ë¬¸', 'mk'],
            'ì„œìš¸ê²½ì œ': ['ì„œìš¸ê²½ì œ', 'ì„œìš¸ê²½ì œì‹ ë¬¸', 'sedaily'],
            'ì—°í•©ë‰´ìŠ¤': ['ì—°í•©', 'ì—°í•©ë‰´ìŠ¤', 'yonhap'],
            'ë‰´ì‹œìŠ¤': ['ë‰´ì‹œìŠ¤', 'newsis'],
            'ë‰´ìŠ¤1': ['ë‰´ìŠ¤1', 'news1'],
            'ë‰´ìŠ¤í•Œ': ['ë‰´ìŠ¤í•Œ', 'newspim'],
            'ë¨¸ë‹ˆíˆ¬ë°ì´': ['ë¨¸ë‹ˆíˆ¬ë°ì´', 'mt', 'ë¨¸íˆ¬'],
            'ì´ë°ì¼ë¦¬': ['ì´ë°ì¼ë¦¬', 'edaily'],
            'íŒŒì´ë‚¸ì…œë‰´ìŠ¤': ['íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'fn', 'íŒŒì´ë‚¸ì…œ'],
            'ì•„ì‹œì•„ê²½ì œ': ['ì•„ì‹œì•„ê²½ì œ', 'ì•„ê²½'],
            'í—¤ëŸ´ë“œê²½ì œ': ['í—¤ëŸ´ë“œê²½ì œ', 'í—¤ëŸ´ë“œ'],
            'í•œê²¨ë ˆ': ['í•œê²¨ë ˆ', 'hani'],
            'ê²½í–¥ì‹ ë¬¸': ['ê²½í–¥', 'ê²½í–¥ì‹ ë¬¸'],
            'êµ­ë¯¼ì¼ë³´': ['êµ­ë¯¼ì¼ë³´', 'êµ­ë¯¼']
        }
        
        return normalization_rules.get(media_name, [media_name])
    
    def _multi_column_media_search(self, media_aliases: List[str]) -> pd.DataFrame:
        """ë‹¤ì¤‘ ì»¬ëŸ¼ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ê²€ìƒ‰"""
        search_results = []
        
        # ê²€ìƒ‰ ëŒ€ìƒ ì»¬ëŸ¼ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        search_columns = ['ì–¸ë¡ ì‚¬', 'ëŒ€ì‘ ê²°ê³¼', 'ì´ìŠˆ ë°œìƒ ë³´ê³ ', 'ë°œìƒ ìœ í˜•']
        
        for column in search_columns:
            if column in self.media_response_data.columns:
                for alias in media_aliases:
                    # ê° ë³„ì¹­ìœ¼ë¡œ ê²€ìƒ‰
                    matches = self.media_response_data[
                        self.media_response_data[column].str.contains(
                            alias, case=False, na=False, regex=False
                        )
                    ]
                    if not matches.empty:
                        search_results.append(matches)
        
        # ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        if search_results:
            combined_results = pd.concat(search_results, ignore_index=True)
            # ì¤‘ë³µ ì œê±° (ìˆœë²ˆ ê¸°ì¤€)
            if 'ìˆœë²ˆ' in combined_results.columns:
                combined_results = combined_results.drop_duplicates(subset=['ìˆœë²ˆ'])
            else:
                combined_results = combined_results.drop_duplicates()
            return combined_results
        
        return pd.DataFrame()
    
    def _add_media_column(self, df: pd.DataFrame, media_name: str) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ì— ì–¸ë¡ ì‚¬ ì»¬ëŸ¼ ì¶”ê°€"""
        df_copy = df.copy()
        
        # ì–¸ë¡ ì‚¬ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if 'ì–¸ë¡ ì‚¬' not in df_copy.columns:
            df_copy['ì–¸ë¡ ì‚¬'] = media_name
            
            # ëŒ€ì‘ ê²°ê³¼ì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ ì‹œë„
            if 'ëŒ€ì‘ ê²°ê³¼' in df_copy.columns:
                for idx, row in df_copy.iterrows():
                    result_text = str(row.get('ëŒ€ì‘ ê²°ê³¼', ''))
                    extracted_media = self._extract_media_from_text(result_text)
                    if extracted_media:
                        df_copy.at[idx, 'ì–¸ë¡ ì‚¬'] = extracted_media
        
        return df_copy
    
    def _extract_media_from_text(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
        if not text or pd.isna(text):
            return None
            
        text_lower = text.lower()
        
        # master_data.jsonì˜ ì–¸ë¡ ì‚¬ ì •ë³´ í™œìš©
        if self.master_data and 'media_contacts' in self.master_data:
            for media_name in self.master_data['media_contacts'].keys():
                if media_name.lower() in text_lower:
                    return media_name
        
        # ì¼ë°˜ì ì¸ ì–¸ë¡ ì‚¬ íŒ¨í„´ ê²€ìƒ‰
        common_media_patterns = [
            'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´', 'í•œêµ­ê²½ì œ', 'ë§¤ì¼ê²½ì œ', 'ì„œìš¸ê²½ì œ',
            'ì—°í•©ë‰´ìŠ¤', 'ë‰´ì‹œìŠ¤', 'ë‰´ìŠ¤1', 'ë‰´ìŠ¤í•Œ', 'ë¨¸ë‹ˆíˆ¬ë°ì´', 'ì´ë°ì¼ë¦¬',
            'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'ì•„ì‹œì•„ê²½ì œ', 'í—¤ëŸ´ë“œê²½ì œ', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'êµ­ë¯¼ì¼ë³´'
        ]
        
        for pattern in common_media_patterns:
            if pattern in text:
                return pattern
        
        return None
    
    def get_media_specific_info(self, media_name: str, reporter_name: str = None) -> Dict[str, Any]:
        """ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ì •ë³´ ì œê³µ (ì •ê·œí™”ëœ ì‹œìŠ¤í…œ)"""
        normalized_media = self._normalize_media_name(media_name)
        
        media_info = {
            'normalized_name': normalized_media,
            'classification': None,
            'reporters': [],
            'contact_person': None,
            'past_cases': [],
            'response_strategy': None,
            'reporter_info': None
        }
        
        # master_data.jsonì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì¡°íšŒ
        if self.master_data and 'media_contacts' in self.master_data:
            media_contacts = self.master_data['media_contacts']
            
            if normalized_media in media_contacts:
                contact_info = media_contacts[normalized_media]
                media_info.update({
                    'classification': contact_info.get('êµ¬ë¶„', 'N/A'),
                    'reporters': contact_info.get('ì¶œì…ê¸°ì', []),
                    'contact_person': contact_info.get('ë‹´ë‹¹ì', 'N/A')
                })
                
                # íŠ¹ì • ê¸°ì ì •ë³´ ê²€ìƒ‰
                if reporter_name:
                    media_info['reporter_info'] = self._get_reporter_info(
                        normalized_media, reporter_name, contact_info
                    )
        
        # ê³¼ê±° ëŒ€ì‘ ì‚¬ë¡€ ì¡°íšŒ
        past_cases = self._search_by_media(normalized_media, limit=5)
        if not past_cases.empty:
            media_info['past_cases'] = self._summarize_past_cases(past_cases)
        
        # ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ëŒ€ì‘ ì „ëµ
        media_info['response_strategy'] = self._get_media_response_strategy(
            normalized_media, media_info['classification']
        )
        
        return media_info
    
    def _get_reporter_info(self, media_name: str, reporter_name: str, contact_info: Dict) -> Dict:
        """íŠ¹ì • ê¸°ì ì •ë³´ ì¡°íšŒ"""
        reporter_info = {
            'name': reporter_name,
            'confirmed': False,
            'beat': None,
            'contact_history': []
        }
        
        # ì¶œì…ê¸°ì ëª…ë‹¨ì—ì„œ í™•ì¸
        reporters = contact_info.get('ì¶œì…ê¸°ì', [])
        for reporter in reporters:
            if reporter_name in reporter or reporter in reporter_name:
                reporter_info['confirmed'] = True
                reporter_info['name'] = reporter
                # ì „ë¬¸ ë¶„ì•¼ ì¶”ì¶œ (ê´„í˜¸ ì•ˆ ì •ë³´)
                if '(' in reporter and ')' in reporter:
                    beat = reporter.split('(')[1].split(')')[0]
                    reporter_info['beat'] = beat
                break
        
        # ê³¼ê±° ëŒ€ì‘ ë‚´ì—­ì—ì„œ í•´ë‹¹ ê¸°ì ê´€ë ¨ ì‚¬ë¡€ ê²€ìƒ‰
        if self.media_response_data is not None:
            reporter_cases = self.media_response_data[
                self.media_response_data['ëŒ€ì‘ ê²°ê³¼'].str.contains(
                    reporter_name, na=False, case=False
                )
            ]
            if not reporter_cases.empty:
                reporter_info['contact_history'] = len(reporter_cases)
        
        return reporter_info
    
    def _summarize_past_cases(self, past_cases: pd.DataFrame) -> List[Dict]:
        """ê³¼ê±° ì‚¬ë¡€ ìš”ì•½"""
        summaries = []
        
        for _, case in past_cases.head(3).iterrows():  # ìµœê·¼ 3ê±´ë§Œ
            summary = {
                'date': case.get('ë°œìƒ ì¼ì‹œ', 'N/A'),
                'type': case.get('ë°œìƒ ìœ í˜•', 'N/A'),
                'stage': case.get('ë‹¨ê³„', 'N/A'),
                'issue': case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ', 'N/A')[:100] + '...' if len(str(case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ', ''))) > 100 else case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ', 'N/A'),
                'outcome': case.get('ëŒ€ì‘ ê²°ê³¼', 'N/A')[:100] + '...' if len(str(case.get('ëŒ€ì‘ ê²°ê³¼', ''))) > 100 else case.get('ëŒ€ì‘ ê²°ê³¼', 'N/A')
            }
            summaries.append(summary)
        
        return summaries
    
    def _get_media_response_strategy(self, media_name: str, classification: str) -> Dict[str, Any]:
        """ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ëŒ€ì‘ ì „ëµ"""
        
        # ì–¸ë¡ ì‚¬ ìœ í˜•ë³„ ê¸°ë³¸ ì „ëµ
        base_strategies = {
            'ì¢…í•©ì§€': {
                'approach': 'í¬ê´„ì  ì •ë³´ ì œê³µ',
                'key_points': ['ì •í™•í•œ ì‚¬ì‹¤ ì „ë‹¬', 'ë°°ê²½ ì„¤ëª… ìƒì„¸', 'í–¥í›„ ê³„íš ëª…ì‹œ'],
                'tone': 'ê³µì‹ì , ì‹ ì¤‘í•œ'
            },
            'ê²½ì œì§€': {
                'approach': 'ê²½ì œì  ì„íŒ©íŠ¸ ì¤‘ì‹¬',
                'key_points': ['ì¬ë¬´ì  ì˜í–¥', 'ì‹œì¥ ì „ë§', 'íˆ¬ìì ê´€ì '],
                'tone': 'ì „ë¬¸ì , ë°ì´í„° ì¤‘ì‹¬'
            },
            'í†µì‹ ì‚¬': {
                'approach': 'ì‹ ì† ì •í™•í•œ íŒ©íŠ¸',
                'key_points': ['í•µì‹¬ ì‚¬ì‹¤', 'ê³µì‹ ì…ì¥', 'í›„ì† ì¼ì •'],
                'tone': 'ê°„ê²°í•˜ê³  ëª…í™•í•œ'
            },
            'ì„ê°„ì§€': {
                'approach': 'ì‹¬ì¸µ ë¶„ì„ ì§€ì›',
                'key_points': ['ì‚°ì—… ë™í–¥', 'ì „ëµì  ì˜ë¯¸', 'ì¥ê¸° ì „ë§'],
                'tone': 'ë¶„ì„ì , ì „ëµì '
            },
            'ì˜ìì§€': {
                'approach': 'ê¸€ë¡œë²Œ ë§¥ë½ ê°•ì¡°',
                'key_points': ['êµ­ì œ ê²½ìŸë ¥', 'í•´ì™¸ ì „ê°œ', 'ê¸€ë¡œë²Œ íŠ¸ë Œë“œ'],
                'tone': 'êµ­ì œì  ê´€ì '
            },
            'ê¸°íƒ€ì˜¨ë¼ì¸': {
                'approach': 'ë””ì§€í„¸ ì¹œí™”ì ',
                'key_points': ['ì‹œê°ì  ìë£Œ', 'ì¸í¬ê·¸ë˜í”½', 'ì†Œì…œ í™•ì‚°'],
                'tone': 'ì ‘ê·¼í•˜ê¸° ì‰¬ìš´'
            }
        }
        
        # ê¸°ë³¸ ì „ëµ
        strategy = base_strategies.get(classification, base_strategies['ì¢…í•©ì§€'])
        
        # ì–¸ë¡ ì‚¬ë³„ íŠ¹í™” ì „ëµ
        specific_strategies = {
            'ì¡°ì„ ì¼ë³´': {
                'priority': 'ì •ë¶€ì •ì±… ì—°ê´€ì„±',
                'focus': ['ì •ì±… ë¶€í•©ì„±', 'êµ­ê°€ ê²½ì œ ê¸°ì—¬', 'ì‚¬íšŒì  ì±…ì„']
            },
            'í•œêµ­ê²½ì œ': {
                'priority': 'ê²½ì œì„±ê³¼ ê°•ì¡°',
                'focus': ['ìˆ˜ìµì„±', 'ì„±ì¥ì„±', 'ì‹œì¥ ì ìœ ìœ¨', 'ROI']
            },
            'ì—°í•©ë‰´ìŠ¤': {
                'priority': 'ê°ê´€ì  ì‚¬ì‹¤ ì „ë‹¬',
                'focus': ['ê²€ì¦ëœ ì •ë³´', 'ë‹¤ê°ë„ ê²€í† ', 'ê· í˜• ì¡íŒ ì‹œê°']
            },
            'ë¨¸ë‹ˆíˆ¬ë°ì´': {
                'priority': 'íˆ¬ì ê´€ì  ë¶€ê°',
                'focus': ['ì£¼ê°€ ì˜í–¥', 'íˆ¬ìì ë°˜ì‘', 'ì• ë„ë¦¬ìŠ¤íŠ¸ ì˜ê²¬']
            }
        }
        
        if media_name in specific_strategies:
            strategy.update(specific_strategies[media_name])
        
        return strategy
    
    def generate_media_customized_response(self, media_name: str, reporter_name: str, 
                                         issue_description: str) -> str:
        """ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤í˜• ëŒ€ì‘ ë©”ì‹œì§€ ìƒì„±"""
        
        # ì–¸ë¡ ì‚¬ ì •ë³´ ìˆ˜ì§‘
        media_info = self.get_media_specific_info(media_name, reporter_name)
        
        # ê¸°ë³¸ ì´ìŠˆ ë¶„ì„
        relevant_depts = self.get_relevant_departments(issue_description)
        crisis_level = self._assess_crisis_level(issue_description)
        
        # ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        customized_prompt = f"""
        ë‹¤ìŒ ì–¸ë¡ ì‚¬ íŠ¹ì„±ì— ë§ëŠ” ë§ì¶¤í˜• ëŒ€ì‘ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        === ì–¸ë¡ ì‚¬ ì •ë³´ ===
        ì–¸ë¡ ì‚¬: {media_info['normalized_name']} ({media_info['classification']})
        ê¸°ì: {reporter_name}
        ë‹´ë‹¹ì: {media_info['contact_person']}
        
        === ê¸°ì ì •ë³´ ===
        {self._format_reporter_info(media_info.get('reporter_info', {}))}
        
        === ì´ìŠˆ ì •ë³´ ===
        ë‚´ìš©: {issue_description}
        ìœ„ê¸°ë‹¨ê³„: {crisis_level}
        ê´€ë ¨ë¶€ì„œ: {', '.join([dept.get('ë¶€ì„œëª…', '') for dept in relevant_depts[:2]])}
        
        === ê³¼ê±° ëŒ€ì‘ ì‚¬ë¡€ ===
        {self._format_past_cases(media_info.get('past_cases', []))}
        
        === ëŒ€ì‘ ì „ëµ ===
        ì ‘ê·¼ë°©ì‹: {media_info['response_strategy']['approach']}
        í•µì‹¬í¬ì¸íŠ¸: {', '.join(media_info['response_strategy']['key_points'])}
        í†¤ì•¤ë©”ë„ˆ: {media_info['response_strategy']['tone']}
        
        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ì˜ ëŒ€ì‘ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        
        1. í•µì‹¬ ë©”ì‹œì§€ (1-2ë¬¸ì¥)
        2. ìƒì„¸ ì„¤ëª… (3-4ë¬¸ì¥)
        3. ì¶”ê°€ ì œê³µ ê°€ëŠ¥í•œ ìë£Œ
        4. í›„ì† ì¼ì • ë˜ëŠ” ì—°ë½ì²˜
        """
        
        try:
            response = self.llm.chat(
                customized_prompt,
                system_prompt="ë‹¹ì‹ ì€ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ì˜ ì–¸ë¡ ëŒ€ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ì–¸ë¡ ì‚¬ì˜ íŠ¹ì„±ê³¼ ê¸°ìì˜ ì„±í–¥ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                temperature=0.3
            )
            return response
        except Exception as e:
            return f"ë§ì¶¤í˜• ëŒ€ì‘ ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def _format_reporter_info(self, reporter_info: Dict) -> str:
        """ê¸°ì ì •ë³´ í¬ë§·íŒ…"""
        if not reporter_info:
            return "ê¸°ì ì •ë³´ ì—†ìŒ"
        
        info_str = f"í™•ì¸ë¨: {'ì˜ˆ' if reporter_info.get('confirmed') else 'ì•„ë‹ˆì˜¤'}"
        if reporter_info.get('beat'):
            info_str += f", ì „ë¬¸ë¶„ì•¼: {reporter_info['beat']}"
        if reporter_info.get('contact_history'):
            info_str += f", ê³¼ê±° ì ‘ì´‰: {reporter_info['contact_history']}íšŒ"
        
        return info_str
    
    def _format_past_cases(self, past_cases: List[Dict]) -> str:
        """ê³¼ê±° ì‚¬ë¡€ í¬ë§·íŒ…"""
        if not past_cases:
            return "ê³¼ê±° ëŒ€ì‘ ì‚¬ë¡€ ì—†ìŒ"
        
        formatted = []
        for case in past_cases:
            case_str = f"- {case['date']} ({case['stage']}): {case['issue']}"
            formatted.append(case_str)
        
        return '\n'.join(formatted)
    
    def _assess_crisis_level(self, issue_description: str, verbose: bool = False) -> str:
        """AI ê¸°ë°˜ ìœ„ê¸° ë‹¨ê³„ ìë™ íŒë‹¨ (ê°œì„ ëœ ë²„ì „)"""
        
        # master_data.jsonì—ì„œ ìœ„ê¸° ë‹¨ê³„ ì •ë³´ ë¡œë“œ
        crisis_levels = self.master_data.get('crisis_levels', {})
        
        # 1ì°¨: ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ˆë¹„ íŒë‹¨
        preliminary_level, score_info = self._preliminary_crisis_assessment(issue_description, return_details=True)
        
        if verbose:
            print(f"[ìœ„ê¸°ë‹¨ê³„ íŒë‹¨] ì˜ˆë¹„ íŒë‹¨: {preliminary_level} (ì ìˆ˜: {score_info['total_score']})")
            if score_info['matched_keywords']:
                print(f"[ìœ„ê¸°ë‹¨ê³„ íŒë‹¨] ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(score_info['matched_keywords'])}")
        
        # 2ì°¨: AI ê¸°ë°˜ ì •ë°€ ë¶„ì„
        final_level = self._ai_based_crisis_assessment(issue_description, crisis_levels, preliminary_level)
        
        if verbose and final_level != preliminary_level:
            print(f"[ìœ„ê¸°ë‹¨ê³„ íŒë‹¨] AI ì •ë°€ ë¶„ì„ ê²°ê³¼: {preliminary_level} â†’ {final_level}")
        
        return final_level
    
    def _preliminary_crisis_assessment(self, issue_description: str, return_details: bool = False):
        """1ì°¨ í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ˆë¹„ ìœ„ê¸° ë‹¨ê³„ íŒë‹¨"""
        issue_lower = issue_description.lower()
        
        # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        crisis_score = 0
        matched_keywords = []
        
        # ìœ„ê¸° ë‹¨ê³„ë³„ í‚¤ì›Œë“œ ë§¤í•‘ (í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ íŠ¹í™”)
        crisis_keywords = {
            4: {  # 4ë‹¨ê³„ (ë¹„ìƒ) - ê°€ì¤‘ì¹˜ 10
                'leadership': ['ceo', 'ëŒ€í‘œì´ì‚¬', 'íšŒì¥', 'ê²½ì˜ì§„', 'ì„ì›ì§„'],
                'major_incidents': ['ìŠ¤ìº”ë“¤', 'ì•ˆì „ì‚¬ê³ ', 'ì¸ëª…í”¼í•´', 'ì¤‘ëŒ€ì¬í•´', 'í­ë°œ', 'í™”ì¬'],
                'group_crisis': ['ê·¸ë£¹ìœ„ê¸°', 'ë¹„ìƒì‚¬íƒœ', 'ì „ì‚¬ìœ„ê¸°', 'í¬ìŠ¤ì½”ê·¸ë£¹'],
                'legal_severe': ['êµ¬ì†', 'ê¸°ì†Œ', 'ê²€ì°°ìˆ˜ì‚¬', 'íŠ¹ê²€', 'êµ­ì •ê°ì‚¬'],
                'business_critical': ['ìƒì¥íì§€', 'íšŒì‚¬ì •ë¦¬', 'íŒŒì‚°', 'í•´ì‚°']
            },
            3: {  # 3ë‹¨ê³„ (ìœ„ê¸°) - ê°€ì¤‘ì¹˜ 7  
                'business_disruption': ['ì‚¬ì—…ì¤‘ë‹¨', 'ê³µì¥íì‡„', 'ê°€ë™ì¤‘ë‹¨', 'ìš´ì˜ì¤‘ë‹¨'],
                'financial_severe': ['ëŒ€ê·œëª¨ì†ì‹¤', 'ì ìì „í™˜', 'ìê¸ˆë‚œ', 'ìœ ë™ì„±ìœ„ê¸°'],
                'legal_major': ['ë²•ì ë¶„ìŸ', 'ì†Œì†¡', 'ì†í•´ë°°ìƒ', 'ì œì¬', 'ê³¼ì§•ê¸ˆ'],
                'regulatory': ['ê·œì œìœ„ë°˜', 'ì²˜ë²Œ', 'ì˜ì—…ì •ì§€', 'í—ˆê°€ì·¨ì†Œ', 'ê³¼íƒœë£Œ'],
                'investigation': ['ìˆ˜ì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ê°ì‚¬', 'ì¡°ì‚¬', 'íŠ¹ë³„ì ê²€'],
                'posco_specific': ['ë¯¸ì–€ë§ˆ', 'ê°€ìŠ¤ì „', 'ì„íƒ„ë°œì „', 'ì² ê°•', 'ì›ë£Œ']
            },
            2: {  # 2ë‹¨ê³„ (ì£¼ì˜) - ê°€ì¤‘ì¹˜ 4
                'negative_issues': ['ë¶€ì •', 'ë…¼ë€', 'ë¬¸ì œì œê¸°', 'ìš°ë ¤', 'ë¹„íŒ', 'ì˜í˜¹ì œê¸°'],
                'environmental': ['í™˜ê²½ë¬¸ì œ', 'ì˜¤ì—¼', 'íƒ„ì†Œë°°ì¶œ', 'í™˜ê²½ê·œì œ', 'ì¹œí™˜ê²½'],
                'labor': ['ë…¸ì‚¬ê°ˆë“±', 'íŒŒì—…', 'ì„ê¸ˆí˜‘ìƒ', 'ê·¼ë¡œì¡°ê±´', 'ê³ ìš©'],
                'community': ['ë¯¼ì›', 'ì£¼ë¯¼ë°˜ëŒ€', 'ì§€ì—­ê°ˆë“±', 'í•­ì˜', 'ì‹œìœ„'],
                'market_negative': ['ì£¼ê°€í•˜ë½', 'ì‹¤ì ë¶€ì§„', 'ë§¤ì¶œê°ì†Œ', 'ìˆ˜ìµì„±ì•…í™”'],
                'operational': ['í’ˆì§ˆë¬¸ì œ', 'ë‚©ê¸°ì§€ì—°', 'ê³µê¸‰ì°¨ì§ˆ', 'ìƒì‚°ì°¨ì§ˆ']
            },
            1: {  # 1ë‹¨ê³„ (ê´€ì‹¬) - ê°€ì¤‘ì¹˜ 2
                'positive_news': ['ë°œí‘œ', 'ì¶œì‹œ', 'ìˆ˜ìƒ', 'ì¸ìˆ˜', 'í™•ì¥', 'ì‹ ì œí’ˆ'],
                'investment': ['íˆ¬ì', 'ì§€ë¶„', 'í€ë“œ', 'ìë³¸', 'ì¶œì', 'ì¦ì'],
                'cooperation': ['í˜‘ë ¥', 'íŒŒíŠ¸ë„ˆì‹­', 'ì œíœ´', 'ê³„ì•½ì²´ê²°', 'mou'],
                'performance': ['ì‹¤ì ë°œí‘œ', 'ì¦ìµ', 'í‘ì', 'ì„±ì¥', 'í™•ëŒ€'],
                'csr': ['csr', 'ì‚¬íšŒê³µí—Œ', 'ê¸°ë¶€', 'ë´‰ì‚¬', 'ì§€ì›ì‚¬ì—…', 'ìƒìƒ'],
                'recognition': ['ì‹œìƒ', 'ì¸ì¦', 'ì„ ì •', 'í‰ê°€', 'ë“±ê¸‰ìƒí–¥']
            }
        }
        
        # ê° ë‹¨ê³„ë³„ í‚¤ì›Œë“œ ì ìˆ˜ ì ìš©
        for stage, categories in crisis_keywords.items():
            for category, keywords in categories.items():
                for keyword in keywords:
                    if keyword in issue_lower:
                        matched_keywords.append(f"{keyword}({stage}ë‹¨ê³„)")
                        if stage == 4:
                            crisis_score += 10
                        elif stage == 3:
                            crisis_score += 7
                        elif stage == 2:
                            crisis_score += 4
                        elif stage == 1:
                            crisis_score += 2
        
        # ë¶€ì •ì  ìˆ˜ì‹ì–´ ë° ê°•ì¡° í‘œí˜„ ì¶”ê°€ ì ìˆ˜
        negative_modifiers = ['ëŒ€ê·œëª¨', 'ì‹¬ê°í•œ', 'ì¤‘ëŒ€í•œ', 'ì¹˜ëª…ì ', 'ë§‰ëŒ€í•œ', 'ì „ë¡€ì—†ëŠ”', 'ì‚¬ìƒìµœëŒ€']
        for modifier in negative_modifiers:
            if modifier in issue_lower:
                crisis_score += 3
                matched_keywords.append(f"{modifier}(ìˆ˜ì‹ì–´)")
        
        # ê¸´ê¸‰ì„± í‘œí˜„ ì¶”ê°€ ì ìˆ˜
        urgency_words = ['ê¸´ê¸‰', 'ì¦‰ì‹œ', 'ì‹ ì†', 'ì¡°ì†', 'ì‹œê¸‰']
        for word in urgency_words:
            if word in issue_lower:
                crisis_score += 2
                matched_keywords.append(f"{word}(ê¸´ê¸‰ì„±)")
        
        # ì ìˆ˜ ê¸°ë°˜ ë‹¨ê³„ ê²°ì •
        if crisis_score >= 15:
            level = "4ë‹¨ê³„ (ë¹„ìƒ)"
        elif crisis_score >= 10:
            level = "3ë‹¨ê³„ (ìœ„ê¸°)"
        elif crisis_score >= 5:
            level = "2ë‹¨ê³„ (ì£¼ì˜)"
        else:
            level = "1ë‹¨ê³„ (ê´€ì‹¬)"
        
        if return_details:
            return level, {
                'total_score': crisis_score,
                'matched_keywords': matched_keywords,
                'level': level
            }
        else:
            return level
    
    def _ai_based_crisis_assessment(self, issue_description: str, crisis_levels: Dict, preliminary_level: str) -> str:
        """AI ê¸°ë°˜ ì •ë°€ ìœ„ê¸° ë‹¨ê³„ íŒë‹¨"""
        
        # ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
        similar_cases = self.search_media_responses(issue_description, limit=5)
        similar_cases_info = ""
        
        if not similar_cases.empty:
            similar_cases_info = "\n=== ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€ ===\n"
            for idx, case in similar_cases.head(3).iterrows():
                similar_cases_info += f"- {case.get('ë‹¨ê³„', 'N/A')}: {case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ', 'N/A')[:100]}...\n"
        
        # ìœ„ê¸° ë‹¨ê³„ ì •ì˜ ì •ë³´ êµ¬ì„±
        crisis_definitions = ""
        for level, info in crisis_levels.items():
            crisis_definitions += f"\n{level}:\n"
            crisis_definitions += f"  - ì •ì˜: {info.get('ì •ì˜', 'N/A')}\n"
            crisis_definitions += f"  - ì„¤ëª…: {info.get('ì„¤ëª…', 'N/A')}\n"
            crisis_definitions += f"  - ì˜ˆì‹œ: {', '.join(info.get('ì˜ˆì‹œ', []))}\n"
        
        # AI íŒë‹¨ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        assessment_prompt = f"""
        ë‹¤ìŒ ì´ìŠˆì˜ ìœ„ê¸° ë‹¨ê³„ë¥¼ ì •í™•íˆ íŒë‹¨í•´ì£¼ì„¸ìš”:
        
        ì´ìŠˆ ë‚´ìš©: {issue_description}
        
        ì˜ˆë¹„ íŒë‹¨ ê²°ê³¼: {preliminary_level}
        
        === ìœ„ê¸° ë‹¨ê³„ ê¸°ì¤€ ===
        {crisis_definitions}
        
        {similar_cases_info}
        
        ìœ„ì˜ ê¸°ì¤€ê³¼ ê³¼ê±° ì‚¬ë¡€ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬, ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ì£¼ì„¸ìš”:
        - 1ë‹¨ê³„ (ê´€ì‹¬)
        - 2ë‹¨ê³„ (ì£¼ì˜)  
        - 3ë‹¨ê³„ (ìœ„ê¸°)
        - 4ë‹¨ê³„ (ë¹„ìƒ)
        
        ë‹µë³€ì€ ìœ„ê¸° ë‹¨ê³„ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        """
        
        try:
            # LLMì„ í†µí•œ ì •ë°€ íŒë‹¨
            ai_assessment = self.llm.chat(
                assessment_prompt, 
                system_prompt="ë‹¹ì‹ ì€ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ì˜ ìœ„ê¸° ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ìŠˆì˜ ì‹¬ê°ë„ë¥¼ ì •í™•íˆ íŒë‹¨í•˜ì—¬ ì ì ˆí•œ ìœ„ê¸° ë‹¨ê³„ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.",
                temperature=0.1  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì°½ì˜ì„± ì„¤ì •
            )
            
            # AI ì‘ë‹µì—ì„œ ìœ„ê¸° ë‹¨ê³„ ì¶”ì¶œ
            ai_level = self._extract_crisis_level_from_response(ai_assessment)
            
            # AI íŒë‹¨ì´ ìœ íš¨í•˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ì˜ˆë¹„ íŒë‹¨ ì‚¬ìš©
            if ai_level:
                return ai_level
            else:
                return preliminary_level
                
        except Exception as e:
            print(f"AI ê¸°ë°˜ ìœ„ê¸° ë‹¨ê³„ íŒë‹¨ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return preliminary_level
    
    def _extract_crisis_level_from_response(self, response: str) -> str:
        """AI ì‘ë‹µì—ì„œ ìœ„ê¸° ë‹¨ê³„ ì¶”ì¶œ"""
        if not response:
            return None
            
        response_lower = response.lower()
        
        # 4ë‹¨ê³„ë¶€í„° ì—­ìˆœìœ¼ë¡œ í™•ì¸ (ë†’ì€ ë‹¨ê³„ ìš°ì„ )
        if "4ë‹¨ê³„" in response_lower or "ë¹„ìƒ" in response_lower:
            return "4ë‹¨ê³„ (ë¹„ìƒ)"
        elif "3ë‹¨ê³„" in response_lower or "ìœ„ê¸°" in response_lower:
            return "3ë‹¨ê³„ (ìœ„ê¸°)"
        elif "2ë‹¨ê³„" in response_lower or "ì£¼ì˜" in response_lower:
            return "2ë‹¨ê³„ (ì£¼ì˜)"
        elif "1ë‹¨ê³„" in response_lower or "ê´€ì‹¬" in response_lower:
            return "1ë‹¨ê³„ (ê´€ì‹¬)"
        else:
            return None
    
    def _format_departments(self, departments: List[Dict]) -> str:
        """ë¶€ì„œ ì •ë³´ êµ¬ì²´ì  í¬ë§·íŒ… (ê°œì„ ëœ ë²„ì „)"""
        if not departments:
            return "ê´€ë ¨ ë¶€ì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for i, dept in enumerate(departments, 1):
            dept_info = f"â–  {dept.get('ë¶€ì„œëª…', 'N/A')} ({i}ìˆœìœ„)"
            
            # ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´
            if dept.get('ë‹´ë‹¹ì'):
                dept_info += f"\n  - ë‹´ë‹¹ì: {dept['ë‹´ë‹¹ì']}"
            if dept.get('ì—°ë½ì²˜'):
                dept_info += f"\n  - ì—°ë½ì²˜: {dept['ì—°ë½ì²˜']}"
            if dept.get('ì´ë©”ì¼'):
                dept_info += f"\n  - ì´ë©”ì¼: {dept['ì´ë©”ì¼']}"
            
            # ë‹´ë‹¹ ì˜ì—­ ì •ë³´
            if dept.get('ë‹´ë‹¹ì´ìŠˆ'):
                issues = ', '.join(dept['ë‹´ë‹¹ì´ìŠˆ']) if isinstance(dept['ë‹´ë‹¹ì´ìŠˆ'], list) else dept['ë‹´ë‹¹ì´ìŠˆ']
                dept_info += f"\n  - ë‹´ë‹¹ì´ìŠˆ: {issues}"
            
            # ë§¤ì¹­ ì •ë³´ (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì œê³µ)
            if dept.get('ê´€ë ¨ì„±ì ìˆ˜'):
                dept_info += f"\n  - ê´€ë ¨ì„±ì ìˆ˜: {dept['ê´€ë ¨ì„±ì ìˆ˜']}ì "
            
            if dept.get('ë§¤ì¹­í•­ëª©'):
                matching_details = ', '.join(dept['ë§¤ì¹­í•­ëª©'])
                dept_info += f"\n  - ë§¤ì¹­ê·¼ê±°: {matching_details}"
            
            # ë¶€ì„œ ìš°ì„ ìˆœìœ„
            if dept.get('ìš°ì„ ìˆœìœ„'):
                dept_info += f"\n  - ì¡°ì§ìš°ì„ ìˆœìœ„: {dept['ìš°ì„ ìˆœìœ„']}"
                
            formatted.append(dept_info)
        
        return "\n\n".join(formatted)
    
    def _format_cases(self, cases: pd.DataFrame) -> str:
        """ì‚¬ë¡€ ì •ë³´ êµ¬ì²´ì  í¬ë§·íŒ…"""
        if cases.empty:
            return "ê´€ë ¨ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for idx, case in cases.iterrows():
            case_info = f"â–  ì‚¬ë¡€ {idx + 1}"
            case_info += f"\n  - ì¼ì‹œ: {case.get('ë°œìƒ ì¼ì‹œ', 'N/A')}"
            case_info += f"\n  - ë‹¨ê³„: {case.get('ë‹¨ê³„', 'N/A')}"
            case_info += f"\n  - ìœ í˜•: {case.get('ë°œìƒ ìœ í˜•', 'N/A')}"
            
            if 'ëŒ€ì‘ ê²°ê³¼' in case and pd.notna(case['ëŒ€ì‘ ê²°ê³¼']):
                result = str(case['ëŒ€ì‘ ê²°ê³¼'])[:100] + "..." if len(str(case['ëŒ€ì‘ ê²°ê³¼'])) > 100 else case['ëŒ€ì‘ ê²°ê³¼']
                case_info += f"\n  - ëŒ€ì‘ê²°ê³¼: {result}"
                
            if 'ì´ìŠˆ ë°œìƒ ë³´ê³ ' in case and pd.notna(case['ì´ìŠˆ ë°œìƒ ë³´ê³ ']):
                issue = str(case['ì´ìŠˆ ë°œìƒ ë³´ê³ '])[:80] + "..." if len(str(case['ì´ìŠˆ ë°œìƒ ë³´ê³ '])) > 80 else case['ì´ìŠˆ ë°œìƒ ë³´ê³ ']
                case_info += f"\n  - ì´ìŠˆë‚´ìš©: {issue}"
                
            formatted.append(case_info)
        
        return "\n\n".join(formatted[:3])  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
    
    def research_issue_with_web_search(self, issue_description: str) -> str:
        """
        ë°œìƒ ì´ìŠˆì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì¢…í•© ì—°êµ¬
        ì–¸ë¡  ì´ê´„ë‹´ë‹¹ìë¥¼ ìœ„í•œ ë°°ê²½ ì •ë³´ ë° ëŒ€ì‘ ì „ëµ ì œê³µ
        
        Args:
            issue_description (str): ë°œìƒí•œ ì´ìŠˆ ì„¤ëª…
        
        Returns:
            str: ì–¸ë¡  ì´ê´„ë‹´ë‹¹ììš© ì´ìŠˆ ì—°êµ¬ ë³´ê³ ì„œ
        """
        if not self.research_service:
            return "âš ï¸ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„¤ì´ë²„ API ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        print(f"RESEARCH: ì´ìŠˆ ì—°êµ¬ ì‹œì‘: {issue_description}")
        
        # 1. ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì •ë³´ ìˆ˜ì§‘
        research_data = self.research_service.research_issue(issue_description)
        
        # 2. ê¸°ì¡´ ë‚´ë¶€ ë°ì´í„°ì™€ ì—°ê²°
        internal_context = self._get_internal_context_for_issue(issue_description)
        
        # 3. LLMì„ í†µí•œ ì¢…í•© ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
        report = self._generate_issue_research_report(research_data, internal_context, issue_description)
        
        return report
    
    def _get_internal_context_for_issue(self, issue_description: str) -> Dict[str, Any]:
        """ì´ìŠˆì™€ ê´€ë ¨ëœ ë‚´ë¶€ ë°ì´í„° ìˆ˜ì§‘"""
        context = {
            "relevant_departments": [],
            "similar_cases": [],
            "response_strategies": []
        }
        
        try:
            # ê´€ë ¨ ë¶€ì„œ ì°¾ê¸°
            context["relevant_departments"] = self.get_relevant_departments(issue_description)
            
            # ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€ ì°¾ê¸°
            if self.media_response_data is not None:
                context["similar_cases"] = self._find_similar_media_cases(issue_description)
            
            # ëŒ€ì‘ ì „ëµ ì¶”ì²œ
            context["response_strategies"] = self._get_response_strategies(issue_description)
            
        except Exception as e:
            print(f"WARNING: ë‚´ë¶€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return context
    
    def _find_similar_media_cases(self, issue_description: str) -> List[Dict]:
        """ìœ ì‚¬í•œ ì–¸ë¡ ëŒ€ì‘ ì‚¬ë¡€ ê²€ìƒ‰"""
        if self.media_response_data is None or len(self.media_response_data) == 0:
            return []
        
        similar_cases = []
        issue_keywords = issue_description.lower().split()
        
        for _, case in self.media_response_data.iterrows():
            case_dict = case.to_dict()
            
            # ì´ìŠˆ ë‚´ìš© ë¹„êµ
            if pd.notna(case.get('ì´ìŠˆ ë°œìƒ ë³´ê³ ')):
                case_issue = str(case['ì´ìŠˆ ë°œìƒ ë³´ê³ ']).lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                match_score = sum(1 for keyword in issue_keywords if keyword in case_issue)
                
                if match_score > 0:
                    case_dict['relevance_score'] = match_score
                    similar_cases.append(case_dict)
        
        # ê´€ë ¨ë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œ ë°˜í™˜
        similar_cases.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        return similar_cases[:3]
    
    def _get_response_strategies(self, issue_description: str) -> List[str]:
        """ì´ìŠˆ ìœ í˜•ì— ë”°ë¥¸ ëŒ€ì‘ ì „ëµ ì œì•ˆ"""
        strategies = []
        issue_lower = issue_description.lower()
        
        # ìœ„ê¸° ìˆ˜ì¤€ë³„ ê¸°ë³¸ ì „ëµ
        if any(word in issue_lower for word in ['í™”ì¬', 'í­ë°œ', 'ì‚¬ê³ ', 'ì•ˆì „', 'ìœ„í—˜']):
            strategies.extend([
                "ì¦‰ì‹œ ì•ˆì „ í™•ë³´ ë° í”¼í•´ ìµœì†Œí™” ì¡°ì¹˜",
                "ê´€ê³„ ê¸°ê´€ ì‹ ì† ë³´ê³  ë° í˜‘ì¡° ì²´ê³„ êµ¬ì¶•",
                "ì •í™•í•œ ì‚¬ì‹¤ ê´€ê³„ íŒŒì•… í›„ íˆ¬ëª…í•œ ì •ë³´ ê³µê°œ"
            ])
        
        if any(word in issue_lower for word in ['í™˜ê²½', 'ì˜¤ì—¼', 'ë°°ì¶œ']):
            strategies.extend([
                "í™˜ê²½ ì˜í–¥ ìµœì†Œí™”ë¥¼ ìœ„í•œ ì¦‰ì‹œ ì¡°ì¹˜",
                "í™˜ê²½ ë‹¹êµ­ê³¼ì˜ í˜‘ë ¥ì  ëŒ€ì‘",
                "í™˜ê²½ ë³µì› ê³„íš ìˆ˜ë¦½ ë° ê³µê°œ"
            ])
        
        if any(word in issue_lower for word in ['í’ˆì§ˆ', 'ë¦¬ì½œ', 'ê²°í•¨']):
            strategies.extend([
                "ê³ ê° ì•ˆì „ ìµœìš°ì„  ì›ì¹™ í•˜ì— ì‹ ì† ëŒ€ì‘",
                "íˆ¬ëª…í•œ ì›ì¸ ì¡°ì‚¬ ë° ê²°ê³¼ ê³µê°œ",
                "ì¬ë°œ ë°©ì§€ ëŒ€ì±… ìˆ˜ë¦½ ë° ì´í–‰"
            ])
        
        # ê¸°ë³¸ ì „ëµì´ ì—†ì„ ê²½ìš° ì¼ë°˜ì ì¸ ìœ„ê¸° ëŒ€ì‘ ì „ëµ ì œê³µ
        if not strategies:
            strategies = [
                "ì •í™•í•œ ì‚¬ì‹¤ ê´€ê³„ íŒŒì•… ë° ê²€ì¦",
                "ì´í•´ê´€ê³„ìë³„ ë§ì¶¤ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ëµ ìˆ˜ë¦½",
                "ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ë° í›„ì† ì¡°ì¹˜ ê³„íš"
            ]
        
        return strategies
    
    def _generate_issue_research_report(self, research_data: Dict, internal_context: Dict, original_issue: str) -> str:
        """ì¢…í•© ì´ìŠˆ ì—°êµ¬ ë³´ê³ ì„œ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì–¸ë¡  ì´ê´„ ë‹´ë‹¹ìì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë°œìƒ ì´ìŠˆì— ëŒ€í•œ ì¢…í•©ì ì¸ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë°œìƒ ì´ìŠˆ:** {original_issue}

**ì›¹ ê²€ìƒ‰ ê²°ê³¼:**
- ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {research_data['analysis_summary']['total_sources']}ê±´
- ë‰´ìŠ¤ ê¸°ì‚¬: {research_data['analysis_summary']['news_count']}ê±´  
- ë¸”ë¡œê·¸/í¬ëŸ¼: {research_data['analysis_summary']['blog_count']}ê±´
- ì–¸ë¡  ê´€ì‹¬ë„: {research_data['analysis_summary']['coverage_level']}

**ì£¼ìš” ë‰´ìŠ¤ í—¤ë“œë¼ì¸:**
{self._format_news_headlines(research_data.get('news_results', []))}

**ë‚´ë¶€ ê´€ë ¨ ì •ë³´:**
- ë‹´ë‹¹ ë¶€ì„œ: {len(internal_context.get('relevant_departments', []))}ê°œ ë¶€ì„œ
- ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€: {len(internal_context.get('similar_cases', []))}ê±´
- ê¶Œì¥ ëŒ€ì‘ ì „ëµ: {len(internal_context.get('response_strategies', []))}ê°œ

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ **ì–¸ë¡  ì´ê´„ë‹´ë‹¹ììš© ì´ìŠˆ ì—°êµ¬ ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“‹ ì´ìŠˆ ì—°êµ¬ ë³´ê³ ì„œ

### 1. ì´ìŠˆ ê°œìš”
- ì´ìŠˆ ì„±ê²© ë° ì‹¬ê°ë„
- í˜„ì¬ ì–¸ë¡  ê´€ì‹¬ë„ ë° ë³´ë„ ë™í–¥

### 2. ì™¸ë¶€ ì—¬ë¡  í˜„í™©
- ì£¼ìš” ì–¸ë¡  ë³´ë„ ë‚´ìš© ë¶„ì„
- ì—¬ë¡ ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ ë° ìš°ë ¤ì‚¬í•­
- í–¥í›„ ë³´ë„ í™•ì‚° ê°€ëŠ¥ì„±

### 3. ë‚´ë¶€ ëŒ€ì‘ í˜„í™©  
- ê´€ë ¨ ë‹´ë‹¹ ë¶€ì„œ ë° ì—­í• 
- ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ëŒ€ì‘ ê²½í—˜
- í˜„ì¬ ëŒ€ì‘ ì²´ê³„ ì ê²€ ì‚¬í•­

### 4. ì–¸ë¡  ëŒ€ì‘ ì „ëµ ê¶Œê³ 
- ì¦‰ì‹œ ëŒ€ì‘ ì‚¬í•­ (24ì‹œê°„ ë‚´)
- ë‹¨ê¸° ëŒ€ì‘ ê³„íš (1ì£¼ì¼ ë‚´)
- ì¤‘ì¥ê¸° ì´ë¯¸ì§€ ê´€ë¦¬ ë°©ì•ˆ

### 5. ì£¼ì˜ì‚¬í•­ ë° ë¦¬ìŠ¤í¬
- ì ì¬ì  2ì°¨ ì´ìŠˆ ê°€ëŠ¥ì„±
- ì–¸ë¡  ëŒ€ì‘ ì‹œ ì£¼ì˜ì‚¬í•­
- ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸

ë³´ê³ ì„œëŠ” ê°„ê²°í•˜ê³  ì‹¤ë¬´ì ìœ¼ë¡œ ì‘ì„±í•˜ë˜, êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆì„ í¬í•¨í•´ì£¼ì„¸ìš”.
"""
        
        # LLM í˜¸ì¶œí•˜ì—¬ ë³´ê³ ì„œ ìƒì„±
        try:
            report = self.llm.chat(prompt)
            return report
        except Exception as e:
            return f"âš ï¸ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _format_news_headlines(self, news_results: List[Dict]) -> str:
        """ë‰´ìŠ¤ í—¤ë“œë¼ì¸ í¬ë§·íŒ…"""
        if not news_results:
            return "ê²€ìƒ‰ëœ ë‰´ìŠ¤ ì—†ìŒ"
        
        headlines = []
        for i, news in enumerate(news_results[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
            headline = news.get('title', 'ì œëª© ì—†ìŒ')
            pub_date = news.get('pub_date', '')
            headlines.append(f"{i}. {headline} ({pub_date})")
        
        return "\n".join(headlines)

def main():
    """ë°ì´í„° ê¸°ë°˜ LLM í…ŒìŠ¤íŠ¸"""
    print("=== ë°ì´í„° ê¸°ë°˜ LLM ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\n")
    
    # ë°ì´í„° ê¸°ë°˜ LLM ì´ˆê¸°í™”
    data_llm = DataBasedLLM()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆì˜ë“¤
    test_queries = [
        "ë°°ë‹¹ ê´€ë ¨ ì´ìŠˆê°€ ë°œìƒí–ˆì„ ë•Œ ëˆ„êµ¬ì—ê²Œ ì—°ë½í•´ì•¼ í•˜ë‚˜ìš”?",
        "ì „ê¸°ì°¨ ê´€ë ¨ ì–¸ë¡  ë³´ë„ì— ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í•˜ë‚˜ìš”?",
        "ìµœê·¼ ì–¸ë¡ ëŒ€ì‘ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"{i}. ì§ˆë¬¸: {query}")
        print("-" * 50)
        
        if "íŠ¸ë Œë“œ" in query:
            response = data_llm.analyze_issue_trends(30)
        else:
            response = data_llm.generate_data_based_response(query)
        
        print(f"ë‹µë³€: {response}\n")
        print("=" * 80 + "\n")

if __name__ == "__main__":
    main()
