"""
ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ì—°êµ¬ ì„œë¹„ìŠ¤
ë„¤ì´ë²„ ë‰´ìŠ¤(ê´€ë ¨ì„±ìˆœ) + ê³µì‹ ë ¥ ìˆëŠ” ì‚¬ì´íŠ¸ ê²€ìƒ‰
"""

import os
import requests
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
from dotenv import load_dotenv
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

class EnhancedResearchService:
    """ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ì—°êµ¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        load_dotenv()
        self.naver_client_id = os.getenv('NAVER_CLIENT_ID')
        self.naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
        if not self.naver_client_id or not self.naver_client_secret:
            raise ValueError("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        self.naver_headers = {
            'X-Naver-Client-Id': self.naver_client_id,
            'X-Naver-Client-Secret': self.naver_client_secret
        }
        
        # ê³µì‹ ë ¥ ìˆëŠ” ì‚¬ì´íŠ¸ URL íŒ¨í„´
        self.official_sites = {
            "posco_international": {
                "base_url": "https://www.poscointl.com",
                "search_patterns": [
                    "/kr/company/news",
                    "/kr/company/announcement"
                ]
            },
            "dart": {
                "base_url": "https://dart.fss.or.kr",
                "api_url": "https://opendart.fss.or.kr/api/list.json"
            },
            "korea_exchange": {
                "base_url": "https://kind.krx.co.kr",
                "search_url": "https://kind.krx.co.kr/disclosure/searchtotalinfo.do"
            }
        }
    
    def research_issue_comprehensive(self, issue_description: str) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ ë‹¤ì¤‘ ì†ŒìŠ¤ ì´ìŠˆ ì—°êµ¬
        ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
        """
        print("START: ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ì—°êµ¬ ì‹œì‘")
        
        start_time = time.time()
        search_query = self._optimize_search_query(issue_description)
        
        research_data = {
            "original_issue": issue_description,
            "search_query": search_query,
            "timestamp": datetime.now().isoformat(),
            "sources": {
                "naver_news": [],
                "posco_official": [],
                "dart_filings": [],
                "krx_disclosures": []
            },
            "analysis_summary": {}
        }
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_tasks = {
                executor.submit(self._search_naver_news, search_query): "naver_news",
                executor.submit(self._search_posco_official, search_query): "posco_official", 
                executor.submit(self._search_dart_filings, search_query): "dart_filings",
                executor.submit(self._search_krx_disclosures, search_query): "krx_disclosures"
            }
            
            print("PROCESSING: 4ê°œ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            
            for future in as_completed(future_tasks):
                source_name = future_tasks[future]
                try:
                    result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                    research_data["sources"][source_name] = result
                    print(f"SUCCESS: {source_name} ê²€ìƒ‰ ì™„ë£Œ ({len(result)}ê±´)")
                except Exception as e:
                    print(f"WARNING: {source_name} ê²€ìƒ‰ ì‹¤íŒ¨ - {str(e)}")
                    research_data["sources"][source_name] = []
        
        # ê²€ìƒ‰ ê²°ê³¼ ì¢…í•© ë¶„ì„
        research_data["analysis_summary"] = self._analyze_comprehensive_results(research_data)
        
        processing_time = time.time() - start_time
        print(f"COMPLETE: ë‹¤ì¤‘ ì†ŒìŠ¤ ì—°êµ¬ ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        
        return research_data
    
    def _search_naver_news(self, query: str) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê´€ë ¨ì„±ìˆœ)"""
        try:
            url = "https://openapi.naver.com/v1/search/news.json"
            params = {
                'query': query,
                'display': 20,  # ë” ë§ì€ ê²°ê³¼
                'sort': 'sim'   # ê´€ë ¨ì„±ìˆœìœ¼ë¡œ ë³€ê²½
            }
            
            response = requests.get(url, headers=self.naver_headers, params=params, timeout=15)
            response.raise_for_status()
            
            results = []
            items = response.json().get("items", [])
            
            for item in items:
                processed_item = {
                    "title": self._clean_html_tags(item.get("title", "")),
                    "link": item.get("link", ""),
                    "description": self._clean_html_tags(item.get("description", "")),
                    "pub_date": item.get("pubDate", ""),
                    "source": "ë„¤ì´ë²„ë‰´ìŠ¤",
                    "relevance_score": self._calculate_relevance_score(item, query)
                }
                results.append(processed_item)
            
            # ê´€ë ¨ì„± ì ìˆ˜ìˆœ ì •ë ¬
            return sorted(results, key=lambda x: x['relevance_score'], reverse=True)
            
        except Exception as e:
            print(f"ERROR: ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ - {str(e)}")
            return []
    
    def _search_posco_official(self, query: str) -> List[Dict]:
        """í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê³µì‹ ì‚¬ì´íŠ¸ ê²€ìƒ‰"""
        try:
            results = []
            base_url = self.official_sites["posco_international"]["base_url"]
            
            # ê³µì‹ ë‰´ìŠ¤/ê³µê³  í˜ì´ì§€ ê²€ìƒ‰
            search_urls = [
                f"{base_url}/kr/company/news",
                f"{base_url}/kr/company/announcement"
            ]
            
            for search_url in search_urls:
                try:
                    response = requests.get(search_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})
                    if response.status_code == 200:
                        # ê°„ë‹¨í•œ HTML íŒŒì‹± (BeautifulSoup ì—†ì´)
                        html_content = response.text
                        
                        # ì œëª© íŒ¨í„´ ê²€ìƒ‰ (ê°„ë‹¨í•œ ì •ê·œì‹ ì‚¬ìš©)
                        title_patterns = [
                            r'<title[^>]*>([^<]+)</title>',
                            r'<h[1-6][^>]*>([^<]*' + re.escape(query) + r'[^<]*)</h[1-6]>',
                            r'<a[^>]*>([^<]*' + re.escape(query) + r'[^<]*)</a>'
                        ]
                        
                        for pattern in title_patterns:
                            matches = re.findall(pattern, html_content, re.IGNORECASE)
                            for match in matches[:5]:  # ìµœëŒ€ 5ê°œ
                                clean_title = re.sub(r'<[^>]+>', '', match).strip()
                                if clean_title and len(clean_title) > 10:
                                    results.append({
                                        "title": clean_title[:100],
                                        "link": search_url,
                                        "description": f"í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                                        "source": "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê³µì‹",
                                        "type": "official_announcement"
                                    })
                                
                except Exception as e:
                    print(f"WARNING: í¬ìŠ¤ì½” ê³µì‹ ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ ({search_url}) - {str(e)}")
                    continue
            
            return results
            
        except Exception as e:
            print(f"ERROR: í¬ìŠ¤ì½” ê³µì‹ ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨ - {str(e)}")
            return []
    
    def _search_dart_filings(self, query: str) -> List[Dict]:
        """DART ê³µì‹œ ê²€ìƒ‰"""
        try:
            # DART APIëŠ” ë³„ë„ ì¸ì¦ì´ í•„ìš”í•˜ë¯€ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ì‹ ì‚¬ìš©
            results = []
            
            # í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ
            posco_keywords = ["í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„", "POSCO INTERNATIONAL", "í¬ìŠ¤ì½”í™€ë”©ìŠ¤"]
            
            for keyword in posco_keywords:
                if keyword.lower() in query.lower():
                    # DART ì „ìê³µì‹œì‹œìŠ¤í…œì—ì„œ í•´ë‹¹ ê¸°ì—… ê³µì‹œ ì •ë³´ ìˆ˜ì§‘
                    dart_url = "https://dart.fss.or.kr/dsaf001/main.do"
                    
                    try:
                        response = requests.get(dart_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
                        if response.status_code == 200:
                            results.append({
                                "title": f"{keyword} ê´€ë ¨ DART ê³µì‹œ ì •ë³´",
                                "link": dart_url,
                                "description": "ì „ìê³µì‹œì‹œìŠ¤í…œì—ì„œ ìµœì‹  ê³µì‹œ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                                "source": "DART ì „ìê³µì‹œ",
                                "type": "regulatory_filing"
                            })
                    except Exception as e:
                        print(f"WARNING: DART ì ‘ê·¼ ì‹¤íŒ¨ - {str(e)}")
            
            return results
            
        except Exception as e:
            print(f"ERROR: DART ê²€ìƒ‰ ì‹¤íŒ¨ - {str(e)}")
            return []
    
    def _search_krx_disclosures(self, query: str) -> List[Dict]:
        """í•œêµ­ê±°ë˜ì†Œ ê³µì‹œ ê²€ìƒ‰"""
        try:
            results = []
            
            # í•œêµ­ê±°ë˜ì†Œ ê³µì‹œ ì‚¬ì´íŠ¸
            krx_url = "https://kind.krx.co.kr"
            
            try:
                response = requests.get(krx_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
                if response.status_code == 200:
                    results.append({
                        "title": "í•œêµ­ê±°ë˜ì†Œ í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê³µì‹œì •ë³´",
                        "link": krx_url + "/disclosure/searchtotalinfo.do",
                        "description": "í•œêµ­ê±°ë˜ì†Œì—ì„œ ì œê³µí•˜ëŠ” ê³µì‹œì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                        "source": "í•œêµ­ê±°ë˜ì†Œ",
                        "type": "exchange_disclosure"
                    })
            except Exception as e:
                print(f"WARNING: í•œêµ­ê±°ë˜ì†Œ ì ‘ê·¼ ì‹¤íŒ¨ - {str(e)}")
            
            return results
            
        except Exception as e:
            print(f"ERROR: í•œêµ­ê±°ë˜ì†Œ ê²€ìƒ‰ ì‹¤íŒ¨ - {str(e)}")
            return []
    
    def _optimize_search_query(self, issue_description: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        query = issue_description.strip()
        
        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        remove_phrases = ["ë°œìƒí•œ", "ì´ìŠˆ", "ë¬¸ì œ", "ìƒí™©", "ê´€ë ¨", "ëŒ€í•´ì„œ", "ì— ëŒ€í•œ", "ê²€í† "]
        for phrase in remove_phrases:
            query = query.replace(phrase, "")
        
        # í•µì‹¬ í‚¤ì›Œë“œ ê°•í™”
        if "í¬ìŠ¤ì½”" in query:
            query = "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ " + query
        
        return " ".join(query.split())
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean_text = re.sub(r'<[^>]+>', '', text)
        clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
        return clean_text.strip()
    
    def _calculate_relevance_score(self, item: Dict, query: str) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        title = item.get("title", "").lower()
        description = item.get("description", "").lower()
        query_lower = query.lower()
        
        score = 0.0
        
        # ì œëª©ì—ì„œ ì¿¼ë¦¬ í‚¤ì›Œë“œ ë§¤ì¹­
        query_words = query_lower.split()
        for word in query_words:
            if word in title:
                score += 2.0
            if word in description:
                score += 1.0
        
        # í¬ìŠ¤ì½” ê´€ë ¨ ê°€ì¤‘ì¹˜
        if "í¬ìŠ¤ì½”" in title:
            score += 3.0
        
        return score
    
    def _analyze_comprehensive_results(self, research_data: Dict) -> Dict[str, Any]:
        """ì¢…í•© ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
        sources = research_data["sources"]
        
        total_sources = sum(len(results) for results in sources.values())
        
        analysis = {
            "total_sources": total_sources,
            "source_breakdown": {
                "naver_news": len(sources["naver_news"]),
                "posco_official": len(sources["posco_official"]),
                "dart_filings": len(sources["dart_filings"]),
                "krx_disclosures": len(sources["krx_disclosures"])
            },
            "credibility_level": self._assess_credibility_level(sources),
            "official_sources_available": len(sources["posco_official"]) + len(sources["dart_filings"]) + len(sources["krx_disclosures"]) > 0,
            "research_completeness": "ì™„ë£Œ" if total_sources > 0 else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        }
        
        return analysis
    
    def _assess_credibility_level(self, sources: Dict) -> str:
        """ì‹ ë¢°ë„ ìˆ˜ì¤€ í‰ê°€"""
        official_count = len(sources["posco_official"]) + len(sources["dart_filings"]) + len(sources["krx_disclosures"])
        news_count = len(sources["naver_news"])
        
        if official_count >= 3:
            return "ë§¤ìš° ë†’ìŒ (ë‹¤ìˆ˜ ê³µì‹ ì†ŒìŠ¤)"
        elif official_count >= 1:
            return "ë†’ìŒ (ê³µì‹ ì†ŒìŠ¤ í¬í•¨)"
        elif news_count >= 10:
            return "ì¤‘ê°„ (ë‹¤ìˆ˜ ì–¸ë¡  ë³´ë„)"
        elif news_count >= 3:
            return "ë³´í†µ (ì¼ë¶€ ì–¸ë¡  ë³´ë„)"
        else:
            return "ë‚®ìŒ (ì œí•œì  ì •ë³´)"

def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    try:
        research_service = EnhancedResearchService()
        
        test_issue = "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ 2ì°¨ì „ì§€ ì†Œì¬ ë¦¬íŠ¬ ë°°í„°ë¦¬ ê²°í•¨ ì „ê¸°ì°¨ ë¦¬ì½œ"
        
        print("=== ê°•í™”ëœ ë‹¤ì¤‘ ì†ŒìŠ¤ ì—°êµ¬ í…ŒìŠ¤íŠ¸ ===")
        results = research_service.research_issue_comprehensive(test_issue)
        
        print(f"\nğŸ“Š ì¢…í•© ì—°êµ¬ ê²°ê³¼:")
        print(f"- ê²€ìƒ‰ ì¿¼ë¦¬: {results['search_query']}")
        print(f"- ì „ì²´ ì†ŒìŠ¤: {results['analysis_summary']['total_sources']}ê±´")
        print(f"- ì‹ ë¢°ë„ ìˆ˜ì¤€: {results['analysis_summary']['credibility_level']}")
        print(f"- ê³µì‹ ì†ŒìŠ¤ í™•ë³´: {'ì˜ˆ' if results['analysis_summary']['official_sources_available'] else 'ì•„ë‹ˆì˜¤'}")
        
        for source_name, count in results['analysis_summary']['source_breakdown'].items():
            print(f"- {source_name}: {count}ê±´")
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()