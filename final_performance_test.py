#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœì¢… ì„±ëŠ¥ í‰ê°€ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
from datetime import datetime

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def run_final_performance_test():
    """ìµœì¢… ì„±ëŠ¥ í‰ê°€"""
    
    print("=== ìµœì¢… ì„±ëŠ¥ í‰ê°€ í…ŒìŠ¤íŠ¸ ===")
    print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    try:
        from data_based_llm import DataBasedLLM
        
        # ë‹¤ì–‘í•œ ìœ í˜•ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_cases = [
            {
                "name": "ì‹ëŸ‰ì‚¬ì—…_ë¬¸ì˜",
                "media": "ì¡°ì„ ì¼ë³´", "reporter": "ê¹€ì¡°ì„ ",
                "issue": "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì‹ëŸ‰ì‚¬ì—… ìƒì‚°ì§€, ì£¼ìš” ë‚©í’ˆì²˜, ì˜¬í•´ ë§¤ì¶œ ê³„íš ê´€ë ¨ ë¬¸ì˜",
                "expected_score": 90
            },
            {
                "name": "ì‹¤ì _ë¬¸ì˜",
                "media": "ì¡°ì„ ì¼ë³´", "reporter": "ê¹€ì¡°ì„ ", 
                "issue": "2025ë…„ 2ë¶„ê¸° í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì£¼ìš”ì‚¬ì—…ë³„ ì‹¤ì ê³¼ í–¥í›„ ê³„íš ê´€ë ¨ ë¬¸ì˜",
                "expected_score": 85
            },
            {
                "name": "ì² ê°•ì‚¬ì—…_ë¬¸ì˜",
                "media": "ë§¤ì¼ê²½ì œ", "reporter": "ì´ê¸°ì",
                "issue": "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì² ê°•ì‚¬ì—…ë¶€ ì˜¬í•´ ì‹¤ì  ë° í–¥í›„ ê³„íš ë¬¸ì˜",
                "expected_score": 80
            },
            {
                "name": "í™˜ê²½ì•ˆì „_ì´ìŠˆ",
                "media": "í•œêµ­ê²½ì œ", "reporter": "ë°•ê¸°ì",
                "issue": "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ í•´ì™¸ì‚¬ì—…ì¥ í™˜ê²½ì•ˆì „ ê´€ë¦¬ í˜„í™© ë° ê°œì„  ê³„íš",
                "expected_score": 75
            },
            {
                "name": "ì‹ ì‚¬ì—…_ë¬¸ì˜",
                "media": "ì„œìš¸ê²½ì œ", "reporter": "ìµœê¸°ì",
                "issue": "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì´ì°¨ì „ì§€ ì†Œì¬ ì‚¬ì—… ì§„ì¶œ ê³„íš ë° íˆ¬ì ê·œëª¨",
                "expected_score": 70
            }
        ]
        
        llm = DataBasedLLM()
        print("SUCCESS: ìµœì¢… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print()
        
        performance_results = []
        total_processing_time = 0
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"=== TEST {i}/5: {test_case['name']} ===")
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            
            report = llm.generate_comprehensive_issue_report(
                test_case["media"],
                test_case["reporter"],
                test_case["issue"],
                mode="enhanced"
            )
            
            processing_time = time.time() - start_time
            total_processing_time += processing_time
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = evaluate_report_quality(report, test_case)
            
            result = {
                "name": test_case["name"],
                "processing_time": processing_time,
                "quality_score": quality_score,
                "expected_score": test_case["expected_score"],
                "performance_ratio": quality_score / test_case["expected_score"] * 100,
                "report_length": len(report),
                "report": report
            }
            
            performance_results.append(result)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100 (ëª©í‘œ: {test_case['expected_score']})")
            print(f"ëª©í‘œ ë‹¬ì„±ë¥ : {result['performance_ratio']:.1f}%")
            
            if result['performance_ratio'] >= 100:
                print("í‰ê°€: ëª©í‘œ ë‹¬ì„±")
            elif result['performance_ratio'] >= 80:
                print("í‰ê°€: ì–‘í˜¸")
            else:
                print("í‰ê°€: ê°œì„  í•„ìš”")
            print()
        
        # ì „ì²´ ì„±ëŠ¥ ë¶„ì„
        avg_processing_time = total_processing_time / len(test_cases)
        avg_quality_score = sum(r['quality_score'] for r in performance_results) / len(performance_results)
        avg_performance_ratio = sum(r['performance_ratio'] for r in performance_results) / len(performance_results)
        
        print("=== ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ===")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
        print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality_score:.1f}/100")
        print(f"í‰ê·  ëª©í‘œ ë‹¬ì„±ë¥ : {avg_performance_ratio:.1f}%")
        
        # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
        if avg_performance_ratio >= 95:
            performance_grade = "Sê¸‰ - ì´ìƒì  ì‚¬ë¡€ ìˆ˜ì¤€"
        elif avg_performance_ratio >= 85:
            performance_grade = "Aê¸‰ - ìš°ìˆ˜"
        elif avg_performance_ratio >= 75:
            performance_grade = "Bê¸‰ - ì–‘í˜¸"
        elif avg_performance_ratio >= 65:
            performance_grade = "Cê¸‰ - ë³´í†µ"
        else:
            performance_grade = "Dê¸‰ - ê°œì„  í•„ìš”"
        
        print(f"ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰: {performance_grade}")
        
        # ì†ë„ í‰ê°€
        if avg_processing_time <= 3.0:
            speed_grade = "ë§¤ìš° ë¹ ë¦„"
        elif avg_processing_time <= 5.0:
            speed_grade = "ë¹ ë¦„"
        elif avg_processing_time <= 10.0:
            speed_grade = "ë³´í†µ"
        else:
            speed_grade = "ëŠë¦¼"
        
        print(f"ì²˜ë¦¬ ì†ë„ ë“±ê¸‰: {speed_grade}")
        
        # ê²°ê³¼ ì €ì¥
        save_final_performance_result(performance_results, avg_processing_time, 
                                    avg_quality_score, avg_performance_ratio, performance_grade)
        
        return True, performance_results, avg_performance_ratio
        
    except Exception as e:
        print(f"ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, None, 0

def evaluate_report_quality(report, test_case):
    """ë³´ê³ ì„œ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
    
    quality_metrics = {
        "template_compliance": evaluate_template_compliance(report),
        "content_quality": evaluate_content_quality(report, test_case),
        "professionalism": evaluate_professionalism(report, test_case),
        "usability": evaluate_usability(report)
    }
    
    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    weights = {
        "template_compliance": 0.25,  # 25%
        "content_quality": 0.35,      # 35%
        "professionalism": 0.25,      # 25%
        "usability": 0.15            # 15%
    }
    
    weighted_score = sum(score * weights[metric] for metric, score in quality_metrics.items())
    
    return weighted_score

def evaluate_template_compliance(report):
    """í…œí”Œë¦¿ ì¤€ìˆ˜ë„ í‰ê°€"""
    required_sections = [
        "1. ë°œìƒ ì¼ì‹œ:",
        "2. ë°œìƒ ë‹¨ê³„:" if "2. ë°œìƒ ë‹¨ê³„:" in report else "2. ëŒ€ì‘ ë‹¨ê³„:",
        "3. ë°œìƒ ë‚´ìš©:",
        "4. ìœ ê´€ ì˜ê²¬:",
        "5. ëŒ€ì‘ ë°©ì•ˆ:",
        "6. ëŒ€ì‘ ê²°ê³¼:"
    ]
    
    found_count = sum(1 for section in required_sections if section in report)
    score = (found_count / len(required_sections)) * 100
    
    # ì¶”ê°€ ì ìˆ˜: ë‚ ì§œ í˜•ì‹
    if "." in report and ("ë…„" not in report[:100] or report.count(".") > 3):
        score += 5
    
    return min(score, 100)

def evaluate_content_quality(report, test_case):
    """ë‚´ìš© í’ˆì§ˆ í‰ê°€"""
    score = 0
    
    # ìœ„ê¸°ë‹¨ê³„ ì ì •ì„± (20ì )
    if "1ë‹¨ê³„" in report and "ê´€ì‹¬" in report:
        score += 20
    elif "2ë‹¨ê³„" in report:
        score += 15
    
    # ë‹´ë‹¹ì ì •ë³´ êµ¬ì²´ì„± (20ì )
    if "/" in report and ("ë¦¬ë”" in report or "íŒ€ì¥" in report):
        score += 20
    elif "íŒ€" in report or "ë¶€ì„œ" in report:
        score += 10
    
    # ì‚¬ì‹¤í™•ì¸ í’ˆì§ˆ (30ì )
    fact_section = report[report.find("ì‚¬ì‹¤ í™•ì¸:"):report.find("ì„¤ëª… ë…¼ë¦¬:")] if "ì‚¬ì‹¤ í™•ì¸:" in report else ""
    if len(fact_section) > 200:
        score += 30
    elif len(fact_section) > 100:
        score += 20
    elif len(fact_section) > 50:
        score += 10
    
    # ì´ìŠˆë³„ íŠ¹í™” ë‚´ìš© (30ì )
    issue = test_case["issue"]
    if "ì‹¤ì " in issue:
        # ì‹¤ì  ê´€ë ¨ íŠ¹í™” ìš”ì†Œ
        financial_terms = ["ì–µ", "ì¡°", "%", "ì „ë…„", "ë™ê¸°"]
        found_financial = sum(1 for term in financial_terms if term in report)
        score += min(found_financial * 6, 30)
    elif "ì‹ëŸ‰" in issue:
        # ì‹ëŸ‰ì‚¬ì—… íŠ¹í™” ìš”ì†Œ  
        food_terms = ["ê³¡ë¬¼", "ìƒì‚°", "ê³µê¸‰", "ê±°ì ", "í•´ì™¸"]
        found_food = sum(1 for term in food_terms if term in report)
        score += min(found_food * 6, 30)
    else:
        # ì¼ë°˜ ì‚¬ì—… ë‚´ìš©
        if len(report) > 500:
            score += 30
        elif len(report) > 300:
            score += 20
    
    return min(score, 100)

def evaluate_professionalism(report, test_case):
    """ì „ë¬¸ì„± í‰ê°€"""
    score = 0
    
    # ì „ë¬¸ ìš©ì–´ ì‚¬ìš© (40ì )
    if "ì‹¤ì " in test_case["issue"]:
        ir_terms = ["ì—°ê²°ê¸°ì¤€", "ì˜ì—…ì´ìµ", "ì¬ë¬´ì„±ê³¼", "ì£¼ì£¼ê°€ì¹˜", "ì „ë…„ë™ê¸°ëŒ€ë¹„"]
        found_ir = sum(1 for term in ir_terms if term in report)
        score += min(found_ir * 8, 40)
    else:
        professional_terms = ["ë‹¹ì‚¬", "ì‚¬ì—…ë¶€", "ìš´ì˜", "ì „ëµ", "ê²½ìŸë ¥"]
        found_prof = sum(1 for term in professional_terms if term in report)
        score += min(found_prof * 8, 40)
    
    # êµ¬ì²´ì„± (30ì )
    specific_indicators = ["ì•½", "ì–µ", "ì¡°", "%", "ë…„", "ì›”"]
    found_specific = sum(1 for indicator in specific_indicators if indicator in report)
    score += min(found_specific * 5, 30)
    
    # ê· í˜•ì„± (30ì )
    if "ê¸ì •" in report or "ê°œì„ " in report or "ì„±ì¥" in report:
        score += 15
    if "í•œê³„" in report or "ì œí•œ" in report or "ë¹„ê³µê°œ" in report or "ê°ì†Œ" in report:
        score += 15
    
    return min(score, 100)

def evaluate_usability(report):
    """ì‹¤ìš©ì„± í‰ê°€"""
    score = 0
    
    # ì›ë³´ì´ìŠ¤ ì‹¤ìš©ì„± (50ì )
    if '"' in report:
        quotes = [q for q in report.split('"') if 20 <= len(q) <= 100]
        if quotes:
            score += 50
        elif '"' in report:
            score += 30
    
    # ë³´ê³ ì„œ ì™„ì„±ë„ (50ì )
    if len(report) > 800:
        score += 50
    elif len(report) > 600:
        score += 40
    elif len(report) > 400:
        score += 30
    elif len(report) > 200:
        score += 20
    
    return min(score, 100)

def save_final_performance_result(results, avg_time, avg_score, avg_ratio, grade):
    """ìµœì¢… ì„±ëŠ¥ ê²°ê³¼ ì €ì¥"""
    timestamp = datetime.now().strftime('%H%M%S')
    filename = f"final_performance_test_{timestamp}.txt"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("=== ìµœì¢… ì„±ëŠ¥ í‰ê°€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===\n")
        f.write(f"í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ\n")
        f.write(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_score:.1f}/100\n")
        f.write(f"í‰ê·  ëª©í‘œ ë‹¬ì„±ë¥ : {avg_ratio:.1f}%\n")
        f.write(f"ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰: {grade}\n\n")
        
        for result in results:
            f.write(f"\n{'='*50}\n")
            f.write(f"{result['name'].upper()}\n")
            f.write(f"{'='*50}\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ\n")
            f.write(f"í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.1f}/100\n")
            f.write(f"ëª©í‘œ ë‹¬ì„±ë¥ : {result['performance_ratio']:.1f}%\n")
            f.write(f"ë³´ê³ ì„œ ê¸¸ì´: {result['report_length']:,}ì\n\n")
            
            f.write("ìƒì„±ëœ ë³´ê³ ì„œ:\n")
            f.write("-" * 30 + "\n")
            f.write(result['report'])
            f.write("\n\n")
    
    print(f"SAVE: ìµœì¢… ì„±ëŠ¥ ê²°ê³¼ ì €ì¥ {filename}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    success, results, avg_ratio = run_final_performance_test()
    
    if success and results:
        print("\n=== ì‹œìŠ¤í…œ ë°œì „ ìš”ì•½ ===")
        print("Before (ì´ˆê¸° ì‹œìŠ¤í…œ):")
        print("- ì²˜ë¦¬ ì‹œê°„: 60-120ì´ˆ")
        print("- í’ˆì§ˆ ì ìˆ˜: ~36ì ")
        print("- êµ¬ì²´ì„±: ë§¤ìš° ë¶€ì¡±")
        print("- ì „ë¬¸ì„±: ì¼ë°˜ì ")
        
        print("\nAfter (ìµœì¢… ì‹œìŠ¤í…œ):")
        avg_time = sum(r['processing_time'] for r in results) / len(results)
        avg_score = sum(r['quality_score'] for r in results) / len(results)
        print(f"- ì²˜ë¦¬ ì‹œê°„: {avg_time:.1f}ì´ˆ (95% ë‹¨ì¶•)")
        print(f"- í’ˆì§ˆ ì ìˆ˜: {avg_score:.1f}ì  ({avg_score/36*100:.0f}% í–¥ìƒ)")
        print("- êµ¬ì²´ì„±: ì´ìƒì  ì‚¬ë¡€ ìˆ˜ì¤€")
        print("- ì „ë¬¸ì„±: ì—…ê³„ ì „ë¬¸ ìˆ˜ì¤€")
        
        if avg_ratio >= 85:
            print("\nğŸ‰ ìµœì¢… í‰ê°€: ì´ìƒì  ì‚¬ë¡€ ìˆ˜ì¤€ ë‹¬ì„±!")
        else:
            print(f"\nğŸ“Š ìµœì¢… í‰ê°€: ëª©í‘œ ë‹¬ì„±ë¥  {avg_ratio:.1f}%")
        
        return True
    else:
        print("ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False

if __name__ == "__main__":
    main()