import os
from dotenv import load_dotenv
from llm_manager import LLMManager
from data_based_llm import DataBasedLLM
from data_version_manager import DataVersionManager

def setup_check():
    """í™˜ê²½ ì„¤ì • í™•ì¸"""
    load_dotenv()
    
    print("=== NEW RISK MANAGEMENT SYSTEM ===")
    print("í™˜ê²½ ì„¤ì • í™•ì¸ ì¤‘...")
    
    # ê¸°ë³¸ í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    db_host = os.getenv('DB_HOST', 'localhost')
    environment = os.getenv('ENVIRONMENT', 'development')
    debug = os.getenv('DEBUG', 'False').lower() == 'true'
    
    print(f"âœ“ í™˜ê²½: {environment}")
    print(f"âœ“ ë””ë²„ê·¸ ëª¨ë“œ: {debug}")
    print(f"âœ“ DB í˜¸ìŠ¤íŠ¸: {db_host}")
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv('OPEN_API_KEY')
    if api_key:
        print(f"âœ“ OpenAI API í‚¤ ì„¤ì •ë¨ (ê¸¸ì´: {len(api_key)})")
        return True
    else:
        print("âœ— OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return False

def run_basic_tests():
    """ê¸°ë³¸ LLM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n=== ê¸°ë³¸ LLM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        llm = LLMManager(model="gpt-4", data_folder="data")
        
        # 1. ê°„ë‹¨í•œ ì±„íŒ… í…ŒìŠ¤íŠ¸
        print("\n1. ê¸°ë³¸ ì±„íŒ… í…ŒìŠ¤íŠ¸")
        response = llm.chat("ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ìê¸°ì†Œê°œ í•´ì£¼ì„¸ìš”.")
        print(f"AI: {response[:100]}..." if len(response) > 100 else f"AI: {response}")
        
        # 2. ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\n2. ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸")
        code = llm.generate_code("ë¦¬ìŠ¤íŠ¸ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜", "Python")
        print(f"ìƒì„±ëœ ì½”ë“œ:\n{code}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def check_data_versions():
    """ë°ì´í„° íŒŒì¼ ë²„ì „ í™•ì¸"""
    print("\n=== ë°ì´í„° ë²„ì „ í™•ì¸ ===")
    
    try:
        version_manager = DataVersionManager()
        
        # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
        integrity_results = version_manager.check_data_integrity()
        
        # ë²„ì „ ì •ë³´ ì¶œë ¥
        version_info = version_manager.list_all_versions()
        print(version_info)
        
        # ëª¨ë“  íŒŒì¼ì´ ì •ìƒì¸ì§€ í™•ì¸
        all_valid = all(integrity_results.values())
        
        if all_valid:
            print("âœ… ëª¨ë“  ë°ì´í„° íŒŒì¼ì´ ì •ìƒì…ë‹ˆë‹¤.")
        else:
            print("âš ï¸ ì¼ë¶€ ë°ì´í„° íŒŒì¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            
        return all_valid
        
    except Exception as e:
        print(f"âœ— ë°ì´í„° ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return False

def run_data_based_tests():
    """ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n=== ë°ì´í„° ê¸°ë°˜ ë‹µë³€ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        data_llm = DataBasedLLM()
        
        # 1. ë¶€ì„œ ì •ë³´ ê¸°ë°˜ ì§ˆì˜
        print("\n1. ë¶€ì„œ ì •ë³´ ê¸°ë°˜ ì§ˆì˜ í…ŒìŠ¤íŠ¸")
        query1 = "ë°°ë‹¹ ê´€ë ¨ ì´ìŠˆ ë‹´ë‹¹ìë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
        response1 = data_llm.generate_data_based_response(query1)
        print(f"ì§ˆë¬¸: {query1}")
        print(f"ë‹µë³€: {response1[:300]}..." if len(response1) > 300 else f"ë‹µë³€: {response1}")
        
        # 2. ì–¸ë¡ ëŒ€ì‘ ì‚¬ë¡€ ê¸°ë°˜ ì§ˆì˜
        print("\n2. ì–¸ë¡ ëŒ€ì‘ ì‚¬ë¡€ ê¸°ë°˜ ì§ˆì˜ í…ŒìŠ¤íŠ¸")
        query2 = "ì „ê¸°ì°¨ ê´€ë ¨ ë³´ë„ì— ëŒ€í•œ ëŒ€ì‘ ë°©ì•ˆ"
        response2 = data_llm.generate_data_based_response(query2)
        print(f"ì§ˆë¬¸: {query2}")
        print(f"ë‹µë³€: {response2[:300]}..." if len(response2) > 300 else f"ë‹µë³€: {response2}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ë°ì´í„° ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def interactive_chat():
    """ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ"""
    print("\n=== ëŒ€í™”í˜• ì±„íŒ… ëª¨ë“œ ===")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ' ì…ë ¥")
    print("ë¦¬ìŠ¤í¬ ë¶„ì„ ëª¨ë“œ: '/risk [íšŒì‚¬ì •ë³´]'")
    print("VaR ê³„ì‚° ëª¨ë“œ: '/var [í¬íŠ¸í´ë¦¬ì˜¤ì •ë³´]'")
    print("ë°ì´í„° ê¸°ë°˜ ë‹µë³€: '/data [ì§ˆë¬¸]'")
    print("ëŒ€ì‘ ì „ëµ ì¶”ì²œ: '/strategy [ì´ìŠˆì„¤ëª…]'")
    print("ì´ìŠˆ ì›¹ ê²€ìƒ‰ ì—°êµ¬: '/research [ë°œìƒì´ìŠˆ]'")
    print("ê°•í™”ëœ ì‚¬ì‹¤ê²€ì¦ & ëŒ€ì‘ë°©ì•ˆ: '/enhanced [ë°œìƒì´ìŠˆ]'")
    print("ğŸš€ ìµœì í™”ëœ ê³ ì† ë¶„ì„: '/fast [ë°œìƒì´ìŠˆ]'")
    print("íŠ¸ë Œë“œ ë¶„ì„: '/trend'")
    print("ìºì‹œ ê´€ë¦¬: '/cache clear' ë˜ëŠ” '/cache status'")
    print("ë²„ì „ ì •ë³´: '/version'")
    print("ë°ì´í„° ë°±ì—…: '/backup [íŒŒì¼ëª…] [ì„¤ëª…] [ë³€ê²½ìœ í˜•]'")
    print("-" * 40)
    
    try:
        llm = LLMManager(model="gpt-4", data_folder="data")
        data_llm = DataBasedLLM()
        version_manager = DataVersionManager()
        
        while True:
            user_input = input("\nì‚¬ìš©ì: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                continue
            
            # íŠ¹ìˆ˜ ëª…ë ¹ì–´ ì²˜ë¦¬
            if user_input.startswith('/risk '):
                company_info = user_input[6:]
                response = llm.analyze_credit_risk(company_info)
            elif user_input.startswith('/var '):
                portfolio_info = user_input[5:]
                response = llm.calculate_var_explanation(portfolio_info)
            elif user_input.startswith('/data '):
                query = user_input[6:]
                response = data_llm.generate_data_based_response(query)
            elif user_input.startswith('/strategy '):
                issue = user_input[10:]
                response = data_llm.recommend_response_strategy(issue)
            elif user_input.startswith('/research '):
                issue = user_input[10:]
                print("ğŸ” ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì´ìŠˆ ì—°êµ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                response = data_llm.research_issue_with_web_search(issue)
            elif user_input.startswith('/enhanced '):
                issue = user_input[11:]
                print("ğŸ” ê°•í™”ëœ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì‚¬ì‹¤ê²€ì¦ ë° ëŒ€ì‘ë°©ì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤...")
                response = data_llm.generate_enhanced_response_with_fact_check(issue)
            elif user_input.startswith('/fast '):
                issue = user_input[6:]
                print("ğŸš€ ìµœì í™”ëœ ê³ ì† ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                response = data_llm.generate_optimized_response_with_fact_check(issue)
            elif user_input == '/cache clear':
                data_llm.clear_all_caches()
                response = "ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
            elif user_input == '/cache status':
                if hasattr(data_llm, 'optimized_research') and data_llm.optimized_research:
                    cache_status = data_llm.optimized_research._get_cache_status()
                    response = f"ìºì‹œ ìƒíƒœ: ë©”ëª¨ë¦¬ {cache_status.get('memory_cache_size', 0)}ê°œ í•­ëª©"
                else:
                    response = "ìºì‹œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            elif user_input == '/trend':
                response = data_llm.analyze_issue_trends(30)
            elif user_input == '/version':
                response = version_manager.list_all_versions()
            elif user_input.startswith('/backup '):
                parts = user_input[8:].split(' ', 2)
                if len(parts) >= 1:
                    file_name = parts[0]
                    description = parts[1] if len(parts) > 1 else "ìˆ˜ë™ ë°±ì—…"
                    change_type = parts[2] if len(parts) > 2 else "patch"
                    
                    success = version_manager.create_backup(file_name, description, change_type)
                    response = f"ë°±ì—… {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}: {file_name}"
                else:
                    response = "ì‚¬ìš©ë²•: /backup [íŒŒì¼ëª…] [ì„¤ëª…] [ë³€ê²½ìœ í˜•]"
            else:
                # ì¼ë°˜ ì±„íŒ…
                response = llm.conversation_chat(user_input)
            
            print(f"AI: {response}")
    
    except KeyboardInterrupt:
        print("\nì±„íŒ…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì±„íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1. í™˜ê²½ ì„¤ì • í™•ì¸
    if not setup_check():
        print("\ní™˜ê²½ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # 2. ë°ì´í„° ë²„ì „ ë° ë¬´ê²°ì„± í™•ì¸
    if not check_data_versions():
        print("\në°ì´í„° íŒŒì¼ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ğŸ“‹ ìì„¸í•œ ë‚´ìš©ì€ DATA_UPDATE_GUIDE.mdë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")
    
    # 3. ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    if not run_basic_tests():
        print("\nê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 4. ë°ì´í„° ê¸°ë°˜ ë‹µë³€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    if not run_data_based_tests():
        print("\në°ì´í„° ê¸°ë°˜ ë‹µë³€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "=" * 50)
    print("âœ“ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("=" * 50)
    
    # 5. ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
    choice = input("\nëŒ€í™”í˜• ì±„íŒ…ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
    if choice in ['y', 'yes', 'ì˜ˆ', 'ã…‡']:
        interactive_chat()

if __name__ == "__main__":
    main()
