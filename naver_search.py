"""
ë„¤ì´ë²„ ê²€ìƒ‰ API ì—°ë™ ëª¨ë“ˆ
ì–¸ë¡  ì´ê´„ë‹´ë‹¹ìë¥¼ ìœ„í•œ ì´ìŠˆ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
"""
import os
import requests
import json
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dotenv import load_dotenv

class NaverSearchAPI:
    """ë„¤ì´ë²„ ê²€ìƒ‰ API í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë„¤ì´ë²„ ê²€ìƒ‰ API ì´ˆê¸°í™”"""
        load_dotenv()
        self.client_id = os.getenv('NAVER_CLIENT_ID')
        self.client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
        if not self.client_id or not self.client_secret:
            raise ValueError("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        self.base_url = "https://openapi.naver.com/v1/search"
        self.headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret
        }
    
    def search_news(self, query: str, display: int = 10, sort: str = "date") -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            display (int): ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (1~100)
            sort (str): ì •ë ¬ ë°©ì‹ ("sim": ì •í™•ë„ìˆœ, "date": ë‚ ì§œìˆœ)
        
        Returns:
            Dict: ê²€ìƒ‰ ê²°ê³¼
        """
        url = f"{self.base_url}/news.json"
        params = {
            'query': query,
            'display': display,
            'sort': sort
        }
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {"items": []}
    
    def search_blog(self, query: str, display: int = 10, sort: str = "date") -> Dict[str, Any]:
        """
        ë¸”ë¡œê·¸ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            display (int): ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (1~100)
            sort (str): ì •ë ¬ ë°©ì‹ ("sim": ì •í™•ë„ìˆœ, "date": ë‚ ì§œìˆœ)
        
        Returns:
            Dict: ê²€ìƒ‰ ê²°ê³¼
        """
        url = f"{self.base_url}/blog.json"
        params = {
            'query': query,
            'display': display,
            'sort': sort
        }
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"ë¸”ë¡œê·¸ ê²€ìƒ‰ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {"items": []}

class IssueResearchService:
    """ë°œìƒ ì´ìŠˆ ì—°êµ¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """ì´ìŠˆ ì—°êµ¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.naver_api = NaverSearchAPI()
    
    def research_issue(self, issue_description: str) -> Dict[str, Any]:
        """
        ë°œìƒ ì´ìŠˆì— ëŒ€í•œ ì¢…í•©ì ì¸ ì—°êµ¬ ìˆ˜í–‰
        
        Args:
            issue_description (str): ë°œìƒ ì´ìŠˆ ì„¤ëª…
        
        Returns:
            Dict: ì—°êµ¬ ê²°ê³¼ (ë‰´ìŠ¤, ë¸”ë¡œê·¸, ë¶„ì„ ì •ë³´ í¬í•¨)
        """
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
        search_query = self._optimize_search_query(issue_description)
        
        # ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì •ë³´ ìˆ˜ì§‘
        research_data = {
            "original_issue": issue_description,
            "search_query": search_query,
            "timestamp": datetime.now().isoformat(),
            "news_results": [],
            "blog_results": [],
            "analysis_summary": {}
        }
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ (ìµœì‹ ìˆœ)
        print(f"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘: {search_query}")
        news_data = self.naver_api.search_news(search_query, display=15, sort="date")
        research_data["news_results"] = self._process_news_results(news_data.get("items", []))
        
        # ë¸”ë¡œê·¸ ê²€ìƒ‰ (ê´€ë ¨ì„±ìˆœ)
        print(f"ğŸ“ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘: {search_query}")
        blog_data = self.naver_api.search_blog(search_query, display=10, sort="sim")
        research_data["blog_results"] = self._process_blog_results(blog_data.get("items", []))
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        research_data["analysis_summary"] = self._analyze_search_results(research_data)
        
        return research_data
    
    def _optimize_search_query(self, issue_description: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ì œ
        query = issue_description.strip()

        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        remove_phrases = ["ë°œìƒí•œ", "ì´ìŠˆ", "ë¬¸ì œ", "ìƒí™©", "ê´€ë ¨", "ëŒ€í•´ì„œ", "ì— ëŒ€í•œ"]
        for phrase in remove_phrases:
            query = query.replace(phrase, "")

        # ê³µë°± ì •ë¦¬
        query = " ".join(query.split())

        # ì •í™•í•œ ê²€ìƒ‰ì´ í•„ìš”í•œ ë³µí•©ì–´ ì²˜ë¦¬
        # "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„"ê³¼ ê°™ì´ ë„ì–´ì“°ê¸° ì—†ëŠ” ê³ ìœ ëª…ì‚¬ëŠ” í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ ì •í™•í•œ ë§¤ì¹­
        exact_match_keywords = [
            "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
            "POSCOì¸í„°ë‚´ì…”ë„",
            "POSCO INTERNATIONAL"
        ]

        for keyword in exact_match_keywords:
            if keyword in query:
                # ì´ë¯¸ í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ì ¸ ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                if f'"{keyword}"' not in query:
                    query = query.replace(keyword, f'"{keyword}"')

        return query
    
    def _process_news_results(self, news_items: List[Dict]) -> List[Dict]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        processed_news = []
        
        for item in news_items:
            processed_item = {
                "title": self._clean_html_tags(item.get("title", "")),
                "link": item.get("link", ""),
                "description": self._clean_html_tags(item.get("description", "")),
                "pub_date": item.get("pubDate", ""),
                "source_type": "news"
            }
            processed_news.append(processed_item)
        
        return processed_news
    
    def _process_blog_results(self, blog_items: List[Dict]) -> List[Dict]:
        """ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        processed_blogs = []
        
        for item in blog_items:
            processed_item = {
                "title": self._clean_html_tags(item.get("title", "")),
                "link": item.get("link", ""),
                "description": self._clean_html_tags(item.get("description", "")),
                "blogger_name": item.get("bloggername", ""),
                "post_date": item.get("postdate", ""),
                "source_type": "blog"
            }
            processed_blogs.append(processed_item)
        
        return processed_blogs
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        import re
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text)
        # HTML ì—”í‹°í‹° ë³€í™˜
        clean_text = clean_text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        clean_text = clean_text.replace('&quot;', '"').replace('&#39;', "'")
        
        return clean_text.strip()
    
    def _analyze_search_results(self, research_data: Dict) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
        news_count = len(research_data.get("news_results", []))
        blog_count = len(research_data.get("blog_results", []))
        
        # ìµœì‹  ë‰´ìŠ¤ ì‹œê°„ ë¶„ì„
        latest_news_date = None
        if research_data.get("news_results"):
            # ì²« ë²ˆì§¸ ë‰´ìŠ¤ê°€ ìµœì‹  (date ìˆœìœ¼ë¡œ ì •ë ¬í–ˆìœ¼ë¯€ë¡œ)
            latest_news_date = research_data["news_results"][0].get("pub_date")
        
        analysis = {
            "total_sources": news_count + blog_count,
            "news_count": news_count,
            "blog_count": blog_count,
            "latest_news_date": latest_news_date,
            "coverage_level": self._assess_coverage_level(news_count, blog_count),
            "research_completeness": "ì™„ë£Œ" if (news_count > 0 or blog_count > 0) else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        }
        
        return analysis
    
    def _assess_coverage_level(self, news_count: int, blog_count: int) -> str:
        """ì–¸ë¡  ë³´ë„ ìˆ˜ì¤€ í‰ê°€"""
        total_coverage = news_count + blog_count
        
        if total_coverage >= 20:
            return "ë†’ìŒ (ê´‘ë²”ìœ„í•œ ë³´ë„)"
        elif total_coverage >= 10:
            return "ì¤‘ê°„ (ì¼ì • ìˆ˜ì¤€ì˜ ê´€ì‹¬)"
        elif total_coverage >= 5:
            return "ë‚®ìŒ (ì œí•œì  ë³´ë„)"
        elif total_coverage > 0:
            return "ë§¤ìš° ë‚®ìŒ (ìµœì†Œ ë³´ë„)"
        else:
            return "ë³´ë„ ì—†ìŒ"

def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    try:
        research_service = IssueResearchService()
        
        # í…ŒìŠ¤íŠ¸ìš© ì´ìŠˆ
        test_issue = "ì „ê¸°ì°¨ ë°°í„°ë¦¬ í™”ì¬ ì•ˆì „ì„±"
        
        print(f"=== ì´ìŠˆ ì—°êµ¬ í…ŒìŠ¤íŠ¸: {test_issue} ===")
        results = research_service.research_issue(test_issue)
        
        print(f"\nğŸ“Š ì—°êµ¬ ê²°ê³¼ ìš”ì•½:")
        print(f"- ê²€ìƒ‰ ì¿¼ë¦¬: {results['search_query']}")
        print(f"- ë‰´ìŠ¤ ê¸°ì‚¬: {results['analysis_summary']['news_count']}ê±´")
        print(f"- ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸: {results['analysis_summary']['blog_count']}ê±´")
        print(f"- ë³´ë„ ìˆ˜ì¤€: {results['analysis_summary']['coverage_level']}")
        
        print(f"\nğŸ“° ì£¼ìš” ë‰´ìŠ¤ (ìƒìœ„ 3ê±´):")
        for i, news in enumerate(results['news_results'][:3], 1):
            print(f"{i}. {news['title']}")
            print(f"   ì„¤ëª…: {news['description'][:100]}...")
            print(f"   ë§í¬: {news['link']}")
            print()
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()