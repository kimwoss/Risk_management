"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ê³µí†µ ëª¨ë“ˆ
Streamlit Appê³¼ Standalone Monitorê°€ ê³µìœ í•˜ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§
"""
import os
import re
import urllib.parse
import json
from datetime import datetime, timezone, timedelta
from html import unescape
import pandas as pd
import requests

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# ======================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ========================

# ì›ë³¸ print ì €ì¥
_original_print = print

def safe_print(*args, **kwargs):
    """Windows cp949 ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€ìš© ì•ˆì „í•œ print"""
    try:
        _original_print(*args, **kwargs)
    except (UnicodeEncodeError, OSError):
        # ì´ëª¨ì§€ ì œê±° í›„ ì¬ì‹œë„
        try:
            safe_args = []
            for arg in args:
                if isinstance(arg, str):
                    # ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
                    safe_arg = arg.encode('ascii', 'ignore').decode('ascii')
                    safe_args.append(safe_arg)
                else:
                    safe_args.append(arg)
            _original_print(*safe_args, **kwargs)
        except:
            # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ë¬´ì‹œ
            pass

# ì „ì—­ printë¥¼ safe_printë¡œ ì˜¤ë²„ë¼ì´ë“œ
print = safe_print

# ======================== ìƒìˆ˜ ì„¤ì • ========================

DATA_FOLDER = os.path.abspath("data")
NEWS_DB_FILE = os.path.join(DATA_FOLDER, "news_monitor.csv")
SENT_CACHE_FILE = os.path.join(DATA_FOLDER, "sent_articles_cache.json")
PENDING_QUEUE_FILE = os.path.join(DATA_FOLDER, "pending_articles.json")  # Pending í íŒŒì¼
API_USAGE_FILE = os.path.join(DATA_FOLDER, "api_usage.json")
STATE_FILE = os.path.join(DATA_FOLDER, "monitor_state.json")
MAX_SENT_CACHE = 10000  # ìºì‹œ í¬ê¸° ì œí•œ (ì•½ 4-5ì¼ë¶„ ì»¤ë²„, ê¸°ì¡´ 500ê°œì—ì„œ í™•ëŒ€)
MAX_API_CALLS_PER_DAY = 25000  # ë„¤ì´ë²„ API ì¼ì¼ í• ë‹¹ëŸ‰
API_QUOTA_WARNING_THRESHOLD = 20000  # 80% ë„ë‹¬ ì‹œ ê²½ê³  (25000ì˜ 80%)
MAX_PENDING_RETRY = 5  # Pending í ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
PENDING_TTL_HOURS = 48  # Pending í TTL (48ì‹œê°„)

# ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ ì„¤ì • (ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›)
KEYWORDS = [
    "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
    "POSCO INTERNATIONAL",
    "í¬ìŠ¤ì½”ì¸í„°",
    "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
    "êµ¬ë™ëª¨í„°ì½”ì•„",
    "êµ¬ë™ëª¨í„°ì½”ì–´",
    "ë¯¸ì–€ë§ˆ LNG",
    "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜",
    "í¬ìŠ¤ì½”í”Œë¡œìš°",  # ì¶”ê°€
    "í¬ìŠ¤ì½”"
]

# "í¬ìŠ¤ì½”" í‚¤ì›Œë“œ ì œì™¸ í•„í„°
EXCLUDE_KEYWORDS = [
    "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
    "POSCO INTERNATIONAL",
    "í¬ìŠ¤ì½”ì¸í„°",
    "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
    "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"
]

# ìˆ˜ì§‘ ì„¤ì •
MAX_ITEMS_PER_RUN = 300  # í‚¤ì›Œë“œë‹¹ ì•½ 30ê°œ ìˆ˜ì§‘ (í•„í„°ë§ í›„ ì¶©ë¶„í•œ ê¸°ì‚¬ í™•ë³´)

# ì „ì†¡ëœ ê¸°ì‚¬ URL ì¶”ì  (ë©”ëª¨ë¦¬ ìºì‹œ)
_sent_articles_cache = set()


# ======================== í—¬í¼ í•¨ìˆ˜ ========================

def _naver_headers():
    """Naver API ì¸ì¦ í—¤ë”"""
    cid = os.getenv("NAVER_CLIENT_ID", "")
    csec = os.getenv("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        print(f"[WARNING] ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    return {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}


def _clean_text(s: str) -> str:
    """HTML íƒœê·¸ ë° ê³µë°± ì •ë¦¬"""
    if not s:
        return ""
    s = unescape(s)
    s = re.sub(r"</?b>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _normalize_url(url: str) -> str:
    """
    URL ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ URLì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë³´ì¡´ (ì¤‘ìš”! ë§ì€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ê°€ ì¿¼ë¦¬ë¡œ ê¸°ì‚¬ êµ¬ë¶„)
    - í”„ë¡œí† ì½œ í†µì¼ (http â†’ https)
    - ë ìŠ¬ë˜ì‹œ ì œê±°
    """
    try:
        if not url:
            return ""
        parsed = urllib.parse.urlparse(url)

        # 1. í”„ë¡œí† ì½œ í†µì¼ (http â†’ https)
        scheme = "https" if parsed.scheme in ["http", "https"] else parsed.scheme

        # 2. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë³´ì¡´ (ê¸°ì‚¬ ID êµ¬ë¶„ì„ ìœ„í•´ í•„ìˆ˜)
        query = f"?{parsed.query}" if parsed.query else ""

        # 3. ë ìŠ¬ë˜ì‹œ ì œê±°
        path = parsed.path.rstrip("/")

        # 4. ì •ê·œí™”ëœ URL ìƒì„±
        normalized = f"{scheme}://{parsed.netloc}{path}{query}"
        return normalized
    except Exception as e:
        print(f"[WARNING] URL ì •ê·œí™” ì‹¤íŒ¨: {url} - {e}")
        return url


def _publisher_from_link(u: str) -> str:
    """ë‰´ìŠ¤ ì›ë¬¸ URLì—ì„œ ë§¤ì²´ëª…ì„ í†µì¼í•´ì„œ ë°˜í™˜"""
    try:
        host = urllib.parse.urlparse(u).netloc.lower().replace("www.", "")
        if not host:
            return ""

        # ì„œë¸Œë„ë©”ì¸ ì •í™• ë§¤í•‘
        host_map = {
            "en.yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
            "news.kbs.co.kr": "KBS",
            "news.mtn.co.kr": "MTN",
            "starin.edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "sports.donga.com": "ë™ì•„ì¼ë³´",
            "biz.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
            "daily.hankooki.com": "ë°ì¼ë¦¬í•œêµ­",
            "news.dealsitetv.com": "ë”œì‚¬ì´íŠ¸TV",
        }
        if host in host_map:
            return host_map[host]

        # ê¸°ë³¸ ë„ë©”ì¸(eTLD+1) ì¶”ì¶œ
        parts = host.split(".")
        if len(parts) >= 3 and parts[-1] == "kr" and parts[-2] in {
            "co","or","go","ne","re","pe","ac","hs","kg","sc",
            "seoul","busan","incheon","daegu","daejeon","gwangju","ulsan",
            "gyeonggi","gangwon","chungbuk","chungnam","jeonbuk","jeonnam",
            "gyeongbuk","gyeongnam","jeju"
        }:
            base = ".".join(parts[-3:])
        else:
            base = ".".join(parts[-2:])

        # ê¸°ë³¸ ë„ë©”ì¸ â†’ ë§¤ì²´ëª… ë§¤í•‘
        base_map = {
            "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "kbs.co.kr": "KBS", "joins.com": "ì¤‘ì•™ì¼ë³´",
            "donga.com": "ë™ì•„ì¼ë³´", "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ", "edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "ajunews.com": "ì•„ì£¼ê²½ì œ", "newspim.com": "ë‰´ìŠ¤í•Œ", "news1.kr": "ë‰´ìŠ¤1",
            "etoday.co.kr": "ì´íˆ¬ë°ì´", "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
            "munhwa.com": "ë¬¸í™”ì¼ë³´", "segye.com": "ì„¸ê³„ì¼ë³´", "hankooki.com": "í•œêµ­ì¼ë³´",
            "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤", "ekn.kr": "ì—ë„ˆì§€ê²½ì œ", "businesskorea.co.kr": "ë¹„ì¦ˆë‹ˆìŠ¤ì½”ë¦¬ì•„",
            "ferrotimes.com": "ì² ê°•ê¸ˆì†ì‹ ë¬¸", "thepublic.kr": "ë”í¼ë¸”ë¦­", "tf.co.kr": "ë”íŒ©íŠ¸",
            "straightnews.co.kr": "ìŠ¤íŠ¸ë ˆì´íŠ¸ë‰´ìŠ¤", "smartfn.co.kr": "ìŠ¤ë§ˆíŠ¸ê²½ì œ", "sisacast.kr": "ì‹œì‚¬ìºìŠ¤íŠ¸",
            "sateconomy.co.kr": "ì‹œì‚¬ê²½ì œ", "safetynews.co.kr": "ì•ˆì „ì‹ ë¬¸", "rpm9.com": "RPM9",
            "pointdaily.co.kr": "í¬ì¸íŠ¸ë°ì¼ë¦¬", "newsworker.co.kr": "ë‰´ìŠ¤ì›Œì»¤", "newsdream.kr": "ë‰´ìŠ¤ë“œë¦¼",
            "nbntv.co.kr": "NBNë‰´ìŠ¤", "megaeconomy.co.kr": "ë©”ê°€ê²½ì œ", "mediapen.com": "ë¯¸ë””ì–´íœ",
            "job-post.co.kr": "ì¡í¬ìŠ¤íŠ¸", "irobotnews.com": "ë¡œë´‡ì‹ ë¬¸ì‚¬", "ifm.kr": "ê²½ì¸ë°©ì†¡",
            "gpkorea.com": "ê¸€ë¡œë²Œì˜¤í† ë‰´ìŠ¤", "energydaily.co.kr": "ì—ë„ˆì§€ë°ì¼ë¦¬",
            "cstimes.com": "ì»¨ìŠˆë¨¸íƒ€ì„ìŠ¤", "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜", "autodaily.co.kr": "ì˜¤í† ë°ì¼ë¦¬",

            # ì¶”ê°€ ì–¸ë¡ ì‚¬ (2025-11-20)
            "newslock.co.kr": "ë‰´ìŠ¤ë½", "mbn.co.kr": "MBN", "kpenews.com": "KPE",
            "koreatimes.co.kr": "ì½”ë¦¬ì•„íƒ€ì„ìŠ¤", "korea.kr": "ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘",
            "investchosun.com": "ì¸ë² ìŠ¤íŠ¸ì¡°ì„ ", "goodnews1.com": "GOODTV", "aitimes.kr": "AIíƒ€ì„ìŠ¤",
            # ì¶”ê°€ ì–¸ë¡ ì‚¬ (2025-12-02)
            "worklaw.co.kr": "ì›Œí¬ë¡œ", "vop.co.kr": "ë¯¼ì¤‘ì˜ì†Œë¦¬", "thefairnews.co.kr": "ë”í˜ì–´ë‰´ìŠ¤",
            "newsway.co.kr": "ë‰´ìŠ¤ì›¨ì´", "newsfreezone.co.kr": "ë‰´ìŠ¤í”„ë¦¬ì¡´", "mtn.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´ë°©ì†¡",
            "kyongbuk.co.kr": "ê²½ë¶ì¼ë³´", "klnews.co.kr": "ë¬¼ë¥˜ì‹ ë¬¸", "geconomy.co.kr": "Gê²½ì œ",
            "enetnews.co.kr": "ì´ë„·ë‰´ìŠ¤", "dkilbo.com": "ëŒ€ê²½ì¼ë³´", "dailysportshankook.co.kr": "ë°ì¼ë¦¬ìŠ¤í¬ì¸ í•œêµ­",
            "dailysecu.com": "ë°ì¼ë¦¬ì‹œí", "ceoscoredaily.com": "CEOìŠ¤ì½”ì–´ë°ì¼ë¦¬", "apparelnews.co.kr": "ì–´íŒ¨ëŸ´ë‰´ìŠ¤",
            # ì¶”ê°€ ì–¸ë¡ ì‚¬ (2025-12-17)
            "theviewers.co.kr": "ë”ë·°ì–´ìŠ¤", "suwonilbo.kr": "ìˆ˜ì›ì¼ë³´", "smedaily.co.kr": "ì¤‘ì†Œê¸°ì—…ì‹ ë¬¸",
            "smarttoday.co.kr": "ìŠ¤ë§ˆíŠ¸íˆ¬ë°ì´", "newswhoplus.com": "ë‰´ìŠ¤í›„í”ŒëŸ¬ìŠ¤",
            "mdtoday.co.kr": "ë©”ë””ì»¬íˆ¬ë°ì´", "jeonmin.co.kr": "ì „ë¯¼ì¼ë³´",
            "globalepic.co.kr": "ê¸€ë¡œë²Œì´ì½”ë…¸ë¯¹", "financialpost.co.kr": "íŒŒì´ë‚¸ì…œí¬ìŠ¤íŠ¸",
            "economytalk.kr": "ì´ì½”ë…¸ë¯¸í†¡ë‰´ìŠ¤", "delighti.co.kr": "ë”œë¼ì´íŠ¸ì´ìŠˆ",
            "ddaily.co.kr": "ë””ì§€í„¸ë°ì¼ë¦¬", "businessplus.kr": "ë¹„ì¦ˆë‹ˆìŠ¤í”ŒëŸ¬ìŠ¤", "bizwnews.com": "ë¹„ì¦ˆì›”ë“œë‰´ìŠ¤",

            # ì¶”ê°€ ì–¸ë¡ ì‚¬ (2025-12-22)
            "wemakenews.co.kr": "ìœ„ë©”ì´í¬ë‰´ìŠ¤", "tournews21.com": "íˆ¬ì–´ì½”ë¦¬ì•„", "siminilbo.co.kr": "ì‹œë¯¼ì¼ë³´",
            "public25.com": "í¼ë¸”ë¦­íƒ€ì„ìŠ¤", "ngetnews.com": "ë‰´ìŠ¤ì €ë„ë¦¬ì¦˜", "livesnews.com": "ë¼ì´ë¸Œë‰´ìŠ¤",
            "lawleader.co.kr": "ë¡œë¦¬ë”", "koreaittimes.com": "ì½”ë¦¬ì•„ITíƒ€ì„ì¦ˆ", "kmaeil.com": "ê²½ì¸ë§¤ì¼",
            "incheonilbo.com": "ì¸ì²œì¼ë³´", "ggilbo.com": "ê¸ˆê°•ì¼ë³´", "dnews.co.kr": "ëŒ€í•œê²½ì œ",
            "discoverynews.kr": "ë””ìŠ¤ì»¤ë²„ë¦¬ë‰´ìŠ¤", "ccdailynews.com": "ì¶©ì²­ì¼ë³´", "bzeronews.com": "ë¶ˆêµê³µë‰´ìŠ¤",
        }
        if base in base_map:
            return base_map[base]

        return ""
    except Exception:
        return ""


# ======================== ê°ì„± ë¶„ì„ í•¨ìˆ˜ ========================

# ê°ì„± ë¶„ì„ ìºì‹œ (URL ê¸°ë°˜)
_sentiment_cache = {}

def analyze_sentiment_rule_based(title: str, summary: str) -> str:
    """
    ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„ (1ì°¨)

    Args:
        title: ê¸°ì‚¬ ì œëª©
        summary: ê¸°ì‚¬ ìš”ì•½

    Returns:
        "pos" (ê¸ì •/ì¤‘ë¦½), "neg" (ë¶€ì •), "unk" (ì• ë§¤í•¨)
    """
    text = f"{title}\n{summary}".lower()

    # ë¶€ì • í‚¤ì›Œë“œ (ìµœì†Œ ì„¸íŠ¸)
    negative_keywords = [
        "ì˜í˜¹", "ë…¼ë€", "ìˆ˜ì‚¬", "ê³ ë°œ", "ì œì¬", "ì‚¬ê³ ", "í­ë°œ", "ì¤‘ë‹¨", "ì‹¤íŒ¨",
        "ì ì", "ê¸‰ë½", "ë¶ˆë²•", "ë°°ì„", "íš¡ë ¹", "ë‹´í•©", "ìœ„ë°˜", "ì²˜ë²Œ", "íŒŒì‚°",
        "í•´ê³ ", "ê°ì¶•", "ì ë°œ", "ê¸°ì†Œ", "ë²Œê¸ˆ", "ì†ì‹¤", "í•˜ë½", "ì·¨ì†Œ", "ì² íšŒ",
        "ë¬¸ì œ", "ìš°ë ¤", "ë¹„íŒ", "ë°˜ë°œ", "ê°ˆë“±", "ì¶©ëŒ", "ë¶€ì‹¤", "ì§€ì—°"
    ]

    # ê¸ì • í‚¤ì›Œë“œ (ìµœì†Œ ì„¸íŠ¸)
    positive_keywords = [
        "í˜‘ë ¥", "í™•ëŒ€", "íˆ¬ì", "ìˆ˜ì£¼", "ê³„ì•½", "ì§„ì¶œ", "ì„±ê³¼", "ê°œì„ ", "ìƒìŠ¹",
        "í˜ì‹ ", "ì¶œì‹œ", "ìˆ˜ìƒ", "ì„ ì •", "ì±„íƒ", "ì¦ê°€", "ì„±ì¥", "ë‹¬ì„±", "ìˆ˜ìµ",
        "ê°œë°œ", "íšë“", "ì²´ê²°", "ì¦ì„¤", "í™•ë³´", "ê¸°ì—¬", "ì°½ì¶œ", "ë„ì…", "ê°•í™”"
    ]

    # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
    neg_count = sum(1 for kw in negative_keywords if kw in text)
    pos_count = sum(1 for kw in positive_keywords if kw in text)

    # íŒì • ë¡œì§
    if neg_count >= 2:
        return "neg"
    elif neg_count == 1 and pos_count == 0:
        return "neg"
    elif pos_count >= 1 and neg_count == 0:
        return "pos"
    elif pos_count > neg_count:
        return "pos"
    elif neg_count > pos_count:
        return "neg"
    else:
        return "unk"


def analyze_sentiment_llm(title: str, summary: str) -> str:
    """
    LLM ê¸°ë°˜ ê°ì„± ë¶„ì„ (2ì°¨ ë³´ì • - unkë§Œ)

    Args:
        title: ê¸°ì‚¬ ì œëª©
        summary: ê¸°ì‚¬ ìš”ì•½

    Returns:
        "pos" (ê¸ì •/ì¤‘ë¦½) or "neg" (ë¶€ì •)
    """
    try:
        # OpenAI API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY", "")
        if not api_key:
            return "unk"

        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ê¸°ì—…ì—ê²Œ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ìš”ì•½: {summary}

ë‹µë³€ì€ "ê¸ì •" ë˜ëŠ” "ë¶€ì •" ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        import requests
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,
            "max_tokens": 10
        }

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=10
        )

        if response.status_code == 200:
            result = response.json()
            answer = result["choices"][0]["message"]["content"].strip().lower()

            if "ë¶€ì •" in answer or "negative" in answer:
                return "neg"
            elif "ê¸ì •" in answer or "positive" in answer:
                return "pos"

        return "unk"

    except Exception as e:
        print(f"[DEBUG] LLM ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        return "unk"


def get_article_sentiment(title: str, summary: str, url: str = "") -> str:
    """
    ê¸°ì‚¬ ê°ì„± ë¶„ì„ (ìºì‹± + 2ë‹¨ê³„ ë¶„ì„)

    Args:
        title: ê¸°ì‚¬ ì œëª©
        summary: ê¸°ì‚¬ ìš”ì•½
        url: ê¸°ì‚¬ URL (ìºì‹± í‚¤)

    Returns:
        "pos" (ê¸ì •/ì¤‘ë¦½) or "neg" (ë¶€ì •)
    """
    # ìºì‹œ í™•ì¸
    cache_key = url if url else f"{title}|{summary}"
    if cache_key in _sentiment_cache:
        return _sentiment_cache[cache_key]

    # 1ì°¨: ê·œì¹™ ê¸°ë°˜
    sentiment = analyze_sentiment_rule_based(title, summary)

    # 2ì°¨: unkì¸ ê²½ìš°ë§Œ LLM í˜¸ì¶œ
    if sentiment == "unk":
        sentiment = analyze_sentiment_llm(title, summary)
        # LLMë„ ì‹¤íŒ¨í•˜ë©´ pos (ì¤‘ë¦½) ì²˜ë¦¬
        if sentiment == "unk":
            sentiment = "pos"

    # ìºì‹œ ì €ì¥
    _sentiment_cache[cache_key] = sentiment

    return sentiment


# ======================== ë„¤ì´ë²„ API í•¨ìˆ˜ ========================

def fetch_naver_news(query: str, start: int = 1, display: int = 50, sort: str = "date"):
    """Naver ë‰´ìŠ¤ API í˜¸ì¶œ (ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€)"""
    r = None
    try:
        url = "https://openapi.naver.com/v1/search/news.json"
        params = {"query": query, "start": start, "display": display, "sort": sort}
        headers = _naver_headers()

        if not headers.get("X-Naver-Client-Id") or not headers.get("X-Naver-Client-Secret"):
            return {"items": [], "error": "missing_keys"}

        r = requests.get(url, headers=headers, params=params, timeout=10)

        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²˜ë¦¬
        if r.status_code == 429:
            error_data = r.json() if r.text else {}
            error_msg = error_data.get("errorMessage", "API quota exceeded")
            print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ (429): {error_msg}")
            return {"items": [], "error": "quota_exceeded", "error_message": error_msg}

        r.raise_for_status()
        return r.json()

    except requests.exceptions.Timeout:
        print(f"[WARNING] Naver API timeout for query: {query}")
        return {"items": [], "error": "timeout"}
    except requests.exceptions.RequestException as e:
        print(f"[WARNING] Naver API request failed for query: {query}, error: {e}")
        if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
            return {"items": [], "error": "quota_exceeded"}
        return {"items": [], "error": "request_failed"}
    except Exception as e:
        print(f"[WARNING] Unexpected error in fetch_naver_news: {e}")
        return {"items": [], "error": "unexpected"}
    finally:
        # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€: ì‘ë‹µ ê°ì²´ ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°
        if r is not None:
            r.close()


def crawl_naver_news(query: str, max_items: int = 200, sort: str = "date") -> pd.DataFrame:
    """Naver ë‰´ìŠ¤ ìˆ˜ì§‘"""
    items, start, total = [], 1, 0
    display = min(50, max_items)
    max_attempts = 2
    attempt_count = 0
    quota_exceeded = False

    while total < max_items and start <= 100 and attempt_count < max_attempts:
        attempt_count += 1

        try:
            data = fetch_naver_news(query, start=start, display=min(display, max_items - total), sort=sort)

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
            if data.get("error") == "quota_exceeded":
                print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            arr = data.get("items", [])
            if not arr:
                break

            for it in arr:
                title = _clean_text(it.get("title"))
                desc = _clean_text(it.get("description"))
                link = it.get("originallink") or it.get("link") or ""
                pub = it.get("pubDate", "")
                try:
                    # GMT â†’ KST ë³€í™˜ í›„ tz ì œê±°
                    dt = pd.to_datetime(pub, utc=True).tz_convert("Asia/Seoul").tz_localize(None)
                    date_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    date_str = ""

                # ê°ì„± ë¶„ì„ ì¶”ê°€
                sentiment = get_article_sentiment(title, desc, link)

                items.append({
                    "ë‚ ì§œ": date_str,
                    "ë§¤ì²´ëª…": _publisher_from_link(link),
                    "ê²€ìƒ‰í‚¤ì›Œë“œ": query,
                    "ê¸°ì‚¬ì œëª©": title,
                    "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½": desc,
                    "URL": link,
                    "sentiment": sentiment
                })

            got = len(arr)
            total += got
            if got == 0:
                break
            start += got

        except Exception as e:
            print(f"[WARNING] Error in crawl_naver_news attempt {attempt_count}: {e}")
            break

    df = pd.DataFrame(items, columns=["ë‚ ì§œ", "ë§¤ì²´ëª…", "ê²€ìƒ‰í‚¤ì›Œë“œ", "ê¸°ì‚¬ì œëª©", "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", "URL", "sentiment"])

    # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì •ë³´ ì €ì¥
    if quota_exceeded:
        df.attrs['quota_exceeded'] = True

    if not df.empty:
        # ìµœì‹ ìˆœ ì •ë ¬
        df["ë‚ ì§œ_datetime"] = pd.to_datetime(df["ë‚ ì§œ"], errors="coerce")
        df = df.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
        df = df.drop("ë‚ ì§œ_datetime", axis=1)

        # ì¤‘ë³µ ì œê±°
        key = df["URL"].where(df["URL"].astype(bool), df["ê¸°ì‚¬ì œëª©"] + "|" + df["ë‚ ì§œ"])
        df = df.loc[~key.duplicated()].reset_index(drop=True)
    return df


# ======================== DB í•¨ìˆ˜ ========================

def load_news_db() -> pd.DataFrame:
    """ë‰´ìŠ¤ DB ë¡œë“œ"""
    if os.path.exists(NEWS_DB_FILE):
        try:
            df = pd.read_csv(NEWS_DB_FILE, encoding="utf-8")
            # ê¸°ì¡´ ë°ì´í„°ì— sentiment ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if "sentiment" not in df.columns:
                df["sentiment"] = "pos"  # ê¸°ë³¸ê°’: ê¸ì •/ì¤‘ë¦½
            return df
        except Exception as e:
            print(f"[WARNING] DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    return pd.DataFrame(columns=["ë‚ ì§œ","ë§¤ì²´ëª…","ê²€ìƒ‰í‚¤ì›Œë“œ","ê¸°ì‚¬ì œëª©","ì£¼ìš”ê¸°ì‚¬ ìš”ì•½","URL","sentiment"])


def save_news_db(df: pd.DataFrame):
    """ë‰´ìŠ¤ DB ì €ì¥"""
    if df.empty:
        print("[DEBUG] save_news_db skipped: empty dataframe")
        return

    # ë§¤ì²´ëª… ì •ë¦¬ (URL ê¸°ë°˜)
    if "ë§¤ì²´ëª…" in df.columns and "URL" in df.columns:
        for idx, row in df.iterrows():
            if pd.notna(row["URL"]):
                df.at[idx, "ë§¤ì²´ëª…"] = _publisher_from_link(row["URL"])

    # ìƒìœ„ 200ê°œë§Œ ì €ì¥
    out = df.head(200).copy()

    # data í´ë” ìƒì„±
    os.makedirs(DATA_FOLDER, exist_ok=True)

    out.to_csv(NEWS_DB_FILE, index=False, encoding="utf-8")
    print(f"[DEBUG] news saved: {len(out)} rows -> {NEWS_DB_FILE}")


# ======================== ìºì‹œ í•¨ìˆ˜ ========================

def load_sent_cache() -> set:
    """ì „ì†¡ëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ (TTL ì ìš©)"""
    if os.path.exists(SENT_CACHE_FILE):
        try:
            with open(SENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)

                # TTL ì§€ì› ì—¬ë¶€ í™•ì¸
                if "url_timestamps" in data:
                    # TTL ê¸°ë°˜ ìºì‹œ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
                    from datetime import timedelta
                    url_timestamps = data.get("url_timestamps", {})
                    ttl_days = data.get("ttl_days", 7)
                    now = datetime.now()
                    cutoff_time = now - timedelta(days=ttl_days)

                    # ìœ íš¨í•œ URLë§Œ ë¡œë“œ
                    valid_urls = set()
                    expired_count = 0
                    for url, timestamp_str in url_timestamps.items():
                        try:
                            timestamp = datetime.fromisoformat(timestamp_str)
                            if timestamp > cutoff_time:
                                valid_urls.add(url)
                            else:
                                expired_count += 1
                        except Exception:
                            # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨ (ì•ˆì „ ì¥ì¹˜)
                            valid_urls.add(url)

                    if expired_count > 0:
                        print(f"[DEBUG] TTL ë§Œë£Œ URL ì œê±°: {expired_count}ê±´")

                    print(f"[DEBUG] ì „ì†¡ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(valid_urls)}ê±´ (TTL: {ttl_days}ì¼)")
                    return valid_urls
                else:
                    # ë ˆê±°ì‹œ ìºì‹œ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ)
                    cache = set(data.get("urls", []))
                    print(f"[DEBUG] ì „ì†¡ ìºì‹œ ë¡œë“œ ì™„ë£Œ (ë ˆê±°ì‹œ): {len(cache)}ê±´")
                    return cache

        except Exception as e:
            print(f"[WARNING] ì „ì†¡ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()
    else:
        print(f"[DEBUG] ì „ì†¡ ìºì‹œ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
        return set()


def save_sent_cache(cache: set, ttl_days: int = 7):
    """
    ì „ì†¡ëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥ (TTL ê¸°ë°˜, ì›ìì  ì“°ê¸°)

    Args:
        cache: ì €ì¥í•  URL ìºì‹œ
        ttl_days: TTL (Time To Live) ì¼ìˆ˜ (ê¸°ë³¸: 7ì¼)
    """
    try:
        from datetime import timedelta
        import tempfile
        os.makedirs(DATA_FOLDER, exist_ok=True)

        # ê¸°ì¡´ ìºì‹œ ë¡œë“œ (íƒ€ì„ìŠ¤íƒ¬í”„ ìœ ì§€)
        existing_timestamps = {}
        if os.path.exists(SENT_CACHE_FILE):
            try:
                with open(SENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_timestamps = existing_data.get("url_timestamps", {})
            except Exception:
                pass

        # í˜„ì¬ ì‹œê°„
        now = datetime.now()
        cutoff_time = now - timedelta(days=ttl_days)

        # URL íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        url_timestamps = {}
        for url in cache:
            if url in existing_timestamps:
                # ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ìœ ì§€
                try:
                    timestamp = datetime.fromisoformat(existing_timestamps[url])
                    if timestamp > cutoff_time:
                        url_timestamps[url] = existing_timestamps[url]
                    else:
                        # ë§Œë£Œëœ ê²½ìš° í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ê°±ì‹ 
                        url_timestamps[url] = now.isoformat()
                except Exception:
                    url_timestamps[url] = now.isoformat()
            else:
                # ì‹ ê·œ URLì€ í˜„ì¬ ì‹œê°„
                url_timestamps[url] = now.isoformat()

        # ìµœì‹  MAX_SENT_CACHEê°œë§Œ ìœ ì§€ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
        if len(url_timestamps) > MAX_SENT_CACHE:
            sorted_urls = sorted(url_timestamps.items(), key=lambda x: x[1], reverse=True)
            url_timestamps = dict(sorted_urls[:MAX_SENT_CACHE])

        data = {
            "url_timestamps": url_timestamps,
            "urls": list(url_timestamps.keys()),  # í•˜ìœ„ í˜¸í™˜ì„±
            "last_updated": now.strftime("%Y-%m-%d %H:%M:%S"),
            "count": len(url_timestamps),
            "ttl_days": ttl_days
        }

        # ì›ìì  ì“°ê¸° (ì„ì‹œ íŒŒì¼ + rename)
        # ë™ì‹œ ì“°ê¸° ì‹œì—ë„ íŒŒì¼ ì†ìƒ ë°©ì§€
        temp_fd, temp_path = tempfile.mkstemp(dir=DATA_FOLDER, suffix='.tmp')
        try:
            # os.fdopenì´ temp_fdë¥¼ ì¸ìˆ˜ë°›ìœ¼ë¯€ë¡œ, ì´í›„ temp_fdëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
            with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # Windowsì—ì„œëŠ” ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•„ìš”
            if os.path.exists(SENT_CACHE_FILE):
                try:
                    os.remove(SENT_CACHE_FILE)
                except Exception:
                    pass

            # ì„ì‹œ íŒŒì¼ì„ ìµœì¢… íŒŒì¼ë¡œ ì´ë™ (ì›ìì  ì—°ì‚°)
            os.replace(temp_path, SENT_CACHE_FILE)

            print(f"[DEBUG] ì „ì†¡ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(url_timestamps)}ê±´ (TTL: {ttl_days}ì¼) -> {SENT_CACHE_FILE}")
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ - íŒŒì¼ì´ ì´ë¯¸ ë‹«í˜”ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
            try:
                if os.path.exists(temp_path):
                    os.remove(temp_path)
            except Exception:
                pass
            raise e
    except Exception as e:
        print(f"[WARNING] ì „ì†¡ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# ======================== Pending í ê´€ë¦¬ ========================

def _generate_article_hash(title: str, date: str) -> str:
    """
    ê¸°ì‚¬ í•´ì‹œ ID ìƒì„± (URL ì™¸ ë³´ì¡° ì‹ë³„ì)
    - title + date ì¡°í•©ìœ¼ë¡œ í•´ì‹œ ìƒì„±
    - ê°™ì€ ê¸°ì‚¬ê°€ ë‹¤ë¥¸ URLë¡œ ì˜¬ ê²½ìš° ëŒ€ì‘
    """
    import hashlib
    try:
        combined = f"{title}|{date}".strip()
        return hashlib.md5(combined.encode('utf-8')).hexdigest()[:16]
    except Exception:
        return ""


def load_pending_queue() -> dict:
    """
    Pending í ë¡œë“œ (TTL ì ìš©)

    Returns:
        dict: {url: {title, link, date, press, keyword, retry_count, last_attempt, hash_id}}
    """
    if os.path.exists(PENDING_QUEUE_FILE):
        try:
            with open(PENDING_QUEUE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)

                pending_queue = data.get("queue", {})
                now = datetime.now()

                # TTL ì ìš©: ì˜¤ë˜ëœ ê¸°ì‚¬ ì œê±°
                valid_queue = {}
                expired_count = 0
                for url, article in pending_queue.items():
                    try:
                        last_attempt = datetime.fromisoformat(article.get("last_attempt", ""))
                        hours_diff = (now - last_attempt).total_seconds() / 3600

                        if hours_diff <= PENDING_TTL_HOURS:
                            valid_queue[url] = article
                        else:
                            expired_count += 1
                    except Exception:
                        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨ (ì•ˆì „ ì¥ì¹˜)
                        valid_queue[url] = article

                if expired_count > 0:
                    print(f"[DEBUG] Pending í TTL ë§Œë£Œ: {expired_count}ê±´ ì œê±°")

                print(f"[DEBUG] Pending í ë¡œë“œ: {len(valid_queue)}ê±´ (TTL: {PENDING_TTL_HOURS}ì‹œê°„)")
                return valid_queue

        except Exception as e:
            print(f"[WARNING] Pending í ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    else:
        print(f"[DEBUG] Pending í íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
        return {}


def save_pending_queue(queue: dict):
    """
    Pending í ì €ì¥ (ì›ìì  ì“°ê¸°)

    Args:
        queue: {url: {title, link, date, press, keyword, retry_count, last_attempt, hash_id}}
    """
    try:
        import tempfile
        os.makedirs(DATA_FOLDER, exist_ok=True)

        data = {
            "queue": queue,
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "count": len(queue)
        }

        # ì›ìì  ì“°ê¸° (ì„ì‹œ íŒŒì¼ + rename)
        temp_fd, temp_path = tempfile.mkstemp(dir=DATA_FOLDER, suffix='.tmp')
        try:
            # os.fdopenì´ temp_fdë¥¼ ì¸ìˆ˜ë°›ìœ¼ë¯€ë¡œ, ì´í›„ temp_fdëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
            with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # Windowsì—ì„œëŠ” ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•„ìš”
            if os.path.exists(PENDING_QUEUE_FILE):
                try:
                    os.remove(PENDING_QUEUE_FILE)
                except Exception:
                    pass

            os.replace(temp_path, PENDING_QUEUE_FILE)
            print(f"[DEBUG] Pending í ì €ì¥ ì™„ë£Œ: {len(queue)}ê±´ -> {PENDING_QUEUE_FILE}")
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ - íŒŒì¼ì´ ì´ë¯¸ ë‹«í˜”ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
            try:
                if os.path.exists(temp_path):
                    os.remove(temp_path)
            except Exception:
                pass
            raise e
    except Exception as e:
        print(f"[WARNING] Pending í ì €ì¥ ì‹¤íŒ¨: {e}")


def add_to_pending(article: dict, pending_queue: dict) -> dict:
    """
    Pending íì— ê¸°ì‚¬ ì¶”ê°€

    Args:
        article: {title, link, date, press, keyword}
        pending_queue: í˜„ì¬ pending í

    Returns:
        dict: ì—…ë°ì´íŠ¸ëœ pending í
    """
    try:
        url = article.get("link", "")
        if not url:
            return pending_queue

        # ì´ë¯¸ pendingì— ìˆìœ¼ë©´ ìŠ¤í‚µ
        if url in pending_queue:
            return pending_queue

        # í•´ì‹œ ID ìƒì„±
        hash_id = _generate_article_hash(article.get("title", ""), article.get("date", ""))

        pending_queue[url] = {
            "title": article.get("title", ""),
            "link": url,
            "date": article.get("date", ""),
            "press": article.get("press", ""),
            "keyword": article.get("keyword", ""),
            "retry_count": 0,
            "last_attempt": datetime.now().isoformat(),
            "hash_id": hash_id
        }

        print(f"[DEBUG] Pending í ì¶”ê°€: {article.get('title', '')[:50]}...")
        return pending_queue

    except Exception as e:
        print(f"[WARNING] Pending í ì¶”ê°€ ì‹¤íŒ¨: {e}")
        return pending_queue


def remove_from_pending(url: str, pending_queue: dict) -> dict:
    """
    Pending íì—ì„œ ê¸°ì‚¬ ì œê±°

    Args:
        url: ì œê±°í•  ê¸°ì‚¬ URL
        pending_queue: í˜„ì¬ pending í

    Returns:
        dict: ì—…ë°ì´íŠ¸ëœ pending í
    """
    try:
        if url in pending_queue:
            title = pending_queue[url].get("title", "")
            del pending_queue[url]
            print(f"[DEBUG] Pending í ì œê±°: {title[:50]}...")
        return pending_queue
    except Exception as e:
        print(f"[WARNING] Pending í ì œê±° ì‹¤íŒ¨: {e}")
        return pending_queue


# ======================== API í• ë‹¹ëŸ‰ ê´€ë¦¬ ========================

def load_api_usage() -> int:
    """ì˜¤ëŠ˜ API ì‚¬ìš©ëŸ‰ ë¡œë“œ"""
    if os.path.exists(API_USAGE_FILE):
        try:
            with open(API_USAGE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                today = datetime.now().strftime("%Y-%m-%d")
                if data.get("date") == today:
                    return data.get("count", 0)
        except Exception as e:
            print(f"[WARNING] API ì‚¬ìš©ëŸ‰ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return 0


def save_api_usage(count: int):
    """ì˜¤ëŠ˜ API ì‚¬ìš©ëŸ‰ ì €ì¥"""
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)
        today = datetime.now().strftime("%Y-%m-%d")
        data = {
            "date": today,
            "count": count,
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "quota_remaining": MAX_API_CALLS_PER_DAY - count,
            "quota_percentage": (count / MAX_API_CALLS_PER_DAY) * 100
        }
        with open(API_USAGE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"[DEBUG] API ì‚¬ìš©ëŸ‰ ì €ì¥: {count}/{MAX_API_CALLS_PER_DAY} ({data['quota_percentage']:.1f}%)")
    except Exception as e:
        print(f"[WARNING] API ì‚¬ìš©ëŸ‰ ì €ì¥ ì‹¤íŒ¨: {e}")


def increment_api_usage(calls: int = 1) -> int:
    """API ì‚¬ìš©ëŸ‰ ì¦ê°€ ë° í˜„ì¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
    current = load_api_usage()
    new_count = current + calls
    save_api_usage(new_count)
    return new_count


def check_api_quota(required_calls: int = 1) -> bool:
    """
    API í• ë‹¹ëŸ‰ í™•ì¸

    Args:
        required_calls: í•„ìš”í•œ API í˜¸ì¶œ íšŸìˆ˜

    Returns:
        True: í• ë‹¹ëŸ‰ ì—¬ìœ  ìˆìŒ, False: í• ë‹¹ëŸ‰ ë¶€ì¡±
    """
    current = load_api_usage()
    remaining = MAX_API_CALLS_PER_DAY - current

    if remaining < required_calls:
        print(f"[WARNING] âš ï¸ API í• ë‹¹ëŸ‰ ë¶€ì¡±: ë‚¨ì€ í˜¸ì¶œ {remaining}íšŒ, í•„ìš” {required_calls}íšŒ")
        return False

    if current >= API_QUOTA_WARNING_THRESHOLD:
        print(f"[WARNING] âš ï¸ API í• ë‹¹ëŸ‰ 80% ë„ë‹¬: {current}/{MAX_API_CALLS_PER_DAY} ({(current/MAX_API_CALLS_PER_DAY)*100:.1f}%)")

    return True


# ======================== ì´ˆê¸°í™” ìƒíƒœ ê´€ë¦¬ ========================

def is_first_run() -> bool:
    """ì²« ì‹¤í–‰ ì—¬ë¶€ í™•ì¸ (ìƒíƒœ íŒŒì¼ ê¸°ì¤€)"""
    if not os.path.exists(STATE_FILE):
        return True

    try:
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            state = json.load(f)
            return not state.get("initialized", False)
    except Exception as e:
        print(f"[WARNING] ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return True


def mark_initialized():
    """ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ"""
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)
        state = {
            "initialized": True,
            "first_run_date": datetime.now().isoformat(),
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        print(f"[DEBUG] ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ")
    except Exception as e:
        print(f"[WARNING] ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


# ======================== ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ ========================

def detect_new_articles(old_df: pd.DataFrame, new_df: pd.DataFrame, sent_cache: set) -> list:
    """
    ê¸°ì¡´ DBì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
    - URLì„ ìš°ì„  ì‹ë³„ìë¡œ ì‚¬ìš©
    - ìºì‹œì™€ DB ì¤‘ë³µ ì²´í¬
    """
    try:
        # ì²« ì‹¤í–‰ ì²´í¬ (ìƒíƒœ íŒŒì¼ ê¸°ì¤€)
        if is_first_run():
            print(f"[DEBUG] ì²« ì‹¤í–‰ ê°ì§€ - ì•Œë¦¼ ìŠ¤í‚µí•˜ê³  ì´ˆê¸°í™”")
            mark_initialized()
            return []

        # DBê°€ ë¹„ì–´ìˆì§€ë§Œ ì²« ì‹¤í–‰ì´ ì•„ë‹ˆë©´ ê²½ê³  (ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±)
        if old_df.empty and not is_first_run():
            print(f"[WARNING] âš ï¸ DBê°€ ë¹„ì–´ìˆì§€ë§Œ ì²« ì‹¤í–‰ì´ ì•„ë‹˜ - ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±")
            # ì´ ê²½ìš°ì—ë„ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì²˜ë¦¬ (ë³µêµ¬ ëª©ì )

        if new_df.empty:
            return []

        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (KST)
        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST).replace(tzinfo=None)  # KST ì‹œê°„ì„ naive datetimeìœ¼ë¡œ

        # ê¸°ì¡´ DBì˜ URL + í•´ì‹œ ID ì„¸íŠ¸ ìƒì„± (ê°•í™”ëœ ì¤‘ë³µ ì²´í¬)
        old_urls = set()
        old_urls_normalized = set()
        old_hash_ids = set()  # í•´ì‹œ ID ê¸°ë°˜ ì¤‘ë³µ ì²´í¬

        for _, row in old_df.iterrows():
            url = str(row.get("URL", "")).strip()
            if url and url != "nan" and url != "":
                old_urls.add(url)
                old_urls_normalized.add(_normalize_url(url))

                # í•´ì‹œ ID ìƒì„± ë° ìˆ˜ì§‘
                title = str(row.get("ê¸°ì‚¬ì œëª©", "")).strip()
                date = str(row.get("ë‚ ì§œ", "")).strip()
                hash_id = _generate_article_hash(title, date)
                if hash_id:
                    old_hash_ids.add(hash_id)

        print(f"[DEBUG] ê¸°ì¡´ DB URL ìˆ˜: {len(old_urls)} (ì •ê·œí™”: {len(old_urls_normalized)})")
        print(f"[DEBUG] ê¸°ì¡´ DB í•´ì‹œ ID ìˆ˜: {len(old_hash_ids)}ê±´")
        print(f"[DEBUG] ìºì‹œ í¬ê¸°: {len(sent_cache)}ê±´")
        print(f"[DEBUG] ìˆ˜ì§‘ëœ ì‹ ê·œ ë°ì´í„° ìˆ˜: {len(new_df)}")

        # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (ì‹œê°„ í•„í„°ë§ ì¶”ê°€)
        new_articles = []
        MAX_ARTICLE_AGE_HOURS = 168  # ìµœê·¼ 7ì¼(168ì‹œê°„) ì´ë‚´ ê¸°ì‚¬ë§Œ ì•Œë¦¼ (ì´ì „: 48ì‹œê°„)

        for _, row in new_df.iterrows():
            url = str(row.get("URL", "")).strip()
            title = str(row.get("ê¸°ì‚¬ì œëª©", "")).strip()
            article_date_str = row.get("ë‚ ì§œ", "")

            # URLì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
            if not url or url == "nan" or url == "":
                continue

            # URL ì •ê·œí™”
            url_normalized = _normalize_url(url)

            # í•´ì‹œ ID ìƒì„± (ë³´ì¡° ì‹ë³„ì)
            hash_id = _generate_article_hash(title, article_date_str)

            # 4ë‹¨ê³„ ì¤‘ë³µ ì²´í¬: URL + ì •ê·œí™” URL + ìºì‹œ + í•´ì‹œ ID
            is_in_db_url = url in old_urls or url_normalized in old_urls_normalized
            is_in_cache = url in sent_cache or url_normalized in sent_cache
            is_in_db_hash = hash_id in old_hash_ids if hash_id else False

            if is_in_db_url or is_in_cache or is_in_db_hash:
                if is_in_db_hash and not is_in_db_url:
                    print(f"[DEBUG] ğŸ” í•´ì‹œ ID ì¤‘ë³µ ê°ì§€ (ë‹¤ë¥¸ URL): {title[:50]}...")
                continue

            # ì‹ ê·œ ê¸°ì‚¬ - ë‚ ì§œ í•„í„°ë§ (ê°œì„ ë¨)
            article_date_str = row.get("ë‚ ì§œ", "")
            should_notify = True  # ê¸°ë³¸ê°’: ì‹ ê·œ ê¸°ì‚¬ë©´ ì•Œë¦¼
            hours_diff = None

            try:
                article_date = pd.to_datetime(article_date_str, errors="coerce")
                if pd.notna(article_date):
                    time_diff = now - article_date
                    hours_diff = time_diff.total_seconds() / 3600

                    # ì‹œê°„ ê¸°ë°˜ í•„í„°ë§: ìµœê·¼ 7ì¼ ì´ë‚´ë§Œ ì•Œë¦¼
                    if hours_diff <= MAX_ARTICLE_AGE_HOURS:
                        print(f"[DEBUG] âœ… ì‹ ê·œ ê¸°ì‚¬ ê°ì§€: {title[:50]}... ({hours_diff:.1f}ì‹œê°„ ì „, {hours_diff/24:.1f}ì¼ ì „)")
                    else:
                        print(f"[DEBUG] â­ï¸ ì˜¤ë˜ëœ ê¸°ì‚¬ ìŠ¤í‚µ: {title[:50]}... ({hours_diff:.1f}ì‹œê°„ ì „, {hours_diff/24:.1f}ì¼ ì „)")
                        continue  # 7ì¼ ì´ìƒ ì˜¤ë˜ëœ ê¸°ì‚¬ëŠ” ì•Œë¦¼ ìŠ¤í‚µ
                else:
                    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì²˜ë¦¬ (ê°œì„ !)
                    print(f"[DEBUG] âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, í•˜ì§€ë§Œ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì•Œë¦¼: {title[:50]}... (ë‚ ì§œ: {article_date_str})")
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì²˜ë¦¬ (ê°œì„ !)
                print(f"[DEBUG] âš ï¸ ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜, í•˜ì§€ë§Œ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì•Œë¦¼: {title[:50]}... - {str(e)}")

            # ë§¤ì²´ëª…ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            press = _publisher_from_link(url)
            keyword = str(row.get("ê²€ìƒ‰í‚¤ì›Œë“œ", "")).strip()

            new_articles.append({
                "title": title if title and title != "nan" else "ì œëª© ì—†ìŒ",
                "link": url,
                "date": article_date_str,
                "press": press,
                "keyword": keyword
            })

        print(f"[DEBUG] ì´ {len(new_articles)}ê±´ì˜ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (DB+ìºì‹œ ì¤‘ë³µ ì œê±°)")
        return new_articles

    except Exception as e:
        print(f"[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return []


# ======================== í…”ë ˆê·¸ë¨ ì•Œë¦¼ ========================

def process_pending_queue_and_send(pending_queue: dict, sent_cache: set) -> tuple:
    """
    Pending íì˜ ê¸°ì‚¬ë“¤ì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ (ê°œì„ ëœ ë²„ì „)

    í•µì‹¬ ê°œì„ ì‚¬í•­:
    - Pending í ê¸°ë°˜ ì „ì†¡ (ëˆ„ë½ ë°©ì§€)
    - í…”ë ˆê·¸ë¨ 429 ì‘ë‹µì˜ retry_after í—¤ë” ì²˜ë¦¬
    - ì¬ì‹œë„ íšŸìˆ˜ ì¶”ì  (ìµœëŒ€ 5íšŒ)
    - ì „ì†¡ ì„±ê³µ ì‹œ pendingì—ì„œ ì œê±° + sent_cache ì¶”ê°€
    - ì „ì†¡ ì‹¤íŒ¨ ì‹œ retry_count ì¦ê°€, ìµœëŒ€ ì´ˆê³¼ ì‹œ ì œê±°

    Args:
        pending_queue: {url: {title, link, date, press, keyword, retry_count, last_attempt, hash_id}}
        sent_cache: ì „ì†¡ ì™„ë£Œëœ ê¸°ì‚¬ URL ìºì‹œ

    Returns:
        tuple: (ì—…ë°ì´íŠ¸ëœ pending_queue, ì—…ë°ì´íŠ¸ëœ sent_cache, ì „ì†¡ ì„±ê³µ ìˆ˜)
    """
    import time
    import traceback

    try:
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        print(f"[DEBUG] Pending í ì²˜ë¦¬ ì‹œì‘ - ëŒ€ê¸° ì¤‘ì¸ ê¸°ì‚¬: {len(pending_queue)}ê±´")

        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not bot_token or not chat_id:
            print("[DEBUG] âš ï¸ í…”ë ˆê·¸ë¨ ì„¤ì • ì—†ìŒ - ì „ì†¡ ìŠ¤í‚µ")
            return pending_queue, sent_cache, 0

        if not pending_queue:
            print("[DEBUG] Pending í ë¹„ì–´ìˆìŒ - ì „ì†¡í•  ê¸°ì‚¬ ì—†ìŒ")
            return pending_queue, sent_cache, 0

        # í…”ë ˆê·¸ë¨ API URL
        api_url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

        success_count = 0
        failed_count = 0
        max_retry_exceeded_count = 0

        # Pending íë¥¼ ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ (ê³¼ê±° â†’ ìµœì‹  ìˆœì„œë¡œ ì „ì†¡)
        urls_to_remove = []

        # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê¸°ì‚¬ë¶€í„° ì „ì†¡)
        sorted_items = sorted(
            pending_queue.items(),
            key=lambda x: x[1].get("date", ""),  # ë‚ ì§œ ë¬¸ìì—´ë¡œ ì •ë ¬ (YYYY-MM-DD HH:MM í˜•ì‹)
            reverse=False  # False = ì˜¤ë¦„ì°¨ìˆœ (ê³¼ê±° â†’ ìµœì‹ )
        )

        for url, article in sorted_items:
            title = article.get("title", "ì œëª© ì—†ìŒ")
            link = article.get("link", url)
            date = article.get("date", "")
            press = article.get("press", "")
            keyword = article.get("keyword", "")
            retry_count = article.get("retry_count", 0)

            # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ì²´í¬
            if retry_count >= MAX_PENDING_RETRY:
                print(f"[DEBUG] âŒ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ({retry_count}íšŒ) - ì œê±°: {title[:50]}...")
                urls_to_remove.append(url)
                max_retry_exceeded_count += 1
                continue

            # ë©”ì‹œì§€ êµ¬ì„±
            message = f"ğŸš¨ *ìƒˆ ë‰´ìŠ¤*\n\n"
            if keyword:
                hashtag = keyword.replace(" ", "")
                message += f"#{hashtag}\n"
            if press:
                message += f"*[{press}]* {title}\n"
            else:
                message += f"*{title}*\n"
            if date:
                message += f"ğŸ• {date}\n"
            if link:
                message += f"ğŸ”— {link}"

            payload = {
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": True
            }

            response = None
            send_success = False

            try:
                # í…”ë ˆê·¸ë¨ API í˜¸ì¶œ (timeout ê°•í™”: connect 3ì´ˆ, read 10ì´ˆ)
                response = requests.post(api_url, json=payload, timeout=(3, 10))

                if response.status_code == 200:
                    # ì „ì†¡ ì„±ê³µ
                    success_count += 1
                    send_success = True
                    print(f"[DEBUG] âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {title[:50]}...")

                    # sent_cacheì— ì¶”ê°€
                    sent_cache.add(link)
                    sent_cache.add(_normalize_url(link))

                    # pendingì—ì„œ ì œê±° ì˜ˆì•½
                    urls_to_remove.append(url)

                elif response.status_code == 429:
                    # Rate Limit - retry_after í—¤ë” ì²´í¬
                    retry_after = None
                    try:
                        error_data = response.json()
                        retry_after = error_data.get("parameters", {}).get("retry_after")
                    except Exception:
                        pass

                    if retry_after:
                        print(f"[DEBUG] âš ï¸ Rate Limit (429) - {retry_after}ì´ˆ í›„ ì¬ì‹œë„ ê¶Œì¥")
                        time.sleep(retry_after)
                    else:
                        print(f"[DEBUG] âš ï¸ Rate Limit (429) - retry_after ì—†ìŒ, 5ì´ˆ ëŒ€ê¸°")
                        time.sleep(5)

                    # retry_count ì¦ê°€
                    pending_queue[url]["retry_count"] = retry_count + 1
                    pending_queue[url]["last_attempt"] = datetime.now().isoformat()
                    failed_count += 1

                else:
                    # ê¸°íƒ€ ì—ëŸ¬
                    print(f"[DEBUG] âŒ ì „ì†¡ ì‹¤íŒ¨ ({response.status_code}): {title[:50]}...")
                    pending_queue[url]["retry_count"] = retry_count + 1
                    pending_queue[url]["last_attempt"] = datetime.now().isoformat()
                    failed_count += 1

            except requests.exceptions.Timeout:
                print(f"[DEBUG] â±ï¸ íƒ€ì„ì•„ì›ƒ: {title[:50]}...")
                pending_queue[url]["retry_count"] = retry_count + 1
                pending_queue[url]["last_attempt"] = datetime.now().isoformat()
                failed_count += 1

            except requests.exceptions.RequestException as e:
                print(f"[DEBUG] âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {title[:50]}... - {str(e)}")
                pending_queue[url]["retry_count"] = retry_count + 1
                pending_queue[url]["last_attempt"] = datetime.now().isoformat()
                failed_count += 1

            except Exception as e:
                print(f"[DEBUG] âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {title[:50]}... - {str(e)}")
                print(f"[DEBUG] ìƒì„¸:\n{traceback.format_exc()}")
                pending_queue[url]["retry_count"] = retry_count + 1
                pending_queue[url]["last_attempt"] = datetime.now().isoformat()
                failed_count += 1

            finally:
                # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€
                if response is not None:
                    response.close()

            # Rate Limit ë°©ì§€: ë©”ì‹œì§€ë‹¹ ìµœì†Œ 100ms ëŒ€ê¸°
            # (í…”ë ˆê·¸ë¨ ê·¸ë£¹: ì´ˆë‹¹ 20ê°œ, ê°œì¸: ì´ˆë‹¹ 30ê°œ ì œí•œ)
            if not send_success:
                time.sleep(0.1)  # ì‹¤íŒ¨ ì‹œ ì§§ê²Œ ëŒ€ê¸°
            else:
                time.sleep(0.05)  # ì„±ê³µ ì‹œ 50ms ëŒ€ê¸°

        # Pending íì—ì„œ ì œê±°
        for url in urls_to_remove:
            pending_queue = remove_from_pending(url, pending_queue)

        # ì „ì†¡ ê²°ê³¼ í†µê³„
        print(f"[DEBUG] âœ… ì „ì†¡ ì„±ê³µ: {success_count}ê±´")
        if failed_count > 0:
            print(f"[DEBUG] âš ï¸ ì „ì†¡ ì‹¤íŒ¨: {failed_count}ê±´ (ë‹¤ìŒ ì‚¬ì´í´ì— ì¬ì‹œë„)")
        if max_retry_exceeded_count > 0:
            print(f"[DEBUG] âŒ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {max_retry_exceeded_count}ê±´ (ì˜êµ¬ ì œê±°)")

        total = success_count + failed_count + max_retry_exceeded_count
        if total > 0:
            print(f"[DEBUG] ğŸ“Š ì „ì†¡ ì„±ê³µë¥ : {success_count/total*100:.1f}% ({success_count}/{total})")

        return pending_queue, sent_cache, success_count

    except Exception as e:
        print(f"[DEBUG] âŒ Pending í ì²˜ë¦¬ ì˜ˆì™¸: {str(e)}")
        print(f"[DEBUG] ìƒì„¸:\n{traceback.format_exc()}")
        return pending_queue, sent_cache, 0


def send_telegram_notification(new_articles: list, sent_cache: set) -> set:
    """
    [ë ˆê±°ì‹œ í˜¸í™˜ì„± ìœ ì§€] ì‹ ê·œ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡

    ì‹¤ì œ ì „ì†¡ì€ pending íë¥¼ í†µí•´ ì²˜ë¦¬ë¨.
    ì´ í•¨ìˆ˜ëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€.

    Args:
        new_articles: ì‹ ê·œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        sent_cache: ì „ì†¡ ì™„ë£Œëœ ê¸°ì‚¬ ìºì‹œ

    Returns:
        ì—…ë°ì´íŠ¸ëœ sent_cache
    """
    # ì´ í•¨ìˆ˜ëŠ” ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    # ì‹¤ì œ ë¡œì§ì€ process_pending_queue_and_send()ë¡œ ì´ë™
    print(f"[DEBUG] send_telegram_notification í˜¸ì¶œ (ë ˆê±°ì‹œ) - {len(new_articles)}ê±´")
    return sent_cache


# ======================== ì‹œìŠ¤í…œ ìƒíƒœ ê´€ë¦¬ ========================

RUN_STATUS_FILE = os.path.join(DATA_FOLDER, "run_status.json")


def update_run_status(success: bool, articles_collected: int, new_articles: int,
                      telegram_sent: int, error_message: str = None):
    """
    ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì—°ì† ì‹¤íŒ¨ ì¶”ì 

    Args:
        success: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        articles_collected: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜
        new_articles: ì‹ ê·œ ê¸°ì‚¬ ìˆ˜
        telegram_sent: í…”ë ˆê·¸ë¨ ë°œì†¡ ìˆ˜
        error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
    """
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)

        # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ
        status_data = {
            "consecutive_failures": 0,
            "last_success_time": None,
            "total_runs": 0,
            "total_failures": 0
        }

        if os.path.exists(RUN_STATUS_FILE):
            try:
                with open(RUN_STATUS_FILE, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
            except Exception:
                pass

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        now = datetime.now()
        status_data["total_runs"] = status_data.get("total_runs", 0) + 1
        status_data["last_run_time"] = now.isoformat()

        if success:
            status_data["consecutive_failures"] = 0
            status_data["last_success_time"] = now.isoformat()
            status_data["last_success_stats"] = {
                "articles_collected": articles_collected,
                "new_articles": new_articles,
                "telegram_sent": telegram_sent
            }
        else:
            status_data["consecutive_failures"] = status_data.get("consecutive_failures", 0) + 1
            status_data["total_failures"] = status_data.get("total_failures", 0) + 1
            status_data["last_error"] = error_message

        # íŒŒì¼ ì €ì¥
        with open(RUN_STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)

        # ì—°ì† ì‹¤íŒ¨ ê²½ê³ 
        if status_data["consecutive_failures"] >= 3:
            send_system_alert(
                f"ğŸš¨ *ì—°ì† {status_data['consecutive_failures']}íšŒ ì‹¤íŒ¨*\n\n"
                f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œì´ ì—°ì†ìœ¼ë¡œ ì‹¤íŒ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
                f"ë§ˆì§€ë§‰ ì—ëŸ¬: {error_message or 'ì•Œ ìˆ˜ ì—†ìŒ'}"
            )

        print(f"[DEBUG] ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì—°ì† ì‹¤íŒ¨ {status_data['consecutive_failures']}íšŒ")

    except Exception as e:
        print(f"[WARNING] ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def check_api_quota_and_alert():
    """
    API í• ë‹¹ëŸ‰ í™•ì¸ ë° ê²½ê³  ì•Œë¦¼

    Returns:
        bool: API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        usage = load_api_usage()
        remaining = MAX_API_CALLS_PER_DAY - usage
        usage_percent = (usage / MAX_API_CALLS_PER_DAY) * 100

        print(f"[DEBUG] API ì‚¬ìš©ëŸ‰: {usage:,}/{MAX_API_CALLS_PER_DAY:,} ({usage_percent:.1f}%)")

        # 80% ë„ë‹¬ ì‹œ ê²½ê³ 
        if usage >= API_QUOTA_WARNING_THRESHOLD and usage < (API_QUOTA_WARNING_THRESHOLD + 100):
            send_system_alert(
                f"âš ï¸ *API í• ë‹¹ëŸ‰ ê²½ê³ *\n\n"
                f"Naver API ì‚¬ìš©ëŸ‰: {usage:,}/{MAX_API_CALLS_PER_DAY:,}\n"
                f"ì‚¬ìš©ë¥ : {usage_percent:.1f}%\n"
                f"ë‚¨ì€ í˜¸ì¶œ ìˆ˜: {remaining:,}"
            )

        # 95% ë„ë‹¬ ì‹œ ê¸´ê¸‰ ê²½ê³ 
        elif usage >= (MAX_API_CALLS_PER_DAY * 0.95):
            send_system_alert(
                f"ğŸš¨ *API í• ë‹¹ëŸ‰ ê¸´ê¸‰*\n\n"
                f"ì‚¬ìš©ëŸ‰ì´ 95%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!\n"
                f"ì‚¬ìš©: {usage:,}/{MAX_API_CALLS_PER_DAY:,}\n"
                f"ì¼ë¶€ í‚¤ì›Œë“œ ìˆ˜ì§‘ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        return usage < MAX_API_CALLS_PER_DAY

    except Exception as e:
        print(f"[WARNING] API í• ë‹¹ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True


def send_system_alert(message: str):
    """
    ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡ (ì¤‘ë³µ ë°©ì§€ í¬í•¨)

    Args:
        message: ì•Œë¦¼ ë©”ì‹œì§€
    """
    try:
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        if not bot_token or not chat_id:
            return

        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }

        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            print(f"[DEBUG] âœ… ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ")
        else:
            print(f"[DEBUG] âŒ ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")

        response.close()

    except Exception as e:
        print(f"[DEBUG] âŒ ì‹œìŠ¤í…œ ì•Œë¦¼ ì˜ˆì™¸: {e}")
