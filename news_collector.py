"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ê³µí†µ ëª¨ë“ˆ
Streamlit Appê³¼ Standalone Monitorê°€ ê³µìœ í•˜ëŠ” ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§
"""
import os
import re
import urllib.parse
import json
from datetime import datetime, timezone, timedelta
from html import unescape
import pandas as pd
import requests

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# ======================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ========================

# ì›ë³¸ print ì €ì¥
_original_print = print

def safe_print(*args, **kwargs):
    """Windows cp949 ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€ìš© ì•ˆì „í•œ print"""
    try:
        _original_print(*args, **kwargs)
    except (UnicodeEncodeError, OSError):
        # ì´ëª¨ì§€ ì œê±° í›„ ì¬ì‹œë„
        try:
            safe_args = []
            for arg in args:
                if isinstance(arg, str):
                    # ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
                    safe_arg = arg.encode('ascii', 'ignore').decode('ascii')
                    safe_args.append(safe_arg)
                else:
                    safe_args.append(arg)
            _original_print(*safe_args, **kwargs)
        except:
            # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ë¬´ì‹œ
            pass

# ì „ì—­ printë¥¼ safe_printë¡œ ì˜¤ë²„ë¼ì´ë“œ
print = safe_print

# ======================== ìƒìˆ˜ ì„¤ì • ========================

DATA_FOLDER = os.path.abspath("data")
NEWS_DB_FILE = os.path.join(DATA_FOLDER, "news_monitor.csv")
SENT_CACHE_FILE = os.path.join(DATA_FOLDER, "sent_articles_cache.json")
API_USAGE_FILE = os.path.join(DATA_FOLDER, "api_usage.json")
STATE_FILE = os.path.join(DATA_FOLDER, "monitor_state.json")
MAX_SENT_CACHE = 10000  # ìºì‹œ í¬ê¸° ì œí•œ (ì•½ 4-5ì¼ë¶„ ì»¤ë²„, ê¸°ì¡´ 500ê°œì—ì„œ í™•ëŒ€)
MAX_API_CALLS_PER_DAY = 25000  # ë„¤ì´ë²„ API ì¼ì¼ í• ë‹¹ëŸ‰
API_QUOTA_WARNING_THRESHOLD = 20000  # 80% ë„ë‹¬ ì‹œ ê²½ê³  (25000ì˜ 80%)

# ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ ì„¤ì • (ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›)
KEYWORDS = [
    "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
    "POSCO INTERNATIONAL",
    "í¬ìŠ¤ì½”ì¸í„°",
    "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
    "êµ¬ë™ëª¨í„°ì½”ì•„",
    "êµ¬ë™ëª¨í„°ì½”ì–´",
    "ë¯¸ì–€ë§ˆ LNG",
    "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜",
    "í¬ìŠ¤ì½”í”Œë¡œìš°",  # ì¶”ê°€
    "í¬ìŠ¤ì½”"
]

# "í¬ìŠ¤ì½”" í‚¤ì›Œë“œ ì œì™¸ í•„í„°
EXCLUDE_KEYWORDS = [
    "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
    "POSCO INTERNATIONAL",
    "í¬ìŠ¤ì½”ì¸í„°",
    "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
    "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"
]

# ìˆ˜ì§‘ ì„¤ì •
MAX_ITEMS_PER_RUN = 300  # í‚¤ì›Œë“œë‹¹ ì•½ 30ê°œ ìˆ˜ì§‘ (í•„í„°ë§ í›„ ì¶©ë¶„í•œ ê¸°ì‚¬ í™•ë³´)

# ì „ì†¡ëœ ê¸°ì‚¬ URL ì¶”ì  (ë©”ëª¨ë¦¬ ìºì‹œ)
_sent_articles_cache = set()


# ======================== í—¬í¼ í•¨ìˆ˜ ========================

def _naver_headers():
    """Naver API ì¸ì¦ í—¤ë”"""
    cid = os.getenv("NAVER_CLIENT_ID", "")
    csec = os.getenv("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        print(f"[WARNING] ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    return {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}


def _clean_text(s: str) -> str:
    """HTML íƒœê·¸ ë° ê³µë°± ì •ë¦¬"""
    if not s:
        return ""
    s = unescape(s)
    s = re.sub(r"</?b>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _normalize_url(url: str) -> str:
    """
    URL ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ URLì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë³´ì¡´ (ì¤‘ìš”! ë§ì€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ê°€ ì¿¼ë¦¬ë¡œ ê¸°ì‚¬ êµ¬ë¶„)
    - í”„ë¡œí† ì½œ í†µì¼ (http â†’ https)
    - ë ìŠ¬ë˜ì‹œ ì œê±°
    """
    try:
        if not url:
            return ""
        parsed = urllib.parse.urlparse(url)

        # 1. í”„ë¡œí† ì½œ í†µì¼ (http â†’ https)
        scheme = "https" if parsed.scheme in ["http", "https"] else parsed.scheme

        # 2. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë³´ì¡´ (ê¸°ì‚¬ ID êµ¬ë¶„ì„ ìœ„í•´ í•„ìˆ˜)
        query = f"?{parsed.query}" if parsed.query else ""

        # 3. ë ìŠ¬ë˜ì‹œ ì œê±°
        path = parsed.path.rstrip("/")

        # 4. ì •ê·œí™”ëœ URL ìƒì„±
        normalized = f"{scheme}://{parsed.netloc}{path}{query}"
        return normalized
    except Exception as e:
        print(f"[WARNING] URL ì •ê·œí™” ì‹¤íŒ¨: {url} - {e}")
        return url


def _publisher_from_link(u: str) -> str:
    """ë‰´ìŠ¤ ì›ë¬¸ URLì—ì„œ ë§¤ì²´ëª…ì„ í†µì¼í•´ì„œ ë°˜í™˜"""
    try:
        host = urllib.parse.urlparse(u).netloc.lower().replace("www.", "")
        if not host:
            return ""

        # ì„œë¸Œë„ë©”ì¸ ì •í™• ë§¤í•‘
        host_map = {
            "en.yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
            "news.kbs.co.kr": "KBS",
            "news.mtn.co.kr": "MTN",
            "starin.edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "sports.donga.com": "ë™ì•„ì¼ë³´",
            "biz.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
            "daily.hankooki.com": "ë°ì¼ë¦¬í•œêµ­",
            "news.dealsitetv.com": "ë”œì‚¬ì´íŠ¸TV",
        }
        if host in host_map:
            return host_map[host]

        # ê¸°ë³¸ ë„ë©”ì¸(eTLD+1) ì¶”ì¶œ
        parts = host.split(".")
        if len(parts) >= 3 and parts[-1] == "kr" and parts[-2] in {
            "co","or","go","ne","re","pe","ac","hs","kg","sc",
            "seoul","busan","incheon","daegu","daejeon","gwangju","ulsan",
            "gyeonggi","gangwon","chungbuk","chungnam","jeonbuk","jeonnam",
            "gyeongbuk","gyeongnam","jeju"
        }:
            base = ".".join(parts[-3:])
        else:
            base = ".".join(parts[-2:])

        # ê¸°ë³¸ ë„ë©”ì¸ â†’ ë§¤ì²´ëª… ë§¤í•‘
        base_map = {
            "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "kbs.co.kr": "KBS", "joins.com": "ì¤‘ì•™ì¼ë³´",
            "donga.com": "ë™ì•„ì¼ë³´", "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ", "edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "ajunews.com": "ì•„ì£¼ê²½ì œ", "newspim.com": "ë‰´ìŠ¤í•Œ", "news1.kr": "ë‰´ìŠ¤1",
            "etoday.co.kr": "ì´íˆ¬ë°ì´", "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
            "munhwa.com": "ë¬¸í™”ì¼ë³´", "segye.com": "ì„¸ê³„ì¼ë³´", "hankooki.com": "í•œêµ­ì¼ë³´",
            "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤", "ekn.kr": "ì—ë„ˆì§€ê²½ì œ", "businesskorea.co.kr": "ë¹„ì¦ˆë‹ˆìŠ¤ì½”ë¦¬ì•„",
            "ferrotimes.com": "ì² ê°•ê¸ˆì†ì‹ ë¬¸", "thepublic.kr": "ë”í¼ë¸”ë¦­", "tf.co.kr": "ë”íŒ©íŠ¸",
            "straightnews.co.kr": "ìŠ¤íŠ¸ë ˆì´íŠ¸ë‰´ìŠ¤", "smartfn.co.kr": "ìŠ¤ë§ˆíŠ¸ê²½ì œ", "sisacast.kr": "ì‹œì‚¬ìºìŠ¤íŠ¸",
            "sateconomy.co.kr": "ì‹œì‚¬ê²½ì œ", "safetynews.co.kr": "ì•ˆì „ì‹ ë¬¸", "rpm9.com": "RPM9",
            "pointdaily.co.kr": "í¬ì¸íŠ¸ë°ì¼ë¦¬", "newsworker.co.kr": "ë‰´ìŠ¤ì›Œì»¤", "newsdream.kr": "ë‰´ìŠ¤ë“œë¦¼",
            "nbntv.co.kr": "NBNë‰´ìŠ¤", "megaeconomy.co.kr": "ë©”ê°€ê²½ì œ", "mediapen.com": "ë¯¸ë””ì–´íœ",
            "job-post.co.kr": "ì¡í¬ìŠ¤íŠ¸", "irobotnews.com": "ë¡œë´‡ì‹ ë¬¸ì‚¬", "ifm.kr": "ê²½ì¸ë°©ì†¡",
            "gpkorea.com": "ê¸€ë¡œë²Œì˜¤í† ë‰´ìŠ¤", "energydaily.co.kr": "ì—ë„ˆì§€ë°ì¼ë¦¬",
            "cstimes.com": "ì»¨ìŠˆë¨¸íƒ€ì„ìŠ¤", "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜", "autodaily.co.kr": "ì˜¤í† ë°ì¼ë¦¬",
            # ì¶”ê°€ ì–¸ë¡ ì‚¬ (2025-11-20)
            "newslock.co.kr": "ë‰´ìŠ¤ë½", "mbn.co.kr": "MBN", "kpenews.com": "KPE",
            "koreatimes.co.kr": "ì½”ë¦¬ì•„íƒ€ì„ìŠ¤", "korea.kr": "ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘",
            "investchosun.com": "ì¸ë² ìŠ¤íŠ¸ì¡°ì„ ", "goodnews1.com": "GOODTV", "aitimes.kr": "AIíƒ€ì„ìŠ¤",
            # ì¶”ê°€ ì–¸ë¡ ì‚¬ (2025-12-02)
            "worklaw.co.kr": "ì›Œí¬ë¡œ", "vop.co.kr": "ë¯¼ì¤‘ì˜ì†Œë¦¬", "thefairnews.co.kr": "ë”í˜ì–´ë‰´ìŠ¤",
            "newsway.co.kr": "ë‰´ìŠ¤ì›¨ì´", "newsfreezone.co.kr": "ë‰´ìŠ¤í”„ë¦¬ì¡´", "mtn.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´ë°©ì†¡",
            "kyongbuk.co.kr": "ê²½ë¶ì¼ë³´", "klnews.co.kr": "ë¬¼ë¥˜ì‹ ë¬¸", "geconomy.co.kr": "Gê²½ì œ",
            "enetnews.co.kr": "ì´ë„·ë‰´ìŠ¤", "dkilbo.com": "ëŒ€ê²½ì¼ë³´", "dailysportshankook.co.kr": "ë°ì¼ë¦¬ìŠ¤í¬ì¸ í•œêµ­",
            "dailysecu.com": "ë°ì¼ë¦¬ì‹œí", "ceoscoredaily.com": "CEOìŠ¤ì½”ì–´ë°ì¼ë¦¬", "apparelnews.co.kr": "ì–´íŒ¨ëŸ´ë‰´ìŠ¤",
        }
        if base in base_map:
            return base_map[base]

        return ""
    except Exception:
        return ""


# ======================== ë„¤ì´ë²„ API í•¨ìˆ˜ ========================

def fetch_naver_news(query: str, start: int = 1, display: int = 50, sort: str = "date"):
    """Naver ë‰´ìŠ¤ API í˜¸ì¶œ (ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€)"""
    r = None
    try:
        url = "https://openapi.naver.com/v1/search/news.json"
        params = {"query": query, "start": start, "display": display, "sort": sort}
        headers = _naver_headers()

        if not headers.get("X-Naver-Client-Id") or not headers.get("X-Naver-Client-Secret"):
            return {"items": [], "error": "missing_keys"}

        r = requests.get(url, headers=headers, params=params, timeout=10)

        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²˜ë¦¬
        if r.status_code == 429:
            error_data = r.json() if r.text else {}
            error_msg = error_data.get("errorMessage", "API quota exceeded")
            print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ (429): {error_msg}")
            return {"items": [], "error": "quota_exceeded", "error_message": error_msg}

        r.raise_for_status()
        return r.json()

    except requests.exceptions.Timeout:
        print(f"[WARNING] Naver API timeout for query: {query}")
        return {"items": [], "error": "timeout"}
    except requests.exceptions.RequestException as e:
        print(f"[WARNING] Naver API request failed for query: {query}, error: {e}")
        if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
            return {"items": [], "error": "quota_exceeded"}
        return {"items": [], "error": "request_failed"}
    except Exception as e:
        print(f"[WARNING] Unexpected error in fetch_naver_news: {e}")
        return {"items": [], "error": "unexpected"}
    finally:
        # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€: ì‘ë‹µ ê°ì²´ ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°
        if r is not None:
            r.close()


def crawl_naver_news(query: str, max_items: int = 200, sort: str = "date") -> pd.DataFrame:
    """Naver ë‰´ìŠ¤ ìˆ˜ì§‘"""
    items, start, total = [], 1, 0
    display = min(50, max_items)
    max_attempts = 2
    attempt_count = 0
    quota_exceeded = False

    while total < max_items and start <= 100 and attempt_count < max_attempts:
        attempt_count += 1

        try:
            data = fetch_naver_news(query, start=start, display=min(display, max_items - total), sort=sort)

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
            if data.get("error") == "quota_exceeded":
                print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            arr = data.get("items", [])
            if not arr:
                break

            for it in arr:
                title = _clean_text(it.get("title"))
                desc = _clean_text(it.get("description"))
                link = it.get("originallink") or it.get("link") or ""
                pub = it.get("pubDate", "")
                try:
                    # GMT â†’ KST ë³€í™˜ í›„ tz ì œê±°
                    dt = pd.to_datetime(pub, utc=True).tz_convert("Asia/Seoul").tz_localize(None)
                    date_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    date_str = ""
                items.append({
                    "ë‚ ì§œ": date_str,
                    "ë§¤ì²´ëª…": _publisher_from_link(link),
                    "ê²€ìƒ‰í‚¤ì›Œë“œ": query,
                    "ê¸°ì‚¬ì œëª©": title,
                    "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½": desc,
                    "URL": link
                })

            got = len(arr)
            total += got
            if got == 0:
                break
            start += got

        except Exception as e:
            print(f"[WARNING] Error in crawl_naver_news attempt {attempt_count}: {e}")
            break

    df = pd.DataFrame(items, columns=["ë‚ ì§œ", "ë§¤ì²´ëª…", "ê²€ìƒ‰í‚¤ì›Œë“œ", "ê¸°ì‚¬ì œëª©", "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", "URL"])

    # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì •ë³´ ì €ì¥
    if quota_exceeded:
        df.attrs['quota_exceeded'] = True

    if not df.empty:
        # ìµœì‹ ìˆœ ì •ë ¬
        df["ë‚ ì§œ_datetime"] = pd.to_datetime(df["ë‚ ì§œ"], errors="coerce")
        df = df.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
        df = df.drop("ë‚ ì§œ_datetime", axis=1)

        # ì¤‘ë³µ ì œê±°
        key = df["URL"].where(df["URL"].astype(bool), df["ê¸°ì‚¬ì œëª©"] + "|" + df["ë‚ ì§œ"])
        df = df.loc[~key.duplicated()].reset_index(drop=True)
    return df


# ======================== DB í•¨ìˆ˜ ========================

def load_news_db() -> pd.DataFrame:
    """ë‰´ìŠ¤ DB ë¡œë“œ"""
    if os.path.exists(NEWS_DB_FILE):
        try:
            return pd.read_csv(NEWS_DB_FILE, encoding="utf-8")
        except Exception as e:
            print(f"[WARNING] DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    return pd.DataFrame(columns=["ë‚ ì§œ","ë§¤ì²´ëª…","ê²€ìƒ‰í‚¤ì›Œë“œ","ê¸°ì‚¬ì œëª©","ì£¼ìš”ê¸°ì‚¬ ìš”ì•½","URL"])


def save_news_db(df: pd.DataFrame):
    """ë‰´ìŠ¤ DB ì €ì¥"""
    if df.empty:
        print("[DEBUG] save_news_db skipped: empty dataframe")
        return

    # ë§¤ì²´ëª… ì •ë¦¬ (URL ê¸°ë°˜)
    if "ë§¤ì²´ëª…" in df.columns and "URL" in df.columns:
        for idx, row in df.iterrows():
            if pd.notna(row["URL"]):
                df.at[idx, "ë§¤ì²´ëª…"] = _publisher_from_link(row["URL"])

    # ìƒìœ„ 200ê°œë§Œ ì €ì¥
    out = df.head(200).copy()

    # data í´ë” ìƒì„±
    os.makedirs(DATA_FOLDER, exist_ok=True)

    out.to_csv(NEWS_DB_FILE, index=False, encoding="utf-8")
    print(f"[DEBUG] news saved: {len(out)} rows -> {NEWS_DB_FILE}")


# ======================== ìºì‹œ í•¨ìˆ˜ ========================

def load_sent_cache() -> set:
    """ì „ì†¡ëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ (TTL ì ìš©)"""
    if os.path.exists(SENT_CACHE_FILE):
        try:
            with open(SENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)

                # TTL ì§€ì› ì—¬ë¶€ í™•ì¸
                if "url_timestamps" in data:
                    # TTL ê¸°ë°˜ ìºì‹œ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
                    from datetime import timedelta
                    url_timestamps = data.get("url_timestamps", {})
                    ttl_days = data.get("ttl_days", 7)
                    now = datetime.now()
                    cutoff_time = now - timedelta(days=ttl_days)

                    # ìœ íš¨í•œ URLë§Œ ë¡œë“œ
                    valid_urls = set()
                    expired_count = 0
                    for url, timestamp_str in url_timestamps.items():
                        try:
                            timestamp = datetime.fromisoformat(timestamp_str)
                            if timestamp > cutoff_time:
                                valid_urls.add(url)
                            else:
                                expired_count += 1
                        except Exception:
                            # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨ (ì•ˆì „ ì¥ì¹˜)
                            valid_urls.add(url)

                    if expired_count > 0:
                        print(f"[DEBUG] TTL ë§Œë£Œ URL ì œê±°: {expired_count}ê±´")

                    print(f"[DEBUG] ì „ì†¡ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(valid_urls)}ê±´ (TTL: {ttl_days}ì¼)")
                    return valid_urls
                else:
                    # ë ˆê±°ì‹œ ìºì‹œ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ)
                    cache = set(data.get("urls", []))
                    print(f"[DEBUG] ì „ì†¡ ìºì‹œ ë¡œë“œ ì™„ë£Œ (ë ˆê±°ì‹œ): {len(cache)}ê±´")
                    return cache

        except Exception as e:
            print(f"[WARNING] ì „ì†¡ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()
    else:
        print(f"[DEBUG] ì „ì†¡ ìºì‹œ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
        return set()


def save_sent_cache(cache: set, ttl_days: int = 7):
    """
    ì „ì†¡ëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥ (TTL ê¸°ë°˜, ì›ìì  ì“°ê¸°)

    Args:
        cache: ì €ì¥í•  URL ìºì‹œ
        ttl_days: TTL (Time To Live) ì¼ìˆ˜ (ê¸°ë³¸: 7ì¼)
    """
    try:
        from datetime import timedelta
        import tempfile
        os.makedirs(DATA_FOLDER, exist_ok=True)

        # ê¸°ì¡´ ìºì‹œ ë¡œë“œ (íƒ€ì„ìŠ¤íƒ¬í”„ ìœ ì§€)
        existing_timestamps = {}
        if os.path.exists(SENT_CACHE_FILE):
            try:
                with open(SENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_timestamps = existing_data.get("url_timestamps", {})
            except Exception:
                pass

        # í˜„ì¬ ì‹œê°„
        now = datetime.now()
        cutoff_time = now - timedelta(days=ttl_days)

        # URL íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸
        url_timestamps = {}
        for url in cache:
            if url in existing_timestamps:
                # ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ìœ ì§€
                try:
                    timestamp = datetime.fromisoformat(existing_timestamps[url])
                    if timestamp > cutoff_time:
                        url_timestamps[url] = existing_timestamps[url]
                    else:
                        # ë§Œë£Œëœ ê²½ìš° í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ê°±ì‹ 
                        url_timestamps[url] = now.isoformat()
                except Exception:
                    url_timestamps[url] = now.isoformat()
            else:
                # ì‹ ê·œ URLì€ í˜„ì¬ ì‹œê°„
                url_timestamps[url] = now.isoformat()

        # ìµœì‹  MAX_SENT_CACHEê°œë§Œ ìœ ì§€ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
        if len(url_timestamps) > MAX_SENT_CACHE:
            sorted_urls = sorted(url_timestamps.items(), key=lambda x: x[1], reverse=True)
            url_timestamps = dict(sorted_urls[:MAX_SENT_CACHE])

        data = {
            "url_timestamps": url_timestamps,
            "urls": list(url_timestamps.keys()),  # í•˜ìœ„ í˜¸í™˜ì„±
            "last_updated": now.strftime("%Y-%m-%d %H:%M:%S"),
            "count": len(url_timestamps),
            "ttl_days": ttl_days
        }

        # ì›ìì  ì“°ê¸° (ì„ì‹œ íŒŒì¼ + rename)
        # ë™ì‹œ ì“°ê¸° ì‹œì—ë„ íŒŒì¼ ì†ìƒ ë°©ì§€
        temp_fd, temp_path = tempfile.mkstemp(dir=DATA_FOLDER, suffix='.tmp')
        try:
            with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # Windowsì—ì„œëŠ” ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•„ìš”
            if os.path.exists(SENT_CACHE_FILE):
                try:
                    os.remove(SENT_CACHE_FILE)
                except Exception:
                    pass

            # ì„ì‹œ íŒŒì¼ì„ ìµœì¢… íŒŒì¼ë¡œ ì´ë™ (ì›ìì  ì—°ì‚°)
            os.replace(temp_path, SENT_CACHE_FILE)

            print(f"[DEBUG] ì „ì†¡ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(url_timestamps)}ê±´ (TTL: {ttl_days}ì¼) -> {SENT_CACHE_FILE}")
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.remove(temp_path)
            except Exception:
                pass
            raise e
    except Exception as e:
        print(f"[WARNING] ì „ì†¡ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# ======================== API í• ë‹¹ëŸ‰ ê´€ë¦¬ ========================

def load_api_usage() -> int:
    """ì˜¤ëŠ˜ API ì‚¬ìš©ëŸ‰ ë¡œë“œ"""
    if os.path.exists(API_USAGE_FILE):
        try:
            with open(API_USAGE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                today = datetime.now().strftime("%Y-%m-%d")
                if data.get("date") == today:
                    return data.get("count", 0)
        except Exception as e:
            print(f"[WARNING] API ì‚¬ìš©ëŸ‰ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return 0


def save_api_usage(count: int):
    """ì˜¤ëŠ˜ API ì‚¬ìš©ëŸ‰ ì €ì¥"""
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)
        today = datetime.now().strftime("%Y-%m-%d")
        data = {
            "date": today,
            "count": count,
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "quota_remaining": MAX_API_CALLS_PER_DAY - count,
            "quota_percentage": (count / MAX_API_CALLS_PER_DAY) * 100
        }
        with open(API_USAGE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"[DEBUG] API ì‚¬ìš©ëŸ‰ ì €ì¥: {count}/{MAX_API_CALLS_PER_DAY} ({data['quota_percentage']:.1f}%)")
    except Exception as e:
        print(f"[WARNING] API ì‚¬ìš©ëŸ‰ ì €ì¥ ì‹¤íŒ¨: {e}")


def increment_api_usage(calls: int = 1) -> int:
    """API ì‚¬ìš©ëŸ‰ ì¦ê°€ ë° í˜„ì¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
    current = load_api_usage()
    new_count = current + calls
    save_api_usage(new_count)
    return new_count


def check_api_quota(required_calls: int = 1) -> bool:
    """
    API í• ë‹¹ëŸ‰ í™•ì¸

    Args:
        required_calls: í•„ìš”í•œ API í˜¸ì¶œ íšŸìˆ˜

    Returns:
        True: í• ë‹¹ëŸ‰ ì—¬ìœ  ìˆìŒ, False: í• ë‹¹ëŸ‰ ë¶€ì¡±
    """
    current = load_api_usage()
    remaining = MAX_API_CALLS_PER_DAY - current

    if remaining < required_calls:
        print(f"[WARNING] âš ï¸ API í• ë‹¹ëŸ‰ ë¶€ì¡±: ë‚¨ì€ í˜¸ì¶œ {remaining}íšŒ, í•„ìš” {required_calls}íšŒ")
        return False

    if current >= API_QUOTA_WARNING_THRESHOLD:
        print(f"[WARNING] âš ï¸ API í• ë‹¹ëŸ‰ 80% ë„ë‹¬: {current}/{MAX_API_CALLS_PER_DAY} ({(current/MAX_API_CALLS_PER_DAY)*100:.1f}%)")

    return True


# ======================== ì´ˆê¸°í™” ìƒíƒœ ê´€ë¦¬ ========================

def is_first_run() -> bool:
    """ì²« ì‹¤í–‰ ì—¬ë¶€ í™•ì¸ (ìƒíƒœ íŒŒì¼ ê¸°ì¤€)"""
    if not os.path.exists(STATE_FILE):
        return True

    try:
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            state = json.load(f)
            return not state.get("initialized", False)
    except Exception as e:
        print(f"[WARNING] ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return True


def mark_initialized():
    """ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ"""
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)
        state = {
            "initialized": True,
            "first_run_date": datetime.now().isoformat(),
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        print(f"[DEBUG] ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ")
    except Exception as e:
        print(f"[WARNING] ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


# ======================== ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ ========================

def detect_new_articles(old_df: pd.DataFrame, new_df: pd.DataFrame, sent_cache: set) -> list:
    """
    ê¸°ì¡´ DBì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
    - URLì„ ìš°ì„  ì‹ë³„ìë¡œ ì‚¬ìš©
    - ìºì‹œì™€ DB ì¤‘ë³µ ì²´í¬
    """
    try:
        # ì²« ì‹¤í–‰ ì²´í¬ (ìƒíƒœ íŒŒì¼ ê¸°ì¤€)
        if is_first_run():
            print(f"[DEBUG] ì²« ì‹¤í–‰ ê°ì§€ - ì•Œë¦¼ ìŠ¤í‚µí•˜ê³  ì´ˆê¸°í™”")
            mark_initialized()
            return []

        # DBê°€ ë¹„ì–´ìˆì§€ë§Œ ì²« ì‹¤í–‰ì´ ì•„ë‹ˆë©´ ê²½ê³  (ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±)
        if old_df.empty and not is_first_run():
            print(f"[WARNING] âš ï¸ DBê°€ ë¹„ì–´ìˆì§€ë§Œ ì²« ì‹¤í–‰ì´ ì•„ë‹˜ - ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±")
            # ì´ ê²½ìš°ì—ë„ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì²˜ë¦¬ (ë³µêµ¬ ëª©ì )

        if new_df.empty:
            return []

        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (KST)
        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST).replace(tzinfo=None)  # KST ì‹œê°„ì„ naive datetimeìœ¼ë¡œ

        # ê¸°ì¡´ DBì˜ URL ì„¸íŠ¸ ìƒì„± (ì •ê·œí™”ëœ URL ì‚¬ìš©)
        old_urls = set()
        old_urls_normalized = set()
        for _, row in old_df.iterrows():
            url = str(row.get("URL", "")).strip()
            if url and url != "nan" and url != "":
                old_urls.add(url)
                old_urls_normalized.add(_normalize_url(url))

        print(f"[DEBUG] ê¸°ì¡´ DB URL ìˆ˜: {len(old_urls)} (ì •ê·œí™”: {len(old_urls_normalized)})")
        print(f"[DEBUG] ìºì‹œ í¬ê¸°: {len(sent_cache)}ê±´")
        print(f"[DEBUG] ìˆ˜ì§‘ëœ ì‹ ê·œ ë°ì´í„° ìˆ˜: {len(new_df)}")

        # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (ì‹œê°„ í•„í„°ë§ ì¶”ê°€)
        new_articles = []
        MAX_ARTICLE_AGE_HOURS = 48  # ìµœê·¼ 48ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ ì•Œë¦¼

        for _, row in new_df.iterrows():
            url = str(row.get("URL", "")).strip()
            title = str(row.get("ê¸°ì‚¬ì œëª©", "")).strip()

            # URLì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
            if not url or url == "nan" or url == "":
                continue

            # URL ì •ê·œí™”
            url_normalized = _normalize_url(url)

            # 3ë‹¨ê³„ ì¤‘ë³µ ì²´í¬: DB + ìºì‹œ + ì •ê·œí™”
            is_in_db = url in old_urls or url_normalized in old_urls_normalized
            is_in_cache = url in sent_cache or url_normalized in sent_cache

            if is_in_db or is_in_cache:
                continue

            # ì‹ ê·œ ê¸°ì‚¬ - ë‚ ì§œ í•„í„°ë§ (ê°œì„ ë¨)
            article_date_str = row.get("ë‚ ì§œ", "")
            should_notify = True  # ê¸°ë³¸ê°’: ì‹ ê·œ ê¸°ì‚¬ë©´ ì•Œë¦¼
            hours_diff = None

            try:
                article_date = pd.to_datetime(article_date_str, errors="coerce")
                if pd.notna(article_date):
                    time_diff = now - article_date
                    hours_diff = time_diff.total_seconds() / 3600

                    # ì‹œê°„ ê¸°ë°˜ í•„í„°ë§: ìµœê·¼ 48ì‹œê°„ ì´ë‚´ë§Œ ì•Œë¦¼
                    if hours_diff <= MAX_ARTICLE_AGE_HOURS:
                        print(f"[DEBUG] âœ… ì‹ ê·œ ê¸°ì‚¬ ê°ì§€: {title[:50]}... ({hours_diff:.1f}ì‹œê°„ ì „)")
                    else:
                        print(f"[DEBUG] â­ï¸ ì˜¤ë˜ëœ ê¸°ì‚¬ ìŠ¤í‚µ: {title[:50]}... ({hours_diff:.1f}ì‹œê°„ ì „, {hours_diff/24:.1f}ì¼ ì „)")
                        continue  # ë„ˆë¬´ ì˜¤ë˜ëœ ê¸°ì‚¬ëŠ” ì•Œë¦¼ ìŠ¤í‚µ
                else:
                    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì²˜ë¦¬ (ê°œì„ !)
                    print(f"[DEBUG] âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, í•˜ì§€ë§Œ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì•Œë¦¼: {title[:50]}... (ë‚ ì§œ: {article_date_str})")
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì²˜ë¦¬ (ê°œì„ !)
                print(f"[DEBUG] âš ï¸ ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜, í•˜ì§€ë§Œ ì‹ ê·œ ê¸°ì‚¬ë¡œ ì•Œë¦¼: {title[:50]}... - {str(e)}")

            # ë§¤ì²´ëª…ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            press = _publisher_from_link(url)
            keyword = str(row.get("ê²€ìƒ‰í‚¤ì›Œë“œ", "")).strip()

            new_articles.append({
                "title": title if title and title != "nan" else "ì œëª© ì—†ìŒ",
                "link": url,
                "date": article_date_str,
                "press": press,
                "keyword": keyword
            })

        print(f"[DEBUG] ì´ {len(new_articles)}ê±´ì˜ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (DB+ìºì‹œ ì¤‘ë³µ ì œê±°)")
        return new_articles

    except Exception as e:
        print(f"[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return []


# ======================== í…”ë ˆê·¸ë¨ ì•Œë¦¼ ========================

def send_telegram_notification(new_articles: list, sent_cache: set) -> set:
    """
    ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡ (ê¸°ì‚¬ë³„ ê°œë³„ ë©”ì‹œì§€)

    - ëª¨ë“  ì‹ ê·œ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ (ê°œìˆ˜ ì œí•œ ì—†ìŒ)
    - ê° ê¸°ì‚¬ë§ˆë‹¤ 3íšŒê¹Œì§€ ì¬ì‹œë„
    - í…”ë ˆê·¸ë¨ API Rate Limit ì¤€ìˆ˜ (ì´ˆë‹¹ ì•½ 28ê°œ)
    - ì „ì†¡ ì„±ê³µí•œ ê¸°ì‚¬ë§Œ ìºì‹œì— ì¶”ê°€í•˜ì—¬ ì¬ì „ì†¡ ë°©ì§€

    Returns: ì—…ë°ì´íŠ¸ëœ sent_cache (ì „ì†¡ ì„±ê³µí•œ ê¸°ì‚¬ URL í¬í•¨)
    """
    try:
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        print(f"[DEBUG] í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œë„ - ê¸°ì‚¬ ìˆ˜: {len(new_articles) if new_articles else 0}")

        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì•Œë¦¼ ìŠ¤í‚µ
        if not bot_token or not chat_id:
            print("[DEBUG] âš ï¸ í…”ë ˆê·¸ë¨ ì„¤ì • ì—†ìŒ - ì•Œë¦¼ ìŠ¤í‚µ")
            return sent_cache

        if not new_articles:
            print("[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ - ì•Œë¦¼ ìŠ¤í‚µ")
            return sent_cache

        # ì´ë¯¸ ì „ì†¡ëœ ê¸°ì‚¬ í•„í„°ë§
        articles_to_send = []
        skipped_already_sent = 0
        for article in new_articles:
            url_key = article.get("link", "")
            url_normalized = _normalize_url(url_key)

            if url_key and url_key not in sent_cache and url_normalized not in sent_cache:
                articles_to_send.append(article)
            else:
                skipped_already_sent += 1
                print(f"[DEBUG] ğŸ“¤ ì´ë¯¸ ì „ì†¡ëœ ê¸°ì‚¬ ìŠ¤í‚µ: {article.get('title', '')[:50]}...")

        if not articles_to_send:
            print(f"[DEBUG] ëª¨ë“  ê¸°ì‚¬ê°€ ì´ë¯¸ ì „ì†¡ë¨ - ì•Œë¦¼ ìŠ¤í‚µ (ìŠ¤í‚µëœ ê¸°ì‚¬: {skipped_already_sent}ê±´)")
            return sent_cache

        print(f"[DEBUG] ì „ì†¡ ëŒ€ìƒ: {len(articles_to_send)}ê±´ (ì´ë¯¸ ì „ì†¡: {skipped_already_sent}ê±´)")

        # ëª¨ë“  ì‹ ê·œ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ (ì œí•œ ì—†ìŒ)
        articles_to_notify = articles_to_send
        print(f"[DEBUG] ğŸ“¤ ì´ {len(articles_to_notify)}ê±´ì˜ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.")

        # í…”ë ˆê·¸ë¨ API URL
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

        # ê° ê¸°ì‚¬ë§ˆë‹¤ ê°œë³„ ë©”ì‹œì§€ ì „ì†¡
        success_count = 0
        for article in articles_to_notify:
            title = article.get("title", "ì œëª© ì—†ìŒ")
            link = article.get("link", "")
            date = article.get("date", "")
            press = article.get("press", "")
            keyword = article.get("keyword", "")

            # ë©”ì‹œì§€ êµ¬ì„±
            message = f"ğŸš¨ *ìƒˆ ë‰´ìŠ¤*\n\n"
            if keyword:
                hashtag = keyword.replace(" ", "")
                message += f"#{hashtag}\n"
            if press:
                message += f"*[{press}]* {title}\n"
            else:
                message += f"*{title}*\n"
            if date:
                message += f"ğŸ• {date}\n"
            if link:
                message += f"ğŸ”— {link}"

            payload = {
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": True
            }

            response = None
            # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ)
            max_retries = 3
            retry_delay = 1  # ì´ˆ

            for attempt in range(max_retries):
                try:
                    response = requests.post(url, json=payload, timeout=10)
                    if response.status_code == 200:
                        success_count += 1
                        print(f"[DEBUG] âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {title[:30]}...")

                        # ì „ì†¡ ì„±ê³µí•œ ê¸°ì‚¬ëŠ” ìºì‹œì— ì¶”ê°€
                        sent_cache.add(link)
                        sent_cache.add(_normalize_url(link))
                        break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                    else:
                        print(f"[DEBUG] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {response.status_code}")
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„

                except Exception as e:
                    print(f"[DEBUG] âŒ ê°œë³„ ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    else:
                        # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•˜ë©´ ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥
                        import traceback
                        print(f"[DEBUG] ìµœì¢… ì‹¤íŒ¨ - ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                finally:
                    # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€
                    if response is not None:
                        response.close()

            # Rate Limit ë°©ì§€ (í…”ë ˆê·¸ë¨ API: ì´ˆë‹¹ 30ê°œ ì œí•œ)
            # 35ms ëŒ€ê¸° = ì´ˆë‹¹ ì•½ 28ê°œë¡œ ì•ˆì „í•œ ì†ë„ ìœ ì§€
            import time
            time.sleep(0.035)

        # ì „ì†¡ ê²°ê³¼ í†µê³„
        failed_count = len(articles_to_notify) - success_count
        if failed_count > 0:
            print(f"[DEBUG] âš ï¸ ì „ì†¡ ì‹¤íŒ¨: {failed_count}ê±´")
            print(f"[DEBUG] ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” ë‹¤ìŒ ìˆ˜ì§‘ ì‚¬ì´í´ì— ì¬ì‹œë„ë©ë‹ˆë‹¤.")

        print(f"[DEBUG] âœ… ì´ {success_count}/{len(articles_to_notify)}ê±´ ì „ì†¡ ì™„ë£Œ (ì„±ê³µë¥ : {success_count/len(articles_to_notify)*100:.1f}%)")

        return sent_cache

    except Exception as e:
        print(f"[DEBUG] âŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return sent_cache


# ======================== ì‹œìŠ¤í…œ ìƒíƒœ ê´€ë¦¬ ========================

RUN_STATUS_FILE = os.path.join(DATA_FOLDER, "run_status.json")


def update_run_status(success: bool, articles_collected: int, new_articles: int,
                      telegram_sent: int, error_message: str = None):
    """
    ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì—°ì† ì‹¤íŒ¨ ì¶”ì 

    Args:
        success: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        articles_collected: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜
        new_articles: ì‹ ê·œ ê¸°ì‚¬ ìˆ˜
        telegram_sent: í…”ë ˆê·¸ë¨ ë°œì†¡ ìˆ˜
        error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
    """
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)

        # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ
        status_data = {
            "consecutive_failures": 0,
            "last_success_time": None,
            "total_runs": 0,
            "total_failures": 0
        }

        if os.path.exists(RUN_STATUS_FILE):
            try:
                with open(RUN_STATUS_FILE, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
            except Exception:
                pass

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        now = datetime.now()
        status_data["total_runs"] = status_data.get("total_runs", 0) + 1
        status_data["last_run_time"] = now.isoformat()

        if success:
            status_data["consecutive_failures"] = 0
            status_data["last_success_time"] = now.isoformat()
            status_data["last_success_stats"] = {
                "articles_collected": articles_collected,
                "new_articles": new_articles,
                "telegram_sent": telegram_sent
            }
        else:
            status_data["consecutive_failures"] = status_data.get("consecutive_failures", 0) + 1
            status_data["total_failures"] = status_data.get("total_failures", 0) + 1
            status_data["last_error"] = error_message

        # íŒŒì¼ ì €ì¥
        with open(RUN_STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)

        # ì—°ì† ì‹¤íŒ¨ ê²½ê³ 
        if status_data["consecutive_failures"] >= 3:
            send_system_alert(
                f"ğŸš¨ *ì—°ì† {status_data['consecutive_failures']}íšŒ ì‹¤íŒ¨*\n\n"
                f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œì´ ì—°ì†ìœ¼ë¡œ ì‹¤íŒ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
                f"ë§ˆì§€ë§‰ ì—ëŸ¬: {error_message or 'ì•Œ ìˆ˜ ì—†ìŒ'}"
            )

        print(f"[DEBUG] ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì—°ì† ì‹¤íŒ¨ {status_data['consecutive_failures']}íšŒ")

    except Exception as e:
        print(f"[WARNING] ì‹¤í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def check_api_quota_and_alert():
    """
    API í• ë‹¹ëŸ‰ í™•ì¸ ë° ê²½ê³  ì•Œë¦¼

    Returns:
        bool: API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        usage = load_api_usage()
        remaining = MAX_API_CALLS_PER_DAY - usage
        usage_percent = (usage / MAX_API_CALLS_PER_DAY) * 100

        print(f"[DEBUG] API ì‚¬ìš©ëŸ‰: {usage:,}/{MAX_API_CALLS_PER_DAY:,} ({usage_percent:.1f}%)")

        # 80% ë„ë‹¬ ì‹œ ê²½ê³ 
        if usage >= API_QUOTA_WARNING_THRESHOLD and usage < (API_QUOTA_WARNING_THRESHOLD + 100):
            send_system_alert(
                f"âš ï¸ *API í• ë‹¹ëŸ‰ ê²½ê³ *\n\n"
                f"Naver API ì‚¬ìš©ëŸ‰: {usage:,}/{MAX_API_CALLS_PER_DAY:,}\n"
                f"ì‚¬ìš©ë¥ : {usage_percent:.1f}%\n"
                f"ë‚¨ì€ í˜¸ì¶œ ìˆ˜: {remaining:,}"
            )

        # 95% ë„ë‹¬ ì‹œ ê¸´ê¸‰ ê²½ê³ 
        elif usage >= (MAX_API_CALLS_PER_DAY * 0.95):
            send_system_alert(
                f"ğŸš¨ *API í• ë‹¹ëŸ‰ ê¸´ê¸‰*\n\n"
                f"ì‚¬ìš©ëŸ‰ì´ 95%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!\n"
                f"ì‚¬ìš©: {usage:,}/{MAX_API_CALLS_PER_DAY:,}\n"
                f"ì¼ë¶€ í‚¤ì›Œë“œ ìˆ˜ì§‘ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        return usage < MAX_API_CALLS_PER_DAY

    except Exception as e:
        print(f"[WARNING] API í• ë‹¹ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True


def send_system_alert(message: str):
    """
    ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡ (ì¤‘ë³µ ë°©ì§€ í¬í•¨)

    Args:
        message: ì•Œë¦¼ ë©”ì‹œì§€
    """
    try:
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        if not bot_token or not chat_id:
            return

        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }

        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            print(f"[DEBUG] âœ… ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡ ì„±ê³µ")
        else:
            print(f"[DEBUG] âŒ ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")

        response.close()

    except Exception as e:
        print(f"[DEBUG] âŒ ì‹œìŠ¤í…œ ì•Œë¦¼ ì˜ˆì™¸: {e}")
