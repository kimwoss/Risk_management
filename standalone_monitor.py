"""
Standalone News Monitor - GitHub Actionsìš©
Streamlit ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  í…”ë ˆê·¸ë¨ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
3ë¶„ë§ˆë‹¤ GitHub Actionsì—ì„œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤.
"""
import pandas as pd
from datetime import datetime

# ê³µí†µ ëª¨ë“ˆ import
from news_collector import (
    KEYWORDS,
    EXCLUDE_KEYWORDS,
    MAX_ITEMS_PER_RUN,
    crawl_naver_news,
    load_news_db,
    save_news_db,
    load_sent_cache,
    save_sent_cache,
    detect_new_articles,
    send_telegram_notification,
    _naver_headers,
    _normalize_url,
)


def safe_print(text: str):
    """Windows ì½˜ì†” ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€"""
    try:
        print(text)
    except UnicodeEncodeError:
        # ì´ëª¨ì§€ ì œê±°í•˜ê³  ì¬ì‹œë„
        text_clean = text.encode('ascii', 'ignore').decode('ascii')
        print(text_clean)


def apply_keyword_filters(df: pd.DataFrame, keyword: str) -> pd.DataFrame:
    """í‚¤ì›Œë“œë³„ í•„í„°ë§ ë¡œì§ ì ìš©"""
    if df.empty:
        return df

    # "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" ì •í™•í•œ ë§¤ì¹­
    if keyword == "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            return "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" in title or "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" in description

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„' ì •í™• ë§¤ì¹­: {len(df)}ê±´")

    # "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" ì •í™•í•œ ë§¤ì¹­
    elif keyword == "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            return "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" in title or "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" in description

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜' ì •í™• ë§¤ì¹­: {len(df)}ê±´")

    # "í¬ìŠ¤ì½”í”Œë¡œìš°" ì •í™•í•œ ë§¤ì¹­
    elif keyword == "í¬ìŠ¤ì½”í”Œë¡œìš°":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            return "í¬ìŠ¤ì½”í”Œë¡œìš°" in title or "í¬ìŠ¤ì½”í”Œë¡œìš°" in description

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”í”Œë¡œìš°' ì •í™• ë§¤ì¹­: {len(df)}ê±´")

    # "í¬ìŠ¤ì½”" í‚¤ì›Œë“œ íŠ¹ë³„ ì²˜ë¦¬
    elif keyword == "í¬ìŠ¤ì½”":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            title_lower = title.lower()
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

            # "í¬ìŠ¤ì½”" ë˜ëŠ” "posco" í¬í•¨ ì²´í¬
            if "í¬ìŠ¤ì½”" not in title and "posco" not in title_lower:
                return False

            # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            for exclude_kw in EXCLUDE_KEYWORDS:
                if exclude_kw.lower() in title_lower:
                    return False

            # ë¶€ë™ì‚° í‚¤ì›Œë“œ ì œì™¸
            exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­", "ì¸í…Œë¦¬ì–´"]
            for exclude_word in exclude_words:
                if exclude_word in title or exclude_word in description:
                    return False

            return True

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”' í•„í„°ë§ ì™„ë£Œ: {len(df)}ê±´")

    # ê¸°íƒ€ í‚¤ì›Œë“œ - ë¶€ë™ì‚° í‚¤ì›Œë“œë§Œ ì œì™¸
    else:
        exclude_words = ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            for exclude_word in exclude_words:
                if exclude_word in title or exclude_word in description:
                    return False
            return True

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)

    return df


def main():
    """ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ë©”ì¸ í•¨ìˆ˜"""
    try:
        safe_print("=" * 80)
        safe_print(f"[MONITOR] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        safe_print("=" * 80)

        # ì „ì†¡ ìºì‹œ ë¡œë“œ
        sent_cache = load_sent_cache()

        # API í‚¤ ì²´í¬
        headers = _naver_headers()
        api_ok = bool(headers.get("X-Naver-Client-Id") and headers.get("X-Naver-Client-Secret"))

        if not api_ok:
            safe_print("[MONITOR] âŒ API í‚¤ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # ê¸°ì¡´ DB ë¡œë“œ
        existing_db = load_news_db()
        safe_print(f"[MONITOR] ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ: {len(existing_db)}ê±´")

        # ë‰´ìŠ¤ ìˆ˜ì§‘
        all_news = []
        quota_exceeded = False
        num_keywords = len(KEYWORDS)
        items_per_keyword = MAX_ITEMS_PER_RUN // num_keywords

        safe_print(f"[MONITOR] ìˆ˜ì§‘ ì„¤ì •: ì´ {MAX_ITEMS_PER_RUN}ê°œ / í‚¤ì›Œë“œë‹¹ ì•½ {items_per_keyword}ê°œ")

        for kw in KEYWORDS:
            safe_print(f"[MONITOR] í‚¤ì›Œë“œ '{kw}' ê²€ìƒ‰ ì¤‘...")
            df_kw = crawl_naver_news(kw, max_items=items_per_keyword, sort="date")

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
            if df_kw.attrs.get('quota_exceeded', False):
                safe_print(f"[MONITOR] âš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            # í‚¤ì›Œë“œë³„ í•„í„°ë§ ì ìš©
            df_kw = apply_keyword_filters(df_kw, kw)

            if not df_kw.empty:
                all_news.append(df_kw)
                safe_print(f"[MONITOR] '{kw}': {len(df_kw)}ê±´ ìˆ˜ì§‘")

        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
        if quota_exceeded:
            safe_print(f"[MONITOR] âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
            safe_print(f"[MONITOR] ğŸ’¡ ë§¤ì¼ ìì •(KST) ì´í›„ í• ë‹¹ëŸ‰ ì¬ì„¤ì •")
            return

        # í†µí•© ì •ë¦¬ & ì €ì¥
        df_new = pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()
        if not df_new.empty:
            safe_print(f"[MONITOR] ì´ ìˆ˜ì§‘: {len(df_new)}ê±´")

            # ë‚ ì§œìˆœ ì •ë ¬
            df_new["ë‚ ì§œ_datetime"] = pd.to_datetime(df_new["ë‚ ì§œ"], errors="coerce")
            df_new = df_new.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
            df_new = df_new.drop("ë‚ ì§œ_datetime", axis=1)

            # ì¤‘ë³µ ì œê±°
            key = df_new["URL"].where(df_new["URL"].astype(bool), df_new["ê¸°ì‚¬ì œëª©"] + "|" + df_new["ë‚ ì§œ"])
            df_new = df_new.loc[~key.duplicated()].reset_index(drop=True)

            # ê¸°ì¡´ DBì™€ ë³‘í•©
            merged = pd.concat([df_new, existing_db], ignore_index=True) if not existing_db.empty else df_new
            merged = merged.drop_duplicates(subset=["URL", "ê¸°ì‚¬ì œëª©"], keep="first").reset_index(drop=True)
            if not merged.empty:
                merged["ë‚ ì§œ"] = pd.to_datetime(merged["ë‚ ì§œ"], errors="coerce")
                merged = merged.sort_values("ë‚ ì§œ", ascending=False, na_position="last").reset_index(drop=True)
                merged["ë‚ ì§œ"] = merged["ë‚ ì§œ"].dt.strftime("%Y-%m-%d %H:%M")

            # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
            new_articles = detect_new_articles(existing_db, df_new, sent_cache)

            # DB ë¨¼ì € ì €ì¥
            save_news_db(merged)
            safe_print(f"[MONITOR] âœ… DB ì €ì¥ ì™„ë£Œ: ì´ {len(merged)}ê±´")

            # í…”ë ˆê·¸ë¨ ì•Œë¦¼ (ê¸°ì¡´ DBê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ)
            if new_articles and not existing_db.empty:
                safe_print(f"[MONITOR] âœ… ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡")
                sent_cache = send_telegram_notification(new_articles, sent_cache)
            elif new_articles:
                safe_print(f"[MONITOR] â­ï¸ ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - ì²« ì‹¤í–‰ì´ë¯€ë¡œ ì•Œë¦¼ ìŠ¤í‚µ")
                # ì²« ì‹¤í–‰ì—ì„œë„ ìºì‹œì— ì¶”ê°€
                for article in new_articles:
                    url = article.get("link", "")
                    if url:
                        sent_cache.add(url)
                        sent_cache.add(_normalize_url(url))
                safe_print(f"[MONITOR] ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ì„ ìºì‹œì— ì¶”ê°€")

            safe_print(f"[MONITOR] âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            safe_print(f"[MONITOR] â„¹ï¸ ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # í•­ìƒ ìºì‹œ ì €ì¥
        safe_print(f"[MONITOR] ìºì‹œ ì €ì¥ ì¤‘... (í˜„ì¬ {len(sent_cache)}ê±´)")
        save_sent_cache(sent_cache)

        safe_print("=" * 80)
        safe_print(f"[MONITOR] ì‘ì—… ì¢…ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        safe_print("=" * 80)

    except Exception as e:
        safe_print(f"[MONITOR] âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        safe_print(f"[MONITOR] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")


if __name__ == "__main__":
    main()
