"""
Standalone News Monitor - GitHub Actionsìš©
Streamlit ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  í…”ë ˆê·¸ë¨ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
3ë¶„ë§ˆë‹¤ GitHub Actionsì—ì„œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤.
"""
import os
import re
import urllib.parse
from datetime import datetime
from html import unescape
import pandas as pd
import requests

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# ìƒìˆ˜
DATA_FOLDER = os.path.abspath("data")
NEWS_DB_FILE = os.path.join(DATA_FOLDER, "news_monitor.csv")
SENT_CACHE_FILE = os.path.join(DATA_FOLDER, "sent_articles_cache.json")
_MAX_SENT_CACHE = 500  # ìºì‹œ í¬ê¸° ì œí•œ (500ê°œ)

# ì „ì†¡ëœ ê¸°ì‚¬ URL ì¶”ì  (íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ ì €ì¥)
_sent_articles_cache = set()


def _naver_headers():
    """Naver API ì¸ì¦ í—¤ë”"""
    cid = os.getenv("NAVER_CLIENT_ID", "")
    csec = os.getenv("NAVER_CLIENT_SECRET", "")
    print(f"[DEBUG] NAVER_CLIENT_ID: '{cid[:10]}...' (length: {len(cid)})")
    print(f"[DEBUG] NAVER_CLIENT_SECRET: '{csec[:5]}...' (length: {len(csec)})")
    if not cid or not csec:
        print(f"[WARNING] ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print(f"[DEBUG] Missing API keys - ID: {bool(cid)}, Secret: {bool(csec)}")
    return {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}


def _clean_text(s: str) -> str:
    """HTML íƒœê·¸ ë° ê³µë°± ì •ë¦¬"""
    if not s:
        return ""
    s = unescape(s)
    s = re.sub(r"</?b>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _normalize_url(url: str) -> str:
    """
    URL ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ URLì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    - í”„ë¡œí† ì½œ í†µì¼ (http â†’ https)
    - ë ìŠ¬ë˜ì‹œ ì œê±°
    """
    try:
        if not url:
            return ""

        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì™€ í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°
        parsed = urllib.parse.urlparse(url)
        # í”„ë¡œí† ì½œì„ httpsë¡œ í†µì¼
        scheme = "https" if parsed.scheme in ["http", "https"] else parsed.scheme
        # ì¬ì¡°ë¦½
        normalized = f"{scheme}://{parsed.netloc}{parsed.path}"
        # ë ìŠ¬ë˜ì‹œ ì œê±°
        normalized = normalized.rstrip("/")

        return normalized
    except Exception as e:
        print(f"[WARNING] URL ì •ê·œí™” ì‹¤íŒ¨: {url} - {e}")
        return url


def _publisher_from_link(u: str) -> str:
    """ë‰´ìŠ¤ ì›ë¬¸ URLì—ì„œ ë§¤ì²´ëª…ì„ í†µì¼í•´ì„œ ë°˜í™˜"""
    try:
        host = urllib.parse.urlparse(u).netloc.lower().replace("www.", "")
        if not host:
            return ""

        # 1) ì„œë¸Œë„ë©”ì¸ê¹Œì§€ ì •í™• ë§¤í•‘
        host_map = {
            "en.yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
            "news.kbs.co.kr": "KBS",
            "news.mtn.co.kr": "MTN",
            "starin.edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "sports.donga.com": "ë™ì•„ì¼ë³´",
            "biz.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
            "daily.hankooki.com": "ë°ì¼ë¦¬í•œêµ­",
            "news.dealsitetv.com": "ë”œì‚¬ì´íŠ¸TV",
        }
        if host in host_map:
            return host_map[host]

        # 2) ê¸°ë³¸ ë„ë©”ì¸(eTLD+1) ì¶”ì¶œ
        parts = host.split(".")
        if len(parts) >= 3 and parts[-1] == "kr" and parts[-2] in {
            "co","or","go","ne","re","pe","ac","hs","kg","sc",
            "seoul","busan","incheon","daegu","daejeon","gwangju","ulsan",
            "gyeonggi","gangwon","chungbuk","chungnam","jeonbuk","jeonnam",
            "gyeongbuk","gyeongnam","jeju"
        }:
            base = ".".join(parts[-3:])
        else:
            base = ".".join(parts[-2:])

        # 3) ê¸°ë³¸ ë„ë©”ì¸ â†’ ë§¤ì²´ëª… ë§¤í•‘ (ì¶•ì•½ ë²„ì „)
        base_map = {
            "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
            "kbs.co.kr": "KBS",
            "joins.com": "ì¤‘ì•™ì¼ë³´",
            "donga.com": "ë™ì•„ì¼ë³´",
            "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
            "edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "ajunews.com": "ì•„ì£¼ê²½ì œ",
            "newspim.com": "ë‰´ìŠ¤í•Œ",
            "news1.kr": "ë‰´ìŠ¤1",
            "etoday.co.kr": "ì´íˆ¬ë°ì´",
            "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
            "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
            "munhwa.com": "ë¬¸í™”ì¼ë³´",
            "segye.com": "ì„¸ê³„ì¼ë³´",
            "hankooki.com": "í•œêµ­ì¼ë³´",
            "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
            "ekn.kr": "ì—ë„ˆì§€ê²½ì œ",
            "businesskorea.co.kr": "ë¹„ì¦ˆë‹ˆìŠ¤ì½”ë¦¬ì•„",
            "ferrotimes.com": "ì² ê°•ê¸ˆì†ì‹ ë¬¸",
            # ì¶”ê°€ ë§¤ì²´ëª… ë§¤í•‘
            "thepublic.kr": "ë”í¼ë¸”ë¦­",
            "tf.co.kr": "ë”íŒ©íŠ¸",
            "straightnews.co.kr": "ìŠ¤íŠ¸ë ˆì´íŠ¸ë‰´ìŠ¤",
            "smartfn.co.kr": "ìŠ¤ë§ˆíŠ¸ê²½ì œ",
            "sisacast.kr": "ì‹œì‚¬ìºìŠ¤íŠ¸",
            "sateconomy.co.kr": "ì‹œì‚¬ê²½ì œ",
            "safetynews.co.kr": "ì•ˆì „ì‹ ë¬¸",
            "rpm9.com": "RPM9",
            "pointdaily.co.kr": "í¬ì¸íŠ¸ë°ì¼ë¦¬",
            "newsworker.co.kr": "ë‰´ìŠ¤ì›Œì»¤",
            "newsdream.kr": "ë‰´ìŠ¤ë“œë¦¼",
            "nbntv.co.kr": "NBNë‰´ìŠ¤",
            "megaeconomy.co.kr": "ë©”ê°€ê²½ì œ",
            "mediapen.com": "ë¯¸ë””ì–´íœ",
            "job-post.co.kr": "ì¡í¬ìŠ¤íŠ¸",
            "irobotnews.com": "ë¡œë´‡ì‹ ë¬¸ì‚¬",
            "ifm.kr": "ê²½ì¸ë°©ì†¡",
            "gpkorea.com": "ê¸€ë¡œë²Œì˜¤í† ë‰´ìŠ¤",
            "energydaily.co.kr": "ì—ë„ˆì§€ë°ì¼ë¦¬",
            "cstimes.com": "ì»¨ìŠˆë¨¸íƒ€ì„ìŠ¤",
            "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜",
            "autodaily.co.kr": "ì˜¤í† ë°ì¼ë¦¬",
        }
        if base in base_map:
            return base_map[base]

        return ""
    except Exception:
        return ""


def fetch_naver_news(query: str, start: int = 1, display: int = 50, sort: str = "date"):
    """Naver ë‰´ìŠ¤ API í˜¸ì¶œ"""
    try:
        url = "https://openapi.naver.com/v1/search/news.json"
        params = {"query": query, "start": start, "display": display, "sort": sort}
        headers = _naver_headers()

        print(f"[DEBUG] API Request - Query: {query}, Params: {params}")

        if not headers.get("X-Naver-Client-Id") or not headers.get("X-Naver-Client-Secret"):
            print("[DEBUG] Missing API keys, returning empty result")
            return {"items": [], "error": "missing_keys"}

        print(f"[DEBUG] Starting API request...")
        r = requests.get(url, headers=headers, params=params, timeout=10)
        print(f"[DEBUG] API Response status: {r.status_code}")

        # 429 ì—ëŸ¬ (í• ë‹¹ëŸ‰ ì´ˆê³¼) ëª…ì‹œì  ì²˜ë¦¬
        if r.status_code == 429:
            error_data = r.json() if r.text else {}
            error_msg = error_data.get("errorMessage", "API quota exceeded")
            print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ (429): {error_msg}")
            return {"items": [], "error": "quota_exceeded", "error_message": error_msg}

        r.raise_for_status()
        result = r.json()
        print(f"[DEBUG] API Response items count: {len(result.get('items', []))}")
        return result

    except requests.exceptions.Timeout:
        print(f"[WARNING] Naver API timeout for query: {query}")
        return {"items": [], "error": "timeout"}
    except requests.exceptions.RequestException as e:
        print(f"[WARNING] Naver API request failed for query: {query}, error: {e}")
        if hasattr(e, 'response') and e.response is not None:
            if e.response.status_code == 429:
                return {"items": [], "error": "quota_exceeded"}
        return {"items": [], "error": "request_failed"}
    except Exception as e:
        print(f"[WARNING] Unexpected error in fetch_naver_news: {e}")
        return {"items": [], "error": "unexpected"}


def crawl_naver_news(query: str, max_items: int = 200, sort: str = "date") -> pd.DataFrame:
    """Naver ë‰´ìŠ¤ ìˆ˜ì§‘"""
    print(f"[DEBUG] Starting crawl_naver_news for query: {query}, max_items: {max_items}")
    items, start, total = [], 1, 0
    display = min(50, max_items)
    max_attempts = 2
    attempt_count = 0
    quota_exceeded = False

    while total < max_items and start <= 100 and attempt_count < max_attempts:
        attempt_count += 1
        print(f"[DEBUG] Attempt {attempt_count} for query: {query}")

        try:
            data = fetch_naver_news(query, start=start, display=min(display, max_items - total), sort=sort)

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²´í¬
            if data.get("error") == "quota_exceeded":
                print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            arr = data.get("items", [])

            if not arr:
                print(f"[DEBUG] No items returned for query: {query}, attempt: {attempt_count}")
                break

            print(f"[DEBUG] Got {len(arr)} items for query: {query}")

            for it in arr:
                title = _clean_text(it.get("title"))
                desc = _clean_text(it.get("description"))
                link = it.get("originallink") or it.get("link") or ""
                pub = it.get("pubDate", "")
                try:
                    # GMT â†’ KST ë³€í™˜ í›„ tz ì œê±°
                    dt = pd.to_datetime(pub, utc=True).tz_convert("Asia/Seoul").tz_localize(None)
                    date_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    date_str = ""
                items.append({"ë‚ ì§œ": date_str, "ë§¤ì²´ëª…": _publisher_from_link(link),
                              "ê²€ìƒ‰í‚¤ì›Œë“œ": query, "ê¸°ì‚¬ì œëª©": title, "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½": desc, "URL": link})

            got = len(arr)
            total += got
            if got == 0:
                break
            start += got

        except Exception as e:
            print(f"[WARNING] Error in crawl_naver_news attempt {attempt_count}: {e}")
            break

    print(f"[DEBUG] crawl_naver_news completed for {query}: {len(items)} items")
    df = pd.DataFrame(items, columns=["ë‚ ì§œ", "ë§¤ì²´ëª…", "ê²€ìƒ‰í‚¤ì›Œë“œ", "ê¸°ì‚¬ì œëª©", "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", "URL"])

    # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì •ë³´ë¥¼ DataFrame ì†ì„±ìœ¼ë¡œ ì €ì¥
    if quota_exceeded:
        df.attrs['quota_exceeded'] = True
        print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")

    if not df.empty:
        # ìµœì‹ ìˆœ ì •ë ¬
        df["ë‚ ì§œ_datetime"] = pd.to_datetime(df["ë‚ ì§œ"], errors="coerce")
        df = df.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
        df = df.drop("ë‚ ì§œ_datetime", axis=1)

        # ì¤‘ë³µ ì œê±° (URL ìš°ì„ , ì—†ìœ¼ë©´ ì œëª©+ë‚ ì§œ)
        key = df["URL"].where(df["URL"].astype(bool), df["ê¸°ì‚¬ì œëª©"] + "|" + df["ë‚ ì§œ"])
        df = df.loc[~key.duplicated()].reset_index(drop=True)
    return df


def load_news_db() -> pd.DataFrame:
    """ë‰´ìŠ¤ DB ë¡œë“œ"""
    if os.path.exists(NEWS_DB_FILE):
        try:
            return pd.read_csv(NEWS_DB_FILE, encoding="utf-8")
        except Exception as e:
            print(f"[WARNING] DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    return pd.DataFrame(columns=["ë‚ ì§œ","ë§¤ì²´ëª…","ê²€ìƒ‰í‚¤ì›Œë“œ","ê¸°ì‚¬ì œëª©","ì£¼ìš”ê¸°ì‚¬ ìš”ì•½","URL"])


def save_news_db(df: pd.DataFrame):
    """ë‰´ìŠ¤ DB ì €ì¥"""
    if df.empty:
        print("[DEBUG] save_news_db skipped: empty dataframe")
        return

    # ë§¤ì²´ëª… ì •ë¦¬ (URL ê¸°ë°˜)
    if "ë§¤ì²´ëª…" in df.columns and "URL" in df.columns:
        for idx, row in df.iterrows():
            if pd.notna(row["URL"]):
                df.at[idx, "ë§¤ì²´ëª…"] = _publisher_from_link(row["URL"])

    # ìƒìœ„ 200ê°œë§Œ ì €ì¥ (ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€)
    out = df.head(200).copy()

    # data í´ë” ìƒì„±
    os.makedirs(DATA_FOLDER, exist_ok=True)

    out.to_csv(NEWS_DB_FILE, index=False, encoding="utf-8")
    print(f"[DEBUG] news saved: {len(out)} rows -> {NEWS_DB_FILE}")


def load_sent_cache() -> set:
    """
    ì „ì†¡ëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ
    Returns:
        ì „ì†¡ëœ ê¸°ì‚¬ URL ì„¸íŠ¸
    """
    if os.path.exists(SENT_CACHE_FILE):
        try:
            import json
            with open(SENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cache = set(data.get("urls", []))
                print(f"[DEBUG] ì „ì†¡ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(cache)}ê±´")
                return cache
        except Exception as e:
            print(f"[WARNING] ì „ì†¡ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()
    else:
        print(f"[DEBUG] ì „ì†¡ ìºì‹œ íŒŒì¼ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
        return set()


def save_sent_cache(cache: set):
    """
    ì „ì†¡ëœ ê¸°ì‚¬ ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥
    Args:
        cache: ì „ì†¡ëœ ê¸°ì‚¬ URL ì„¸íŠ¸
    """
    try:
        import json
        # data í´ë” ìƒì„±
        os.makedirs(DATA_FOLDER, exist_ok=True)

        # ìµœê·¼ _MAX_SENT_CACHEê°œë§Œ ìœ ì§€
        cache_list = list(cache)[-_MAX_SENT_CACHE:]

        data = {
            "urls": cache_list,
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "count": len(cache_list)
        }

        with open(SENT_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"[DEBUG] ì „ì†¡ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(cache_list)}ê±´ -> {SENT_CACHE_FILE}")
    except Exception as e:
        print(f"[WARNING] ì „ì†¡ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


def detect_new_articles(old_df: pd.DataFrame, new_df: pd.DataFrame) -> list:
    """
    ê¸°ì¡´ DBì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
    - URLì„ ìš°ì„  ì‹ë³„ìë¡œ ì‚¬ìš©
    - ìµœê·¼ 1ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ ì•Œë¦¼ ëŒ€ìƒ (ì¤‘ë³µ ë°©ì§€ ê°•í™”)
    - ìºì‹œë„ í•¨ê»˜ ì²´í¬
    """
    global _sent_articles_cache

    try:
        # ê¸°ì¡´ DBê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬ (ì²« ì‹¤í–‰ ìŠ¤íŒ¸ ë°©ì§€)
        if old_df.empty:
            print(f"[DEBUG] ê¸°ì¡´ DB ë¹„ì–´ìˆìŒ - ì²« ì‹¤í–‰ì´ë¯€ë¡œ ì•Œë¦¼ ìŠ¤í‚µ")
            return []

        if new_df.empty:
            return []

        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€
        now = datetime.now()

        # ê¸°ì¡´ DBì˜ URL ì„¸íŠ¸ ìƒì„± (ì •ê·œí™”ëœ URL ì‚¬ìš©)
        old_urls = set()
        old_urls_normalized = set()
        for _, row in old_df.iterrows():
            url = str(row.get("URL", "")).strip()
            if url and url != "nan" and url != "":
                old_urls.add(url)
                old_urls_normalized.add(_normalize_url(url))

        print(f"[DEBUG] ê¸°ì¡´ DB URL ìˆ˜: {len(old_urls)} (ì •ê·œí™”: {len(old_urls_normalized)})")
        print(f"[DEBUG] ìºì‹œ í¬ê¸°: {len(_sent_articles_cache)}ê±´")
        print(f"[DEBUG] ìˆ˜ì§‘ëœ ì‹ ê·œ ë°ì´í„° ìˆ˜: {len(new_df)}")

        # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
        new_articles = []
        for _, row in new_df.iterrows():
            url = str(row.get("URL", "")).strip()
            title = str(row.get("ê¸°ì‚¬ì œëª©", "")).strip()

            # URLì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
            if not url or url == "nan" or url == "":
                continue

            # URL ì •ê·œí™”
            url_normalized = _normalize_url(url)

            # 3ë‹¨ê³„ ì¤‘ë³µ ì²´í¬: DB + ìºì‹œ + ì •ê·œí™”
            is_in_db = url in old_urls or url_normalized in old_urls_normalized
            is_in_cache = url in _sent_articles_cache or url_normalized in _sent_articles_cache

            if is_in_db or is_in_cache:
                # ì´ë¯¸ DBì— ìˆê±°ë‚˜ ìºì‹œì— ìˆìœ¼ë©´ ìŠ¤í‚µ
                continue

            # ì—¬ê¸°ê¹Œì§€ ì™”ìœ¼ë©´ ì§„ì§œ ì‹ ê·œ ê¸°ì‚¬
            # ë‚ ì§œ ì •ë³´ ë¡œê¹… (ì‹œê°„ í•„í„°ë§ ì œê±° - GitHub Actions ì‹¤í–‰ì´ ë¶ˆê·œì¹™í•˜ë¯€ë¡œ)
            article_date_str = row.get("ë‚ ì§œ", "")
            try:
                article_date = pd.to_datetime(article_date_str, errors="coerce")
                if pd.notna(article_date):
                    time_diff = now - article_date
                    hours_diff = time_diff.total_seconds() / 3600
                    print(f"[DEBUG] âœ… ì‹ ê·œ ê¸°ì‚¬ ê°ì§€: {title[:50]}... ({hours_diff:.1f}ì‹œê°„ ì „)")
                else:
                    print(f"[DEBUG] âœ… ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨): {title[:50]}...")
            except Exception as e:
                print(f"[DEBUG] âœ… ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜): {title[:50]}... - {str(e)}")

            # URLì—ì„œ ë§¤ì²´ëª… ì¶”ì¶œ
            press = _publisher_from_link(url)

            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword = str(row.get("ê²€ìƒ‰í‚¤ì›Œë“œ", "")).strip()

            new_articles.append({
                "title": title if title and title != "nan" else "ì œëª© ì—†ìŒ",
                "link": url,
                "date": article_date_str,
                "press": press,
                "keyword": keyword
            })

        print(f"[DEBUG] ì´ {len(new_articles)}ê±´ì˜ ì§„ì§œ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (DB+ìºì‹œ ì¤‘ë³µ ì œê±°, ì‹œê°„ ì œí•œ ì—†ìŒ)")
        return new_articles

    except Exception as e:
        print(f"[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return []


def send_telegram_notification(new_articles: list):
    """
    ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ë©´ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡ (ê¸°ì‚¬ë³„ ê°œë³„ ë©”ì‹œì§€)
    """
    global _sent_articles_cache

    try:
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        print(f"[DEBUG] í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œë„ - ê¸°ì‚¬ ìˆ˜: {len(new_articles) if new_articles else 0}")
        print(f"[DEBUG] ë´‡ í† í° ì¡´ì¬: {bool(bot_token)}, Chat ID ì¡´ì¬: {bool(chat_id)}")
        print(f"[DEBUG] í˜„ì¬ ìºì‹œ í¬ê¸°: {len(_sent_articles_cache)}ê±´")

        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì•Œë¦¼ ìŠ¤í‚µ
        if not bot_token or not chat_id:
            print("[DEBUG] âš ï¸ í…”ë ˆê·¸ë¨ ì„¤ì • ì—†ìŒ - ì•Œë¦¼ ìŠ¤í‚µ")
            print("[DEBUG] ğŸ’¡ GitHub Secretsì—ì„œ TELEGRAM_BOT_TOKENê³¼ TELEGRAM_CHAT_ID ì„¤ì • í•„ìš”")
            return

        if not new_articles:
            print("[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ - ì•Œë¦¼ ìŠ¤í‚µ")
            return

        # ì´ë¯¸ ì „ì†¡ëœ ê¸°ì‚¬ í•„í„°ë§ (ìºì‹œ ê¸°ë°˜, URL ì •ê·œí™”)
        articles_to_send = []
        for article in new_articles:
            url_key = article.get("link", "")
            url_normalized = _normalize_url(url_key)

            # ì›ë³¸ URLê³¼ ì •ê·œí™” URL ëª¨ë‘ ì²´í¬
            if url_key and url_key not in _sent_articles_cache and url_normalized not in _sent_articles_cache:
                articles_to_send.append(article)
            else:
                print(f"[DEBUG] ìºì‹œì— ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìŠ¤í‚µ: {article.get('title', '')[:30]}...")

        if not articles_to_send:
            print("[DEBUG] ëª¨ë“  ê¸°ì‚¬ê°€ ì´ë¯¸ ì „ì†¡ë¨ - ì•Œë¦¼ ìŠ¤í‚µ")
            return

        print(f"[DEBUG] ì „ì†¡ ëŒ€ìƒ: {len(articles_to_send)}ê±´ (ìºì‹œ ì¤‘ë³µ ì œì™¸: {len(new_articles) - len(articles_to_send)}ê±´)")

        # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì•Œë¦¼
        articles_to_notify = articles_to_send[:10]

        # í…”ë ˆê·¸ë¨ API URL
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

        # ê° ê¸°ì‚¬ë§ˆë‹¤ ê°œë³„ ë©”ì‹œì§€ ì „ì†¡
        success_count = 0
        for article in articles_to_notify:
            title = article.get("title", "ì œëª© ì—†ìŒ")
            link = article.get("link", "")
            date = article.get("date", "")
            press = article.get("press", "")
            keyword = article.get("keyword", "")

            # ë‹¨ë¬¸ ë©”ì‹œì§€ êµ¬ì„±
            message = f"ğŸš¨ *ìƒˆ ë‰´ìŠ¤*\n\n"

            # ê²€ìƒ‰ í‚¤ì›Œë“œ í•´ì‹œíƒœê·¸ ì¶”ê°€
            if keyword:
                # ê³µë°±ì„ ì œê±°í•˜ì—¬ í•´ì‹œíƒœê·¸ë¡œ ë³€í™˜
                hashtag = keyword.replace(" ", "")
                message += f"#{hashtag}\n"

            # ì œëª© ì•ì— [ì–¸ë¡ ì‚¬] ì¶”ê°€
            if press:
                message += f"*[{press}]* {title}\n"
            else:
                message += f"*{title}*\n"

            # ë‚ ì§œì™€ ë§í¬
            if date:
                message += f"ğŸ• {date}\n"
            if link:
                message += f"ğŸ”— {link}"

            payload = {
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": True
            }

            try:
                response = requests.post(url, json=payload, timeout=10)
                if response.status_code == 200:
                    success_count += 1
                    print(f"[DEBUG] âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {title[:30]}...")

                    # ì „ì†¡ ì„±ê³µí•œ ê¸°ì‚¬ëŠ” ìºì‹œì— ì¶”ê°€ (ì›ë³¸ + ì •ê·œí™” URL ëª¨ë‘)
                    _sent_articles_cache.add(link)
                    _sent_articles_cache.add(_normalize_url(link))
                else:
                    print(f"[DEBUG] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {response.status_code} - {title[:30]}...")

                # í…”ë ˆê·¸ë¨ Rate Limit ë°©ì§€ (ì´ˆë‹¹ 30ê°œ ë©”ì‹œì§€ ì œí•œ)
                import time
                time.sleep(0.05)  # 50ms ëŒ€ê¸°

            except Exception as e:
                print(f"[DEBUG] âŒ ê°œë³„ ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")

        print(f"[DEBUG] âœ… ì´ {success_count}/{len(articles_to_notify)}ê±´ ì „ì†¡ ì™„ë£Œ")
        print(f"[DEBUG] ì „ì†¡ í›„ ìºì‹œ í¬ê¸°: {len(_sent_articles_cache)}ê±´")

        # ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥ (ì˜êµ¬ ë³´ê´€)
        save_sent_cache(_sent_articles_cache)

        # 5ê°œ ì´ìƒ ë‚¨ì€ ê¸°ì‚¬ê°€ ìˆìœ¼ë©´ ìš”ì•½ ë©”ì‹œì§€
        if len(new_articles) > 10:
            summary_message = f"ğŸ“¢ _ì™¸ {len(new_articles) - 10}ê±´ì˜ ë‰´ìŠ¤ê°€ ë” ìˆìŠµë‹ˆë‹¤._"
            payload = {
                "chat_id": chat_id,
                "text": summary_message,
                "parse_mode": "Markdown"
            }
            requests.post(url, json=payload, timeout=10)

    except Exception as e:
        print(f"[DEBUG] âŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")


def safe_print(text: str):
    """Windows ì½˜ì†” ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€"""
    try:
        print(text)
    except UnicodeEncodeError:
        # ì´ëª¨ì§€ ì œê±°í•˜ê³  ì¬ì‹œë„
        text_clean = text.encode('ascii', 'ignore').decode('ascii')
        print(text_clean)


def main():
    """
    ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ë©”ì¸ í•¨ìˆ˜
    """
    global _sent_articles_cache

    try:
        safe_print("=" * 80)
        safe_print(f"[MONITOR] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        safe_print("=" * 80)

        # ì „ì†¡ ìºì‹œ ë¡œë“œ (ì´ì „ ì‹¤í–‰ì—ì„œ ì „ì†¡í•œ ê¸°ì‚¬ ì •ë³´)
        _sent_articles_cache = load_sent_cache()

        # í‚¤ì›Œë“œ ì„¤ì •
        keywords = [
            "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
            "POSCO INTERNATIONAL",
            "í¬ìŠ¤ì½”ì¸í„°",
            "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
            "êµ¬ë™ëª¨í„°ì½”ì•„",
            "êµ¬ë™ëª¨í„°ì½”ì–´",
            "ë¯¸ì–€ë§ˆ LNG",
            "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜",
            "í¬ìŠ¤ì½”"
        ]
        exclude_keywords = ["í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„", "POSCO INTERNATIONAL", "í¬ìŠ¤ì½”ì¸í„°",
                           "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ", "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"]
        max_items = 30  # API ì‚¬ìš©ëŸ‰ ìµœì í™”

        # API í‚¤ ì²´í¬
        headers = _naver_headers()
        api_ok = bool(headers.get("X-Naver-Client-Id") and headers.get("X-Naver-Client-Secret"))

        if not api_ok:
            print("[MONITOR] âŒ API í‚¤ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # ê¸°ì¡´ DB ë¡œë“œ
        existing_db = load_news_db()
        safe_print(f"[MONITOR] ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ: {len(existing_db)}ê±´")

        # ë‰´ìŠ¤ ìˆ˜ì§‘
        all_news = []
        quota_exceeded = False

        for kw in keywords:
            safe_print(f"[MONITOR] í‚¤ì›Œë“œ '{kw}' ê²€ìƒ‰ ì¤‘...")
            df_kw = crawl_naver_news(kw, max_items=max_items // len(keywords), sort="date")

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
            if df_kw.attrs.get('quota_exceeded', False):
                safe_print(f"[MONITOR] [WARNING] API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            if not df_kw.empty:
                # "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" ì •í™•í•œ ë§¤ì¹­ ê°•í™”
                if kw == "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„":
                    def should_include_posco_intl(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                        # ì •í™•íˆ "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„"ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
                        if "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" not in title and "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" not in description:
                            return False

                        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ ì œê±° - ì •í™•í•œ íšŒì‚¬ëª…ì´ë¯€ë¡œ ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘
                        return True

                    mask = df_kw.apply(should_include_posco_intl, axis=1)
                    df_kw = df_kw[mask].reset_index(drop=True)
                    if not df_kw.empty:
                        safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„' ì •í™• ë§¤ì¹­ í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                # "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" ì •í™•í•œ ë§¤ì¹­ ê°•í™”
                elif kw == "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜":
                    def should_include_posco_mobility(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                        # ì •í™•íˆ "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
                        if "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" not in title and "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" not in description:
                            return False

                        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ ì œê±° - ì •í™•í•œ íšŒì‚¬ëª…ì´ë¯€ë¡œ ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘
                        return True

                    mask = df_kw.apply(should_include_posco_mobility, axis=1)
                    df_kw = df_kw[mask].reset_index(drop=True)
                    if not df_kw.empty:
                        safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜' ì •í™• ë§¤ì¹­ í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                # "í¬ìŠ¤ì½”" í‚¤ì›Œë“œì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                elif kw == "í¬ìŠ¤ì½”":
                    def should_include_posco(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        title_lower = title.lower()
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                        if "í¬ìŠ¤ì½”" not in title and "posco" not in title_lower:
                            return False

                        for exclude_kw in exclude_keywords:
                            if exclude_kw.lower() in title_lower:
                                return False

                        exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­", "ì¸í…Œë¦¬ì–´"]
                        for exclude_word in exclude_words:
                            if exclude_word in title or exclude_word in description:
                                return False

                        return True

                    mask_posco = df_kw.apply(should_include_posco, axis=1)
                    df_kw = df_kw[mask_posco].reset_index(drop=True)
                    if not df_kw.empty:
                        safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”' í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                else:
                    # ë‹¤ë¥¸ í‚¤ì›Œë“œëŠ” ì œëª©ê³¼ ìš”ì•½ ëª¨ë‘ì—ì„œ ë¶€ë™ì‚° ê´€ë ¨ í‚¤ì›Œë“œ ì œê±°
                    exclude_words = ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                    def should_include_general(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                        # ì œëª©ê³¼ ìš”ì•½ ëª¨ë‘ ì²´í¬
                        for exclude_word in exclude_words:
                            if exclude_word in title or exclude_word in description:
                                return False
                        return True

                    mask_general = df_kw.apply(should_include_general, axis=1)
                    df_kw = df_kw[mask_general].reset_index(drop=True)

                if not df_kw.empty:
                    all_news.append(df_kw)
                    safe_print(f"[MONITOR] '{kw}': {len(df_kw)}ê±´ ìˆ˜ì§‘")

        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
        if quota_exceeded:
            safe_print(f"[MONITOR] [ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
            safe_print(f"[MONITOR] [TIP] í•´ê²° ë°©ë²•:")
            safe_print(f"[MONITOR]    1. ìƒˆë¡œìš´ ë„¤ì´ë²„ ê°œë°œì ê³„ì •ìœ¼ë¡œ API í‚¤ ì¬ë°œê¸‰")
            safe_print(f"[MONITOR]    2. ë§¤ì¼ ìì •(KST) ì´í›„ í• ë‹¹ëŸ‰ ì¬ì„¤ì •")
            return

        # í†µí•© ì •ë¦¬ & ì €ì¥
        df_new = pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()
        if not df_new.empty:
            safe_print(f"[MONITOR] ì´ ìˆ˜ì§‘: {len(df_new)}ê±´")

            df_new["ë‚ ì§œ_datetime"] = pd.to_datetime(df_new["ë‚ ì§œ"], errors="coerce")
            df_new = df_new.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
            df_new = df_new.drop("ë‚ ì§œ_datetime", axis=1)

            # ì¤‘ë³µ ì œê±°
            key = df_new["URL"].where(df_new["URL"].astype(bool), df_new["ê¸°ì‚¬ì œëª©"] + "|" + df_new["ë‚ ì§œ"])
            df_new = df_new.loc[~key.duplicated()].reset_index(drop=True)

            # ê¸°ì¡´ DBì™€ ë³‘í•©
            merged = pd.concat([df_new, existing_db], ignore_index=True) if not existing_db.empty else df_new
            merged = merged.drop_duplicates(subset=["URL", "ê¸°ì‚¬ì œëª©"], keep="first").reset_index(drop=True)
            if not merged.empty:
                merged["ë‚ ì§œ"] = pd.to_datetime(merged["ë‚ ì§œ"], errors="coerce")
                merged = merged.sort_values("ë‚ ì§œ", ascending=False, na_position="last").reset_index(drop=True)
                merged["ë‚ ì§œ"] = merged["ë‚ ì§œ"].dt.strftime("%Y-%m-%d %H:%M")

            # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
            new_articles = detect_new_articles(existing_db, df_new)

            # DB ë¨¼ì € ì €ì¥ (race condition ë°©ì§€)
            save_news_db(merged)
            safe_print(f"[MONITOR] [SUCCESS] DB ì €ì¥ ì™„ë£Œ: ì´ {len(merged)}ê±´")

            # ê¸°ì¡´ DBê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì•Œë¦¼ ì „ì†¡ (ì²« ì‹¤í–‰ ìŠ¤íŒ¸ ë°©ì§€)
            if new_articles and not existing_db.empty:
                safe_print(f"[MONITOR] [SUCCESS] ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡")
                send_telegram_notification(new_articles)
            elif new_articles:
                safe_print(f"[MONITOR] [SKIP] ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - ì²« ì‹¤í–‰ì´ë¯€ë¡œ ì•Œë¦¼ ìŠ¤í‚µ")
                # ì²« ì‹¤í–‰ì—ì„œë„ ìºì‹œì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¤‘ë³µ ë°©ì§€
                for article in new_articles:
                    url = article.get("link", "")
                    if url:
                        _sent_articles_cache.add(url)
                        _sent_articles_cache.add(_normalize_url(url))
                print(f"[DEBUG] ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ì„ ìºì‹œì— ì¶”ê°€ (ì•Œë¦¼ ë¯¸ì „ì†¡)")
                save_sent_cache(_sent_articles_cache)

            safe_print(f"[MONITOR] [SUCCESS] ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            safe_print(f"[MONITOR] [INFO] ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸ”§ í•­ìƒ ìºì‹œ ì €ì¥ (ì‹ ê·œ ê¸°ì‚¬ ì—†ì–´ë„)
        safe_print(f"[MONITOR] ìºì‹œ ì €ì¥ ì¤‘... (í˜„ì¬ {len(_sent_articles_cache)}ê±´)")
        save_sent_cache(_sent_articles_cache)

        safe_print("=" * 80)
        safe_print(f"[MONITOR] ì‘ì—… ì¢…ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        safe_print("=" * 80)

    except Exception as e:
        safe_print(f"[MONITOR] [ERROR] ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        safe_print(f"[MONITOR] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")


if __name__ == "__main__":
    main()
