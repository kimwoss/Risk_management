"""
Standalone News Monitor - GitHub Actionsìš©
Streamlit ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  í…”ë ˆê·¸ë¨ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
3ë¶„ë§ˆë‹¤ GitHub Actionsì—ì„œ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤ (*/3 * * * *).
"""
import pandas as pd
from datetime import datetime

# ê³µí†µ ëª¨ë“ˆ import
from news_collector import (
    KEYWORDS,
    EXCLUDE_KEYWORDS,
    MAX_ITEMS_PER_RUN,
    crawl_naver_news,
    crawl_google_news_rss,
    merge_news_sources,
    load_news_db,
    save_news_db,
    load_sent_cache,
    save_sent_cache,
    load_pending_queue,  # Pending í ë¡œë“œ
    save_pending_queue,  # Pending í ì €ì¥
    add_to_pending,  # Pending íì— ê¸°ì‚¬ ì¶”ê°€
    process_pending_queue_and_send,  # Pending í ì²˜ë¦¬ ë° í…”ë ˆê·¸ë¨ ì „ì†¡
    detect_new_articles,
    send_telegram_notification,
    _naver_headers,
    _normalize_url,
    load_api_usage,
    increment_api_usage,
    check_api_quota,
    is_first_run,
    mark_initialized,
    update_run_status,
    check_api_quota_and_alert,
    send_system_alert,
)

# í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì •ì˜ (1=ìµœìš°ì„ , ìˆ«ìê°€ ë‚®ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
KEYWORD_PRIORITY = {
    "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„": 1,  # ìµœìš°ì„ 
    "POSCO INTERNATIONAL": 1,
    "í¬ìŠ¤ì½”ì¸í„°": 1,
    "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ": 2,
    "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜": 2,
    "í¬ìŠ¤ì½”í”Œë¡œìš°": 2,
    "êµ¬ë™ëª¨í„°ì½”ì•„": 3,
    "êµ¬ë™ëª¨í„°ì½”ì–´": 3,
    "ë¯¸ì–€ë§ˆ LNG": 3,
    "í¬ìŠ¤ì½”": 4,  # ê°€ì¥ ë‚®ì€ ìš°ì„ ìˆœìœ„
}

# ë¡œê±° import
try:
    from logger import logger
    LOGGER_AVAILABLE = True
except ImportError:
    LOGGER_AVAILABLE = False
    print("[WARNING] logger.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œê¹… ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")


def safe_print(text: str):
    """Windows ì½˜ì†” ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€"""
    try:
        print(text)
    except UnicodeEncodeError:
        # ì´ëª¨ì§€ ì œê±°í•˜ê³  ì¬ì‹œë„
        text_clean = text.encode('ascii', 'ignore').decode('ascii')
        print(text_clean)


def apply_keyword_filters(df: pd.DataFrame, keyword: str) -> pd.DataFrame:
    """í‚¤ì›Œë“œë³„ í•„í„°ë§ ë¡œì§ ì ìš©"""
    if df.empty:
        return df

    # "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" ì •í™•í•œ ë§¤ì¹­
    if keyword == "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            return "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" in title or "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" in description

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„' ì •í™• ë§¤ì¹­: {len(df)}ê±´")

    # "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" ì •í™•í•œ ë§¤ì¹­
    elif keyword == "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            return "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" in title or "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" in description

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜' ì •í™• ë§¤ì¹­: {len(df)}ê±´")

    # "í¬ìŠ¤ì½”í”Œë¡œìš°" ì •í™•í•œ ë§¤ì¹­
    elif keyword == "í¬ìŠ¤ì½”í”Œë¡œìš°":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            return "í¬ìŠ¤ì½”í”Œë¡œìš°" in title or "í¬ìŠ¤ì½”í”Œë¡œìš°" in description

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”í”Œë¡œìš°' ì •í™• ë§¤ì¹­: {len(df)}ê±´")

    # "í¬ìŠ¤ì½”" í‚¤ì›Œë“œ íŠ¹ë³„ ì²˜ë¦¬
    elif keyword == "í¬ìŠ¤ì½”":
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            title_lower = title.lower()
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

            # "í¬ìŠ¤ì½”" ë˜ëŠ” "posco" í¬í•¨ ì²´í¬
            if "í¬ìŠ¤ì½”" not in title and "posco" not in title_lower:
                return False

            # ë¶€ë™ì‚° í‚¤ì›Œë“œ ì œì™¸
            exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­", "ì¸í…Œë¦¬ì–´"]
            for exclude_word in exclude_words:
                if exclude_word in title or exclude_word in description:
                    return False

            return True

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)
        if not df.empty:
            safe_print(f"[MONITOR] 'í¬ìŠ¤ì½”' í•„í„°ë§ ì™„ë£Œ: {len(df)}ê±´")

    # ê¸°íƒ€ í‚¤ì›Œë“œ - ë¶€ë™ì‚° í‚¤ì›Œë“œë§Œ ì œì™¸
    else:
        exclude_words = ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
        def should_include(row):
            title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
            description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            for exclude_word in exclude_words:
                if exclude_word in title or exclude_word in description:
                    return False
            return True

        mask = df.apply(should_include, axis=1)
        df = df[mask].reset_index(drop=True)

    return df


def main():
    """ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ë©”ì¸ í•¨ìˆ˜"""
    error_count = 0
    total_collected = 0
    telegram_success = 0
    run_success = False

    try:
        safe_print("=" * 80)
        safe_print(f"[MONITOR] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        safe_print("=" * 80)

        # API í• ë‹¹ëŸ‰ í™•ì¸ ë° ê²½ê³ 
        check_api_quota_and_alert()

        # ì „ì†¡ ìºì‹œ ë¡œë“œ
        sent_cache = load_sent_cache()

        # Pending í ë¡œë“œ (ì¬ì‹œë„ ëŒ€ê¸° ì¤‘ì¸ ê¸°ì‚¬)
        pending_queue = load_pending_queue()
        safe_print(f"[MONITOR] Pending í ë¡œë“œ ì™„ë£Œ: {len(pending_queue)}ê±´")

        # ê¸°ì¡´ Pending í ë¨¼ì € ì²˜ë¦¬ (ì¬ì‹œë„)
        if pending_queue:
            safe_print(f"[MONITOR] ğŸ“¤ ê¸°ì¡´ Pending í ì²˜ë¦¬ ì‹œì‘...")
            pending_queue, sent_cache, retry_success = process_pending_queue_and_send(pending_queue, sent_cache)
            telegram_success += retry_success
            safe_print(f"[MONITOR] ğŸ“¤ Pending í ì¬ì‹œë„ ì™„ë£Œ: {retry_success}ê±´ ì „ì†¡")

            # Pending í ì¦‰ì‹œ ì €ì¥ (ì¬ì‹œë„ ê²°ê³¼ ë°˜ì˜)
            save_pending_queue(pending_queue)
            save_sent_cache(sent_cache)

        # API í‚¤ ì²´í¬
        headers = _naver_headers()
        api_ok = bool(headers.get("X-Naver-Client-Id") and headers.get("X-Naver-Client-Secret"))

        if not api_ok:
            safe_print("[MONITOR] âŒ API í‚¤ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            if LOGGER_AVAILABLE:
                logger.log_error("missing_api_key", "Naver API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            update_run_status(False, 0, 0, 0, "API í‚¤ ì—†ìŒ")
            return

        # ê¸°ì¡´ DB ë¡œë“œ
        existing_db = load_news_db()
        safe_print(f"[MONITOR] ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ: {len(existing_db)}ê±´")

        # ë‰´ìŠ¤ ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
        all_news = []
        quota_exceeded = False
        num_keywords = len(KEYWORDS)
        items_per_keyword = MAX_ITEMS_PER_RUN // num_keywords

        # í˜„ì¬ API ì‚¬ìš©ëŸ‰ í™•ì¸
        current_api_usage = load_api_usage()
        safe_print(f"[MONITOR] í˜„ì¬ API ì‚¬ìš©ëŸ‰: {current_api_usage}íšŒ")
        safe_print(f"[MONITOR] ìˆ˜ì§‘ ì„¤ì •: ì´ {MAX_ITEMS_PER_RUN}ê°œ / í‚¤ì›Œë“œë‹¹ ì•½ {items_per_keyword}ê°œ")

        # í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
        keywords_sorted = sorted(KEYWORDS, key=lambda k: KEYWORD_PRIORITY.get(k, 999))
        safe_print(f"[MONITOR] ìš°ì„ ìˆœìœ„ ì •ë ¬: {', '.join([f'{kw}(P{KEYWORD_PRIORITY.get(kw, 999)})' for kw in keywords_sorted[:3]])}...")

        for kw in keywords_sorted:
            # API í• ë‹¹ëŸ‰ í™•ì¸ (2íšŒ í˜¸ì¶œ í•„ìš” - í‰ê·  í˜ì´ì§€ë„¤ì´ì…˜)
            if not check_api_quota(required_calls=2):
                priority = KEYWORD_PRIORITY.get(kw, 999)

                # ìš°ì„ ìˆœìœ„ê°€ ë‚®ì€ í‚¤ì›Œë“œëŠ” ìŠ¤í‚µ
                if priority >= 3:
                    safe_print(f"[MONITOR] â­ï¸ API í• ë‹¹ëŸ‰ ë¶€ì¡± - ìš°ì„ ìˆœìœ„ ë‚®ì€ í‚¤ì›Œë“œ ìŠ¤í‚µ: '{kw}' (P{priority})")
                    continue
                else:
                    safe_print(f"[MONITOR] âš ï¸ API í• ë‹¹ëŸ‰ ë¶€ì¡±í•˜ì§€ë§Œ ìš°ì„ ìˆœìœ„ ë†’ìŒ: '{kw}' (P{priority}) - ê³„ì† ìˆ˜ì§‘")

            safe_print(f"[MONITOR] í‚¤ì›Œë“œ '{kw}' ê²€ìƒ‰ ì¤‘... (ìš°ì„ ìˆœìœ„: {KEYWORD_PRIORITY.get(kw, 999)})")
            naver_df = crawl_naver_news(kw, max_items=items_per_keyword, sort="date")

            # Google News RSS ì¶”ê°€ ìˆ˜ì§‘ (POSCO International í‚¤ì›Œë“œì¼ ë•Œë§Œ)
            google_df = pd.DataFrame()
            if "posco" in kw.lower() and "international" in kw.lower():
                try:
                    safe_print(f"[MONITOR] Google News RSS ìˆ˜ì§‘ ì¤‘: {kw}")
                    google_df = crawl_google_news_rss(query="POSCO International", max_items=50)
                except Exception as e:
                    safe_print(f"[MONITOR] Google News RSS ì‹¤íŒ¨: {e}")
                    google_df = pd.DataFrame()

            # Naver + Google ë³‘í•©
            df_kw = merge_news_sources(naver_df, google_df)
            if naver_df.attrs.get('quota_exceeded', False):
                df_kw.attrs['quota_exceeded'] = True

            # API ì‚¬ìš©ëŸ‰ ì¦ê°€
            current_api_usage = increment_api_usage(calls=2)

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
            if df_kw.attrs.get('quota_exceeded', False):
                safe_print(f"[MONITOR] âš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                if LOGGER_AVAILABLE:
                    logger.log_error("api_quota_exceeded", "Naver API í• ë‹¹ëŸ‰ ì´ˆê³¼")
                error_count += 1
                break

            # í‚¤ì›Œë“œë³„ í•„í„°ë§ ì ìš©
            df_kw = apply_keyword_filters(df_kw, kw)

            if not df_kw.empty:
                all_news.append(df_kw)
                total_collected += len(df_kw)
                safe_print(f"[MONITOR] '{kw}': {len(df_kw)}ê±´ ìˆ˜ì§‘")

                # ìˆ˜ì§‘ ë¡œê¹…
                if LOGGER_AVAILABLE:
                    logger.log_collection(kw, len(df_kw), api_calls=2)

        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
        if quota_exceeded:
            safe_print(f"[MONITOR] âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
            safe_print(f"[MONITOR] ğŸ’¡ ë§¤ì¼ ìì •(KST) ì´í›„ í• ë‹¹ëŸ‰ ì¬ì„¤ì •")
            return

        # í†µí•© ì •ë¦¬ & ì €ì¥
        df_new = pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()
        if not df_new.empty:
            safe_print(f"[MONITOR] ì´ ìˆ˜ì§‘: {len(df_new)}ê±´")

            # ë‚ ì§œìˆœ ì •ë ¬
            df_new["ë‚ ì§œ_datetime"] = pd.to_datetime(df_new["ë‚ ì§œ"], errors="coerce")
            df_new = df_new.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
            df_new = df_new.drop("ë‚ ì§œ_datetime", axis=1)

            # ì¤‘ë³µ ì œê±°
            key = df_new["URL"].where(df_new["URL"].astype(bool), df_new["ê¸°ì‚¬ì œëª©"] + "|" + df_new["ë‚ ì§œ"])
            df_new = df_new.loc[~key.duplicated()].reset_index(drop=True)

            # ê¸°ì¡´ DBì™€ ë³‘í•©
            merged = pd.concat([df_new, existing_db], ignore_index=True) if not existing_db.empty else df_new
            merged = merged.drop_duplicates(subset=["URL", "ê¸°ì‚¬ì œëª©"], keep="first").reset_index(drop=True)
            if not merged.empty:
                merged["ë‚ ì§œ"] = pd.to_datetime(merged["ë‚ ì§œ"], errors="coerce")
                merged = merged.sort_values("ë‚ ì§œ", ascending=False, na_position="last").reset_index(drop=True)
                merged["ë‚ ì§œ"] = merged["ë‚ ì§œ"].dt.strftime("%Y-%m-%d %H:%M")

            # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
            new_articles = detect_new_articles(existing_db, df_new, sent_cache)

            # ì‹ ê·œ ê¸°ì‚¬ë¥¼ Pending íì— ì¶”ê°€ (ëˆ„ë½ ë°©ì§€)
            if new_articles:
                safe_print(f"[MONITOR] âœ… ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - Pending íì— ì¶”ê°€")
                for article in new_articles:
                    pending_queue = add_to_pending(article, pending_queue)

                # Pending í ì¦‰ì‹œ ì €ì¥ (ë°ì´í„° ì†ì‹¤ ë°©ì§€)
                save_pending_queue(pending_queue)
                safe_print(f"[MONITOR] ğŸ’¾ Pending í ì €ì¥ ì™„ë£Œ: {len(pending_queue)}ê±´")

                # Pending í ì²˜ë¦¬ (í…”ë ˆê·¸ë¨ ì „ì†¡)
                if not is_first_run():
                    safe_print(f"[MONITOR] ğŸ“¤ Pending í ì²˜ë¦¬ ì‹œì‘ (ì‹ ê·œ ê¸°ì‚¬ ì „ì†¡)...")
                    pending_queue, sent_cache, new_success = process_pending_queue_and_send(pending_queue, sent_cache)
                    telegram_success += new_success
                    safe_print(f"[MONITOR] ğŸ“¤ ì‹ ê·œ ê¸°ì‚¬ ì „ì†¡ ì™„ë£Œ: {new_success}ê±´")

                    # Pending í ë° ìºì‹œ ì¦‰ì‹œ ì €ì¥
                    save_pending_queue(pending_queue)
                    save_sent_cache(sent_cache)

                    # í…”ë ˆê·¸ë¨ ë¡œê¹…
                    if LOGGER_AVAILABLE:
                        failed = len(new_articles) - new_success
                        logger.log_telegram(new_success, failed, len(new_articles))
                else:
                    safe_print(f"[MONITOR] â­ï¸ ì²« ì‹¤í–‰ ê°ì§€ - í…”ë ˆê·¸ë¨ ì „ì†¡ ìŠ¤í‚µ")
                    # ì²« ì‹¤í–‰ì—ì„œë„ Pending íëŠ” ìœ ì§€ (ë‹¤ìŒ ëŸ°ì—ì„œ ì „ì†¡)

            # DB ì €ì¥ (í…”ë ˆê·¸ë¨ ë°œì†¡ í›„)
            save_news_db(merged)
            safe_print(f"[MONITOR] âœ… DB ì €ì¥ ì™„ë£Œ: ì´ {len(merged)}ê±´")

            # DBì— ì €ì¥ëœ ëª¨ë“  ê¸°ì‚¬ë¥¼ ìºì‹œì— ì¶”ê°€ (ë™ê¸°í™” ë³´ì¥)
            for _, row in merged.iterrows():
                url = str(row.get("URL", "")).strip()
                if url and url != "nan" and url != "":
                    sent_cache.add(url)
                    sent_cache.add(_normalize_url(url))
            safe_print(f"[MONITOR] âœ… ìºì‹œ ë™ê¸°í™” ì™„ë£Œ: DBì˜ ëª¨ë“  ê¸°ì‚¬ URLì„ ìºì‹œì— ì¶”ê°€ ({len(sent_cache)}ê±´)")

            safe_print(f"[MONITOR] âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            safe_print(f"[MONITOR] â„¹ï¸ ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë§ˆì§€ë§‰ ìºì‹œ ë° Pending í ì €ì¥ (ì•ˆì „ì„± í™•ë³´)
        safe_print(f"[MONITOR] ìµœì¢… ìºì‹œ ì €ì¥ ì¤‘... (í˜„ì¬ {len(sent_cache)}ê±´)")
        save_sent_cache(sent_cache)

        safe_print(f"[MONITOR] ìµœì¢… Pending í ì €ì¥ ì¤‘... (í˜„ì¬ {len(pending_queue)}ê±´)")
        save_pending_queue(pending_queue)

        # ì‹¤í–‰ ìš”ì•½ ë¡œê¹…
        if LOGGER_AVAILABLE:
            logger.log_run_summary(
                total_articles=total_collected,
                new_articles=len(new_articles) if 'new_articles' in locals() else 0,
                telegram_sent=telegram_success,
                errors=error_count
            )
            # ì¼ì¼ í†µê³„ ì¶œë ¥
            logger.print_daily_summary()
            logger.save_daily_stats()

        # ì‹¤í–‰ ì„±ê³µ ìƒíƒœ ì—…ë°ì´íŠ¸
        run_success = True
        update_run_status(
            success=True,
            articles_collected=total_collected,
            new_articles=len(new_articles) if 'new_articles' in locals() else 0,
            telegram_sent=telegram_success
        )

        safe_print("=" * 80)
        safe_print(f"[MONITOR] âœ… ì‘ì—… ì„±ê³µ ì¢…ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        safe_print("=" * 80)

    except Exception as e:
        safe_print(f"[MONITOR] âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        error_details = traceback.format_exc()
        safe_print(f"[MONITOR] ìƒì„¸ ì˜¤ë¥˜:\n{error_details}")

        # ì—ëŸ¬ ë¡œê¹…
        if LOGGER_AVAILABLE:
            logger.log_error("unexpected_error", str(e), error_details)

        # ì‹¤í–‰ ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
        update_run_status(
            success=False,
            articles_collected=total_collected,
            new_articles=0,
            telegram_sent=telegram_success,
            error_message=str(e)
        )


if __name__ == "__main__":
    main()
