# streamlit_app.py
# Version: 2025-12-02-10:45 (Publisher mapping update trigger)
"""
í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì–¸ë¡ ëŒ€ì‘ ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œ
- ìƒë‹¨ ë„¤ë¹„: ìˆœìˆ˜ Streamlit ë²„íŠ¼ ê¸°ë°˜ (iFrame/JS ì œê±°, í™•ì‹¤í•œ ë¦¬ëŸ°)
- ì¤‘ë³µëœ ë¡œë”/ìŠ¤íƒ€ì¼ ì •ë¦¬
"""
import os, json, re, time, base64, mimetypes, urllib.parse, requests
from datetime import datetime, timezone, timedelta
import threading
import atexit

# ê³µí†µ ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ import
from news_collector import (
    KEYWORDS,
    EXCLUDE_KEYWORDS,
    MAX_ITEMS_PER_RUN,
    crawl_naver_news,
    load_news_db,
    _publisher_from_link,
    _clean_text,
    _naver_headers,
)

# APScheduler import with error handling
# ğŸš¨ DISABLED: GitHub Actionsë¡œ ëŒ€ì²´ - ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€
SCHEDULER_AVAILABLE = False
print("[INFO] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„í™œì„±í™” (GitHub Actions ì‚¬ìš©)")

import pandas as pd
import streamlit as st
from streamlit_autorefresh import st_autorefresh
from PIL import Image
from html import unescape
from dotenv import load_dotenv
from bs4 import BeautifulSoup  # NEW

from data_based_llm import DataBasedLLM
from components.status_dashboard import render_status_dashboard
from components.publisher_dashboard import render_publisher_dashboard
from components.news_dashboard import render_news_dashboard

# ì§€ì› ì—¬ë¶€ í”Œë˜ê·¸
SUPPORTS_FRAGMENT = hasattr(st, "fragment")
# from llm_manager import LLMManager  # ì‚¬ìš©í•˜ì§€ ì•Šì•„ ì£¼ì„ì²˜ë¦¬ (ì›í•˜ë©´ ë³µêµ¬)

# ì•ˆì „í•œ print í•¨ìˆ˜ ì •ì˜
def safe_print(*args, **kwargs):
    try:
        print(*args, **kwargs)
    except (UnicodeEncodeError, OSError):
        try:
            print("[DEBUG] Print encoding issue")
        except:
            pass

# .env íŒŒì¼ ë¡œë“œ ë° ë””ë²„ê¹…
env_loaded = load_dotenv()
try:
    print("[DEBUG] .env file loaded:", env_loaded)
    print("[DEBUG] Environment variables loaded from:", os.getcwd())

    # í™˜ê²½ë³€ìˆ˜ ì§ì ‘ í™•ì¸
    naver_id = os.getenv("NAVER_CLIENT_ID", "")
    naver_secret = os.getenv("NAVER_CLIENT_SECRET", "")
    print("[DEBUG] Initial env check - ID exists:", bool(naver_id), "Secret exists:", bool(naver_secret))
except Exception as e:
    print("DEBUG print error:", str(e))

# ----------------------------- ê¸°ë³¸ ì„¤ì • -----------------------------
st.set_page_config(
    page_title="ìœ„ê¸°ê´€ë¦¬ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ AI",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ----------------------------- ì „ì—­ CSS (iframe ì „ì²´ í­ ê°•ì œ) -----------------------------
st.markdown("""
<style>
/* Streamlit components.html iframeì„ ì „ì²´ í­ìœ¼ë¡œ ê°•ì œ */
iframe[title] {
    width: 100% !important;
    max-width: 100% !important;
}

/* ëŒ€ì‹œë³´ë“œ iframeì„ ìœ„í•œ ì¶”ê°€ ìŠ¤íƒ€ì¼ */
.stApp iframe {
    width: 100% !important;
}

/* Block ì»¨í…Œì´ë„ˆ ì „ì²´ í­ ì‚¬ìš© */
.block-container {
    max-width: 100%;
    padding-left: 2rem;
    padding-right: 2rem;
}
</style>
""", unsafe_allow_html=True)

# ----------------------------- ì¸ì¦ ì„¤ì • -----------------------------
ACCESS_CODE = "pointl"  # ë¹„ë°€ì½”ë“œ
import hashlib

def get_auth_token():
    """ì¸ì¦ í† í° ìƒì„± (ë³´ì•ˆì„ ìœ„í•´ í•´ì‹œ ì‚¬ìš©)"""
    return hashlib.sha256(f"{ACCESS_CODE}_secret_salt".encode()).hexdigest()

def check_cookie_auth():
    """ì¿ í‚¤ì—ì„œ ì¸ì¦ ì •ë³´ í™•ì¸"""
    auth_token = get_auth_token()

    # JavaScriptë¡œ ì¿ í‚¤ í™•ì¸
    cookie_script = f"""
    <script>
        function getCookie(name) {{
            const value = `; ${{document.cookie}}`;
            const parts = value.split(`; ${{name}}=`);
            if (parts.length === 2) return parts.pop().split(';').shift();
            return null;
        }}

        const authToken = getCookie('posco_auth_token');
        const authTokenValue = '{auth_token}';

        if (authToken === authTokenValue) {{
            // ì¸ì¦ ì„±ê³µ - URL íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
            if (!window.location.search.includes('auto_login=1')) {{
                const url = new URL(window.location);
                url.searchParams.set('auto_login', '1');
                window.location.href = url.toString();
            }}
        }}
    </script>
    """
    st.components.v1.html(cookie_script, height=0)

    # URL íŒŒë¼ë¯¸í„° í™•ì¸
    if st.query_params.get("auto_login") == "1":
        st.session_state.authenticated = True
        # íŒŒë¼ë¯¸í„° ì œê±° (ê¹”ë”í•œ URL ìœ ì§€)
        st.query_params.clear()
        return True

    return False

def check_authentication():
    """ì¸ì¦ í™•ì¸ í•¨ìˆ˜ (ì¿ í‚¤ + ì„¸ì…˜)"""
    # ì´ë¯¸ ì„¸ì…˜ì—ì„œ ì¸ì¦ë¨
    if st.session_state.get("authenticated", False):
        return True

    # ì¿ í‚¤ì—ì„œ ì¸ì¦ í™•ì¸
    if check_cookie_auth():
        return True

    return False

def show_login_page():
    """ë¡œê·¸ì¸ í˜ì´ì§€ í‘œì‹œ - Genesis ìŠ¤íƒ€ì¼"""
    # ë² ì´ìŠ¤ CSS ë¡œë“œ
    st.markdown("""
    <style>
      /* ë°°ê²½/í°íŠ¸ */
      @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Noto+Sans+KR:wght@400;600;700&display=swap');
      [data-testid="stAppViewContainer"]{
        background: linear-gradient(180deg,#0c0d10 0%, #0a0b0d 100%) !important;
        color:#eee; font-family:'Inter','Noto Sans KR',system-ui,Segoe UI,Roboto,Apple SD Gothic Neo,Pretendard,sans-serif;
      }
      [data-testid="stHeader"]{background:transparent; height:0;}
      section[data-testid="stSidebar"] {display:none !important;}

      /* ì¤‘ì•™ ê³ ì • ì˜¤ë²„ë ˆì´ */
      .login-overlay {
        position: fixed;
        inset: 0;
        display: grid;
        place-items: center;
        padding: 24px;
        z-index: 1;
        pointer-events: none;
      }
      .login-box {
        background: linear-gradient(135deg, rgba(24,24,28,.75), rgba(16,16,20,.9));
        border: 1px solid rgba(212,175,55,.25);
        border-radius: 16px;
        padding: 48px 40px;
        max-width: 520px;
        width: 100%;
        box-shadow: 0 12px 48px rgba(0,0,0,.4), 0 1px 0 rgba(255,255,255,.06) inset;
        backdrop-filter: blur(12px);
        text-align: center;
        pointer-events: auto;
        position: relative;
        z-index: 2;
      }

      /* ì…ë ¥/ë²„íŠ¼ í­ ì œí•œ ì»¨í…Œì´ë„ˆ */
      .login-form-wrapper {
        max-width: 420px;
        margin: 0 auto;
      }
      .login-logo {
        font-size: 56px;
        margin-bottom: 16px;
        filter: drop-shadow(0 4px 12px rgba(212,175,55,.2));
      }
      .login-title {
        font-size: 28px;
        font-weight: 700;
        margin-bottom: 8px;
        color: #fff;
        letter-spacing: -0.02em;
      }
      .login-subtitle {
        font-size: 15px;
        color: rgba(255,255,255,.6);
        margin-bottom: 36px;
        font-weight: 400;
      }

      /* ì…ë ¥ í•„ë“œ ìŠ¤íƒ€ì¼ */
      .stTextInput>div>div>input {
        background: rgba(0,0,0,.4) !important;
        border: 1px solid rgba(255,255,255,.2) !important;
        border-radius: 10px !important;
        color: #ffffff !important;
        padding: 14px 16px !important;
        font-size: 15px !important;
        transition: all 0.25s ease !important;
        position: relative !important;
        z-index: 3 !important;
        caret-color: #ffffff !important;
        -webkit-text-fill-color: #ffffff !important;
      }
      .stTextInput>div>div>input:focus {
        border-color: rgba(212,175,55,.6) !important;
        box-shadow: 0 0 0 2px rgba(212,175,55,.15) !important;
        background: rgba(0,0,0,.5) !important;
        color: #ffffff !important;
      }
      .stTextInput>div>div>input::placeholder {
        color: rgba(255,255,255,.4) !important;
      }
      .stTextInput>div>div>input:-webkit-autofill,
      .stTextInput>div>div>input:-webkit-autofill:hover,
      .stTextInput>div>div>input:-webkit-autofill:focus {
        -webkit-text-fill-color: #ffffff !important;
        -webkit-box-shadow: 0 0 0px 1000px rgba(0,0,0,.5) inset !important;
        transition: background-color 5000s ease-in-out 0s !important;
      }
      .stTextInput {
        position: relative !important;
        z-index: 3 !important;
      }

      /* ë¡œê·¸ì¸ ë²„íŠ¼ */
      .stButton>button {
        background: linear-gradient(135deg, #D4AF37, #B8941F) !important;
        border: 1px solid rgba(212,175,55,.4) !important;
        border-radius: 10px !important;
        color: #000 !important;
        font-weight: 700 !important;
        padding: 14px 24px !important;
        font-size: 16px !important;
        letter-spacing: 0.02em !important;
        transition: all 0.25s ease !important;
        box-shadow: 0 4px 16px rgba(212,175,55,.2) !important;
        position: relative !important;
        z-index: 3 !important;
        cursor: pointer !important;
      }
      .stButton>button:hover {
        background: linear-gradient(135deg, #E6C55A, #D4AF37) !important;
        border-color: rgba(212,175,55,.8) !important;
        box-shadow: 0 6px 24px rgba(212,175,55,.35) !important;
        transform: translateY(-1px) !important;
      }
      .stButton {
        position: relative !important;
        z-index: 3 !important;
      }

      /* ì—ëŸ¬ ë©”ì‹œì§€ */
      .stAlert {
        background: rgba(220,38,38,.15) !important;
        border: 1px solid rgba(220,38,38,.3) !important;
        border-radius: 8px !important;
        color: #fca5a5 !important;
      }

      /* ëª¨ë°”ì¼ ìµœì í™” (ë¡œê·¸ì¸ í˜ì´ì§€) */
      @media (max-width: 768px) {
        .login-box {
          padding: 32px 24px !important;
          max-width: 90% !important;
        }
        .login-logo {
          font-size: 48px !important;
        }
        .login-title {
          font-size: 24px !important;
        }
        .login-subtitle {
          font-size: 14px !important;
        }
      }

      @media (max-width: 480px) {
        .login-box {
          padding: 24px 20px !important;
        }
        .login-logo {
          font-size: 40px !important;
        }
        .login-title {
          font-size: 20px !important;
        }
        .login-subtitle {
          font-size: 13px !important;
          margin-bottom: 24px !important;
        }
      }
    </style>
    """, unsafe_allow_html=True)

    # ì¤‘ì•™ ê³ ì • ì˜¤ë²„ë ˆì´ë¡œ ê°ì‹¸ê¸°
    st.markdown('<div class="login-overlay">', unsafe_allow_html=True)

    # ì»¬ëŸ¼ ì—†ì´ ë°”ë¡œ ë°•ìŠ¤ ë Œë”ë§
    st.markdown('<div class="login-box">', unsafe_allow_html=True)
    st.markdown('<div class="login-logo">ğŸ›¡ï¸</div>', unsafe_allow_html=True)
    st.markdown('<div class="login-title">ìœ„ê¸°ê´€ë¦¬ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ AI</div>', unsafe_allow_html=True)
    st.markdown('<div class="login-subtitle">í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì–¸ë¡ ëŒ€ì‘ ì‹œìŠ¤í…œ</div>', unsafe_allow_html=True)

    # í¼ ë˜í¼ë¡œ ì…ë ¥/ë²„íŠ¼ í­ ì œí•œ
    st.markdown('<div class="login-form-wrapper">', unsafe_allow_html=True)

    # st.formì„ ì‚¬ìš©í•˜ì—¬ ì—”í„° í‚¤ë¡œ ì œì¶œ ê°€ëŠ¥í•˜ë„ë¡ ê°œì„ 
    with st.form(key="login_form", clear_on_submit=False):
        st.markdown('<div style="margin-bottom: 12px; text-align: left; color: rgba(255,255,255,.7); font-size: 13px; font-weight: 600;">ë¹„ë°€ì½”ë“œ</div>', unsafe_allow_html=True)
        code_input = st.text_input(
            "ë¹„ë°€ì½”ë“œ",
            type="password",
            placeholder="ë¹„ë°€ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            label_visibility="collapsed",
            key="login_code_input"
        )

        st.markdown('<div style="margin-top: 24px;"></div>', unsafe_allow_html=True)
        submit_button = st.form_submit_button("ë¡œê·¸ì¸", use_container_width=True)

    # í¼ ì œì¶œ ì²˜ë¦¬ (ì—”í„° í‚¤ ë˜ëŠ” ë²„íŠ¼ í´ë¦­)
    if submit_button:
        if code_input == ACCESS_CODE:
            # 1ë‹¨ê³„: ì„¸ì…˜ ìƒíƒœ ì¦‰ì‹œ ì„¤ì • (ìš°ì„ ìˆœìœ„ ìµœìƒìœ„)
            st.session_state.authenticated = True
            st.session_state.login_pending_cookie = True

            # 2ë‹¨ê³„: ì¿ í‚¤ ì„¤ì • (ë°±ê·¸ë¼ìš´ë“œ, ë¹„ë™ê¸°)
            auth_token = get_auth_token()
            set_cookie_script = f"""
            <script>
                (function() {{
                    const days = 30;
                    const date = new Date();
                    date.setTime(date.getTime() + (days * 24 * 60 * 60 * 1000));
                    const expires = "expires=" + date.toUTCString();
                    document.cookie = "posco_auth_token={auth_token}; " + expires + "; path=/; SameSite=Lax";
                }})();
            </script>
            """
            st.components.v1.html(set_cookie_script, height=0)

            # 3ë‹¨ê³„: ì¦‰ì‹œ ë¦¬ëŸ° (ì„¸ì…˜ ìƒíƒœ ê¸°ë°˜ìœ¼ë¡œ ë©”ì¸ í™”ë©´ í‘œì‹œ)
            st.rerun()
        else:
            st.error("ì˜ëª»ëœ ë¹„ë°€ì½”ë“œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    st.markdown('</div>', unsafe_allow_html=True)  # </div class="login-form-wrapper">
    st.markdown('</div>', unsafe_allow_html=True)  # </div class="login-box">
    st.markdown('</div>', unsafe_allow_html=True)  # </div class="login-overlay">

DATA_FOLDER = os.path.abspath("data")
MASTER_DATA_FILE = os.path.join(DATA_FOLDER, "master_data.json")
MEDIA_RESPONSE_FILE = os.path.join(DATA_FOLDER, "ì–¸ë¡ ëŒ€ì‘ë‚´ì—­.csv")
NEWS_DB_FILE = os.path.join(DATA_FOLDER, "news_monitor.csv")
try:
    print("[DEBUG] NEWS_DB_FILE:", NEWS_DB_FILE)
except:
    pass

# ----------------------------- ìºì‹œ ë¡œë”(ë‹¨ì¼) -----------------------------
@st.cache_data
def _load_json_with_key(path: str, _cache_key: float) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def load_master_data():
    try:
        mtime = os.path.getmtime(MASTER_DATA_FILE)
    except OSError:
        mtime = 0.0
    return _load_json_with_key(MASTER_DATA_FILE, mtime)

def load_master_data_fresh():
    """ìºì‹œ ì—†ì´ í•­ìƒ ìµœì‹  ë°ì´í„°ë¥¼ ë¡œë“œ"""
    try:
        print(f"[DEBUG] ìºì‹œ ì—†ì´ ì§ì ‘ ë¡œë“œ: {MASTER_DATA_FILE}")
        with open(MASTER_DATA_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"[DEBUG] ë¡œë“œëœ ë°ì´í„° í‚¤: {list(data.keys())}")
        print(f"[DEBUG] ì–¸ë¡ ì‚¬ ìˆ˜: {len(data.get('media_contacts', {}))}")
        return data
    except Exception as e:
        print(f"[DEBUG] ì§ì ‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def clear_data_cache():
    """ìºì‹œë¥¼ í´ë¦¬ì–´í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # Streamlit ìºì‹œ ì™„ì „ ì´ˆê¸°í™”
        st.cache_data.clear()
        st.cache_resource.clear()
        
        # ê°œë³„ í•¨ìˆ˜ ìºì‹œë„ í´ë¦¬ì–´
        if hasattr(_load_json_with_key, 'clear'):
            _load_json_with_key.clear()
        if hasattr(_load_csv_with_key, 'clear'):
            _load_csv_with_key.clear()
            
        # ì„¸ì…˜ ìƒíƒœë„ ì´ˆê¸°í™”
        if 'master_data_cache' in st.session_state:
            del st.session_state['master_data_cache']
            
        print("[DEBUG] ìºì‹œê°€ ì™„ì „íˆ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"[DEBUG] ìºì‹œ í´ë¦¬ì–´ ì¤‘ ì˜¤ë¥˜: {e}")

@st.cache_data
def _load_csv_with_key(path: str, _cache_key: float) -> pd.DataFrame:
    try:
        return pd.read_csv(path, encoding="utf-8")
    except Exception:
        return pd.DataFrame()

def load_media_response_data():
    try:
        mtime = os.path.getmtime(MEDIA_RESPONSE_FILE)
        fsize = os.path.getsize(MEDIA_RESPONSE_FILE)
        # íŒŒì¼ í¬ê¸°ì™€ ìˆ˜ì • ì‹œê°„ì„ ì¡°í•©í•˜ì—¬ ìºì‹œ í‚¤ ìƒì„± (ë” ê°•ë ¥í•œ ìºì‹œ ë¬´íš¨í™”)
        cache_key = mtime + (fsize / 1000000.0)  # í¬ê¸°ë¥¼ MB ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
    except OSError:
        cache_key = 0.0
    return _load_csv_with_key(MEDIA_RESPONSE_FILE, cache_key)

# ----------------------------- ë°ì´í„° API -----------------------------
def get_media_contacts():
    return load_master_data().get("media_contacts", {})

def search_media_info(media_name: str):
    try:
        # ìµœì‹  ë°ì´í„° ë¡œë“œ (ìºì‹œ ì—†ì´)
        master_data = load_master_data_fresh()
        media_contacts = master_data.get("media_contacts", {})
        
        if not media_contacts:
            return None

        # 1ì°¨: ì™„ì „ ì¼ì¹˜
        if media_name in media_contacts:
            md = media_contacts[media_name]
        else:
            # 2ì°¨: ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰
            matched_key = None
            for key in media_contacts.keys():
                if media_name in key or key in media_name:
                    matched_key = key
                    break
            
            if matched_key:
                md = media_contacts[matched_key]
                media_name = matched_key  # ë§¤ì¹˜ëœ í‚¤ë¡œ ì—…ë°ì´íŠ¸
            else:
                return None
        
        # ì¶œì…ê¸°ì ë°ì´í„° ì²˜ë¦¬ (ìƒˆë¡œìš´ êµ¬ì¡°ì™€ ê¸°ì¡´ êµ¬ì¡° ëª¨ë‘ ì§€ì›)
        reporters = md.get("ì¶œì…ê¸°ì", [])
        processed_reporters = []
        
        for reporter in reporters:
            if isinstance(reporter, dict):
                # ìƒˆë¡œìš´ êµ¬ì¡° (ë”•ì…”ë„ˆë¦¬) - ë¹ˆ í•„ë“œëŠ” ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬
                processed_reporter = {
                    "ì´ë¦„": reporter.get("ì´ë¦„", ""),
                    "ì§ì±…": reporter.get("ì§ì±…", ""),
                    "ì—°ë½ì²˜": reporter.get("ì—°ë½ì²˜", ""),
                    "ì´ë©”ì¼": reporter.get("ì´ë©”ì¼", "")
                }
                processed_reporters.append(processed_reporter)
            else:
                # ê¸°ì¡´ êµ¬ì¡° (ë¬¸ìì—´)
                processed_reporters.append({
                    "ì´ë¦„": reporter,
                    "ì§ì±…": "",
                    "ì—°ë½ì²˜": "", 
                    "ì´ë©”ì¼": ""
                })
        
        return {
            "name": media_name,
            "type": md.get("êµ¬ë¶„", "N/A"),
            "contact_person": md.get("ë‹´ë‹¹ì", "N/A"),
            "main_phone": md.get("ëŒ€í‘œì—°ë½ì²˜", "N/A"),
            "fax": md.get("íŒ©ìŠ¤", "N/A"),
            "address": md.get("ì£¼ì†Œ", "N/A"),
            "desk": md.get("DESK", []),
            "reporters": processed_reporters,
            "raw_data": md,
        }
    except Exception as e:
        st.error(f"ì–¸ë¡ ì‚¬ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def generate_issue_report(media_name, reporter_name, issue_description):
    try:
        data_llm = DataBasedLLM()
        return data_llm.generate_issue_report(media_name, reporter_name, issue_description)
    except Exception as e:
        return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ----------------------------- ë‰´ìŠ¤ ìœ í‹¸ -----------------------------
def _naver_headers():
    cid = os.getenv("NAVER_CLIENT_ID", "")
    csec = os.getenv("NAVER_CLIENT_SECRET", "")
    print(f"[DEBUG] NAVER_CLIENT_ID: '{cid[:10]}...' (length: {len(cid)})")
    print(f"[DEBUG] NAVER_CLIENT_SECRET: '{csec[:5]}...' (length: {len(csec)})")
    if not cid or not csec:
        st.error("ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print(f"[DEBUG] Missing API keys - ID: {bool(cid)}, Secret: {bool(csec)}")
    return {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}

def _clean_text(s: str) -> str:
    if not s:
        return ""
    s = unescape(s)
    s = re.sub(r"</?b>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _publisher_from_link(u: str) -> str:
    """ë‰´ìŠ¤ ì›ë¬¸ URLì—ì„œ ë§¤ì²´ëª…ì„ í†µì¼í•´ì„œ ë°˜í™˜í•œë‹¤."""
    try:
        host = urllib.parse.urlparse(u).netloc.lower().replace("www.", "")
        if not host:
            return ""

        # 1) ì„œë¸Œë„ë©”ì¸ê¹Œì§€ ì •í™• ë§¤í•‘(ì„œë¸Œë„ë©”ì¸ ìì²´ê°€ ë§¤ì²´ì¸ ì¼€ì´ìŠ¤ ìš°ì„ )
        host_map = {
            "en.yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
            "news.kbs.co.kr": "KBS",
            "news.mtn.co.kr": "MTN",
            "starin.edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "sports.donga.com": "ë™ì•„ì¼ë³´",
            "biz.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
            "daily.hankooki.com": "ë°ì¼ë¦¬í•œêµ­",
            "news.dealsitetv.com": "ë”œì‚¬ì´íŠ¸TV",
        }
        if host in host_map:
            return host_map[host]

        # 2) ê¸°ë³¸ ë„ë©”ì¸(eTLD+1) ì¶”ì¶œ: *.co.kr ë“± í•œêµ­í˜• 2ë ˆë²¨ ë„ë©”ì¸ ì²˜ë¦¬
        parts = host.split(".")
        if len(parts) >= 3 and parts[-1] == "kr" and parts[-2] in {
            "co","or","go","ne","re","pe","ac","hs","kg","sc",
            "seoul","busan","incheon","daegu","daejeon","gwangju","ulsan",
            "gyeonggi","gangwon","chungbuk","chungnam","jeonbuk","jeonnam",
            "gyeongbuk","gyeongnam","jeju"
        }:
            base = ".".join(parts[-3:])   # ì˜ˆ) en.yna.co.kr â†’ yna.co.kr
        else:
            base = ".".join(parts[-2:])   # ì˜ˆ) starnewskorea.com â†’ starnewskorea.com

        # 3) ê¸°ë³¸ ë„ë©”ì¸ â†’ ë§¤ì²´ëª… ë§¤í•‘
        base_map = {
            # ì£¼ìš” ì¢…í•©ì§€ / ë°©ì†¡
            "chosun.com": "ì¡°ì„ ì¼ë³´",
            "donga.com": "ë™ì•„ì¼ë³´",
            "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
            "joins.com": "ì¤‘ì•™ì¼ë³´",
            "hani.co.kr": "í•œê²¨ë ˆ",
            "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
            "mk.co.kr": "ë§¤ì¼ê²½ì œ",
            "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
            "hankyung.com": "í•œêµ­ê²½ì œ",
            "kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
            "seoul.co.kr": "ì„œìš¸ì‹ ë¬¸",
            "segye.com": "ì„¸ê³„ì¼ë³´",
            "naeil.com": "ë‚´ì¼ì‹ ë¬¸",
            "imaeil.com": "ë§¤ì¼ì‹ ë¬¸",
            "nongmin.com": "ë†ë¯¼ì‹ ë¬¸",
            "yeongnam.com": "ì˜ë‚¨ì¼ë³´",
            "kwnews.co.kr": "ê°•ì›ì¼ë³´",
            "kado.net": "ê°•ì›ë„ë¯¼ì¼ë³´",
            "kihoilbo.co.kr": "ê¸°í˜¸ì¼ë³´",
            "ksmnews.co.kr": "ê²½ìƒë§¤ì¼ì‹ ë¬¸",
            "kbmaeil.com": "ê²½ë¶ë§¤ì¼",
            "idaegu.co.kr": "IDNëŒ€êµ¬ì‹ ë¬¸",
            "hidomin.com": "í•˜ì´ë„ë¯¼",
            "incheonilbo.com": "ì¸ì²œì¼ë³´",
            "incheonnews.com": "ì¸ì²œë‰´ìŠ¤",
            "ggilbo.com": "ê¸ˆê°•ì¼ë³´",
            "joongdo.co.kr": "ì¤‘ë„ì¼ë³´",
            "dnews.co.kr": "ëŒ€í•œê²½ì œ",
            "jeonmae.co.kr": "ì „êµ­ë§¤ì¼ì‹ ë¬¸",

            # ë°©ì†¡ì‚¬
            "kbs.co.kr": "KBS",
            "mbc.co.kr": "MBC",
            "sbs.co.kr": "SBS",
            "ytn.co.kr": "YTN",
            "yonhapnewstv.co.kr": "ì—°í•©ë‰´ìŠ¤TV",
            "ifm.kr": "ê²½ì¸ë°©ì†¡",
            "kbsm.net": "KBSë¶€ì‚°Â·ê²½ë‚¨",
            "spotvnews.co.kr": "ìŠ¤í¬TV",

            # í†µì‹ ì‚¬
            "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
            "news1.kr": "ë‰´ìŠ¤1",
            "newsis.com": "ë‰´ì‹œìŠ¤",
            "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
            "newspim.com": "ë‰´ìŠ¤í•Œ",

            # ê²½ì œ / ê¸ˆìœµ
            "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
            "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
            "herald.co.kr": "í—¤ëŸ´ë“œê²½ì œ",
            "sedaily.com": "ì„œìš¸ê²½ì œ",
            "etoday.co.kr": "ì´íˆ¬ë°ì´",
            "edaily.co.kr": "ì´ë°ì¼ë¦¬",
            "bizwatch.co.kr": "ë¹„ì¦ˆì›Œì¹˜",
            "businesspost.co.kr": "ë¹„ì¦ˆë‹ˆìŠ¤í¬ìŠ¤íŠ¸",
            "businesskorea.co.kr": "ë¹„ì¦ˆë‹ˆìŠ¤ì½”ë¦¬ì•„",
            "finomy.com": "í˜„ëŒ€ê²½ì œì‹ ë¬¸",
            "econovill.com": "ì´ì½”ë…¸ë¹Œ",
            "econonews.co.kr": "ì´ì½”ë…¸ë‰´ìŠ¤",
            "ezyeconomy.com": "ì´ì§€ê²½ì œ",
            "queen.co.kr": "ì´ì½”ë…¸ë¯¸í€¸",
            "widedaily.com": "ì™€ì´ë“œê²½ì œ",
            "goodkyung.com": "êµ¿ëª¨ë‹ê²½ì œ",
            "smartfn.co.kr": "ìŠ¤ë§ˆíŠ¸ê²½ì œ",
            "megaeconomy.co.kr": "ë©”ê°€ê²½ì œ",
            "pointe.co.kr": "í¬ì¸íŠ¸ê²½ì œ",
            "pointdaily.co.kr": "í¬ì¸íŠ¸ë°ì¼ë¦¬",
            "marketnews.co.kr": "ë§ˆì¼“ë‰´ìŠ¤",
            "womaneconomy.co.kr": "ì—¬ì„±ê²½ì œì‹ ë¬¸",

            # ì¡°ì„  ê³„ì—´
            "chosunbiz.com": "ì¡°ì„ ë¹„ì¦ˆ",
            "investchosun.com": "ì¸ë² ìŠ¤íŠ¸ì¡°ì„ ",
            "futurechosun.com": "ë”ë‚˜ì€ë¯¸ë˜",
            "it.chosun.com": "ITì¡°ì„ ",
            "dizzo.com": "ë””ì§€í‹€ì¡°ì„ ì¼ë³´",
            "economist.co.kr": "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸",

            # IT / í…Œí¬
            "zdnet.co.kr": "ì§€ë””ë„·ì½”ë¦¬ì•„",
            "ddaily.co.kr": "ë””ì§€í„¸ë°ì¼ë¦¬",
            "bloter.net": "ë¸”ë¡œí„°",
            "digitaltoday.co.kr": "ë””ì§€í„¸íˆ¬ë°ì´",
            "thelec.kr": "ë”ì¼ë ‰",
            "theguru.co.kr": "ë”êµ¬ë£¨",
            "techholic.co.kr": "í…Œí¬í™€ë¦­",
            "e-science.co.kr": "eì‚¬ì´ì–¸ìŠ¤",
            "e-platform.net": "eí”Œë«í¼",
            "irobotnews.com": "ë¡œë´‡ì‹ ë¬¸ì‚¬",
            "koit.co.kr": "ì •ë³´í†µì‹ ì‹ ë¬¸",

            # ì •ì¹˜ / ì‹œì‚¬
            "polinews.co.kr": "í´ë¦¬ë‰´ìŠ¤",
            "sisajournal.com": "ì‹œì‚¬ì €ë„",
            "sisajournal-e.com": "ì‹œì‚¬ì €ë„e",
            "sisaweek.com": "ì‹œì‚¬ìœ„í¬",
            "sisaon.co.kr": "ì‹œì‚¬ON",
            "sisafocus.co.kr": "ì‹œì‚¬í¬ì»¤ìŠ¤",
            "sisacast.kr": "ì‹œì‚¬ìºìŠ¤íŠ¸",
            "sateconomy.co.kr": "ì‹œì‚¬ê²½ì œ",
            "straightnews.co.kr": "ìŠ¤íŠ¸ë ˆì´íŠ¸ë‰´ìŠ¤",
            "thepublic.kr": "ë”í¼ë¸”ë¦­",
            "mediapen.com": "ë¯¸ë””ì–´íœ",
            "newdaily.co.kr": "ë‰´ë°ì¼ë¦¬",
            "breaknews.com": "ë¸Œë ˆì´í¬ë‰´ìŠ¤",

            # ì˜¨ë¼ì¸ / ê¸°íƒ€
            "wikitree.co.kr": "ìœ„í‚¤íŠ¸ë¦¬",
            "insight.co.kr": "ì¸ì‚¬ì´íŠ¸",
            "insightkorea.co.kr": "ì¸ì‚¬ì´íŠ¸ì½”ë¦¬ì•„",
            "newstapa.org": "ë‰´ìŠ¤íƒ€íŒŒ",
            "tf.co.kr": "ë”íŒ©íŠ¸",
            "newsway.co.kr": "ë‰´ìŠ¤ì›¨ì´",
            "newspost.kr": "ë‰´ìŠ¤í¬ìŠ¤íŠ¸",
            "newswatch.kr": "ë‰´ìŠ¤ì›Œì¹˜",
            "newsprime.co.kr": "ë‰´ìŠ¤í”„ë¼ì„",
            "newsinside.kr": "ë‰´ìŠ¤ì¸ì‚¬ì´ë“œ",
            "news2day.co.kr": "ë‰´ìŠ¤2ë°ì´",
            "newsquest.co.kr": "ë‰´ìŠ¤í€˜ìŠ¤íŠ¸",
            "newsworker.co.kr": "ë‰´ìŠ¤ì›Œì»¤",
            "newsdream.kr": "ë‰´ìŠ¤ë“œë¦¼",
            "newsbrite.net": "ë‰´ìŠ¤ë¸Œë¼ì´íŠ¸",
            "newsmaker.or.kr": "ë‰´ìŠ¤ë©”ì´ì»¤",

            # ì‚°ì—… / ì—ë„ˆì§€
            "ekn.kr": "ì—ë„ˆì§€ê²½ì œ",
            "energy-news.co.kr": "ì—ë„ˆì§€ë‰´ìŠ¤",
            "energydaily.co.kr": "ì—ë„ˆì§€ë°ì¼ë¦¬",
            "todayenergy.kr": "íˆ¬ë°ì´ì—ë„ˆì§€",
            "gasnews.com": "ê°€ìŠ¤ì‹ ë¬¸",
            "epj.co.kr": "ì¼ë ‰íŠ¸ë¦­íŒŒì›Œ",
            "amenews.kr": "ì‹ ì†Œì¬ê²½ì œì‹ ë¬¸",
            "ferrotimes.com": "ì² ê°•ê¸ˆì†ì‹ ë¬¸",

            # ìŠ¤í¬ì¸  / ì—”í„°
            "sportsseoul.com": "ìŠ¤í¬ì¸ ì„œìš¸",
            "xportsnews.com": "ì—‘ìŠ¤í¬ì¸ ë‰´ìŠ¤",
            "starnewskorea.com": "ìŠ¤íƒ€ë‰´ìŠ¤",
            "topstarnews.net": "íƒ‘ìŠ¤íƒ€ë‰´ìŠ¤",
            "isplus.com": "ì¼ê°„ìŠ¤í¬ì¸ ",
            "swtvnews.com": "ìŠ¤í¬ì¸ W",

            # ì¢…êµ / íŠ¹ìˆ˜
            "bbsi.co.kr": "ë¶ˆêµë°©ì†¡",
            "bzeronews.com": "ë¶ˆêµê³µë‰´ìŠ¤",

            # ê¸°íƒ€
            "kpinews.kr": "KPIë‰´ìŠ¤",
            "nbnews.kr": "NBNë‰´ìŠ¤",
            "nbntv.co.kr": "NBNë‰´ìŠ¤",
            "dkilbo.com": "ëŒ€ê²½ì¼ë³´",
            "asiatime.co.kr": "ì•„ì‹œì•„íƒ€ì„ì¦ˆ",
            "kukinews.com": "ì¿ í‚¤ë‰´ìŠ¤",
            "wikileaks-kr.org": "ìœ„í‚¤ë¦¬í¬ìŠ¤í•œêµ­",
            "thepowernews.co.kr": "ë”íŒŒì›Œ",
            "shinailbo.co.kr": "ì‹ ì•„ì¼ë³´",
            "pinpointnews.co.kr": "í•€í¬ì¸íŠ¸ë‰´ìŠ¤",
            "newsworks.co.kr": "ë‰´ìŠ¤ì›ìŠ¤",
            "newstomato.com": "ë‰´ìŠ¤í† ë§ˆí† ",
            "munhwa.com": "ë¬¸í™”ì¼ë³´",
            "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
            "metroseoul.co.kr": "ë©”íŠ¸ë¡œì„œìš¸",
            "m-i.kr": "ë§¤ì¼ì¼ë³´",
            "lawissue.co.kr": "ë²•ë¥ ì €ë„",
            "joongangenews.com": "ì¤‘ì•™ì´ì½”ë…¸ë¯¸ë‰´ìŠ¤",
            "hellot.net": "í—¬ë¡œí‹°",
            "enewstoday.co.kr": "ì´ë‰´ìŠ¤íˆ¬ë°ì´",
            "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
            "bokuennews.com": "ë³µì§€ë‰´ìŠ¤",
            "snmnews.com": "ì² ê°•ê¸ˆì†ì‹ ë¬¸",
            "whitepaper.co.kr": "í™”ì´íŠ¸í˜ì´í¼",
            "theviewers.co.kr": "ë”ë·°ì–´ìŠ¤",
            "thevaluenews.co.kr": "ë”ë°¸ë¥˜ë‰´ìŠ¤",
            "thebigdata.co.kr": "ë”ë¹…ë°ì´í„°",
            "stardailynews.co.kr": "ìŠ¤íƒ€ë°ì¼ë¦¬ë‰´ìŠ¤",
            "smedaily.co.kr": "SMEë°ì¼ë¦¬",
            "smarttoday.co.kr": "ìŠ¤ë§ˆíŠ¸íˆ¬ë°ì´",
            "pressian.com": "í”„ë ˆì‹œì•ˆ",
            "ntoday.co.kr": "ì—”íˆ¬ë°ì´",
            "nspna.com": "NSPí†µì‹ ",
            "moneys.co.kr": "ë¨¸ë‹ˆS",
            "klnews.co.kr": "ë¬¼ë¥˜ì‹ ë¬¸",
            "job-post.co.kr": "ì¡í¬ìŠ¤íŠ¸",
            "ilyosisa.co.kr": "ì¼ìš”ì‹œì‚¬",
            "greened.kr": "ë…¹ìƒ‰ê²½ì œì‹ ë¬¸",
            "globalepic.co.kr": "ê¸€ë¡œë²Œì´ì½”ë…¸ë¯¹",
            "electimes.com": "ì „ê¸°ì‹ ë¬¸",
            "einfomax.co.kr": "ì—°í•©ì¸í¬ë§¥ìŠ¤",
            "dealsite.co.kr": "ë”œì‚¬ì´íŠ¸",
            "dailycar.co.kr": "ë°ì¼ë¦¬ì¹´",
            "cnbnews.com": "CNBë‰´ìŠ¤",
            "ceoscoredaily.com": "CEOìŠ¤ì½”ì–´ë°ì¼ë¦¬",
            "autodaily.co.kr": "ì˜¤í† ë°ì¼ë¦¬",
            "weeklytoday.com": "ìœ„í´ë¦¬íˆ¬ë°ì´",
            "viva100.com": "ë¸Œë¦¿ì§€ê²½ì œ",
            "veritas-a.com": "ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ",
            "thetracker.co.kr": "ë”íŠ¸ë˜ì»¤",
            "sportschosun.com": "ìŠ¤í¬ì¸ ì¡°ì„ ",
            "seoulfn.com": "ì„œìš¸íŒŒì´ë‚¸ìŠ¤",
            "nextdaily.co.kr": "ë„¥ìŠ¤íŠ¸ë°ì¼ë¦¬",
            "newscj.com": "ì²œì§€ì¼ë³´",
            "newscape.co.kr": "ë‰´ìŠ¤ìŠ¤ì¼€ì´í”„",
            "mhj21.com": "ë¬¸í™”ì €ë„21",
            "kpenews.com": "í•œêµ­ì •ì¹˜ê²½ì œì‹ ë¬¸",
            "iminju.net": "ê²½ê¸°ë¯¼ì£¼ì–¸ë¡ ì‹œë¯¼ì—°í•©",
            "ilyoseoul.co.kr": "ì¼ìš”ì„œìš¸",
            "ibabynews.com": "ë² ì´ë¹„ë‰´ìŠ¤",
            "hansbiz.co.kr": "í•œìŠ¤ê²½ì œ",
            "gukjenews.com": "êµ­ì œë‰´ìŠ¤",
            "ftoday.co.kr": "í“¨ì³ë°ì¼ë¦¬",
            "financialpost.co.kr": "íŒŒì´ë‚¸ì…œí¬ìŠ¤íŠ¸",
            "fetv.co.kr": "FETV",
            "etnews.com": "ì „ìì‹ ë¬¸",
            "dailian.co.kr": "ë°ì¼ë¦¬ì•ˆ",
            "cstimes.com": "ì»¨ìŠˆë¨¸íƒ€ì„ìŠ¤",
            "bizwork.co.kr": "ë¹„ì¦ˆì›ìŠ¤",
            "betanews.net": "ë² íƒ€ë‰´ìŠ¤",
            "banronbodo.com": "ë°˜ë¡ ë³´ë„ë‹·ì»´",
            "topdaily.kr": "í†±ë°ì¼ë¦¬",
            "thebell.co.kr": "ë”ë²¨",
            "the-pr.co.kr": "ë”í”¼ì•Œ",
            "stoo.com": "ìŠ¤í¬ì¸ íˆ¬ë°ì´",
            "sportsworldi.com": "ìŠ¤í¬ì¸ ì›”ë“œ",
            "seoulwire.com": "ì„œìš¸ì™€ì´ì–´",
            "press9.kr": "í”„ë ˆìŠ¤ë‚˜ì¸",
            "newstown.co.kr": "ë‰´ìŠ¤íƒ€ìš´",
            "mhnse.com": "MHNìŠ¤í¬ì¸ (ê²½ì œ)",
            "koreastocknews.com": "ì½”ë¦¬ì•„ìŠ¤íƒë‰´ìŠ¤",
            "fntimes.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤íƒ€ì„ì¦ˆ",
            "choicenews.co.kr": "ì´ˆì´ìŠ¤ê²½ì œ",
            "asiatoday.co.kr": "ì•„ì‹œì•„íˆ¬ë°ì´",
        }

        return base_map.get(base, base)  # ëª¨ë¥´ëŠ” ë„ë©”ì¸ì€ 'ê¸°ë³¸ ë„ë©”ì¸'ìœ¼ë¡œ í†µì¼
    except Exception:
        return ""

# --- OpenAI í‚¤ ì¡°íšŒ (OPENAI_API_KEY ë˜ëŠ” OPEN_API_KEY ë‘˜ ë‹¤ ì§€ì›) ---
def _get_openai_key():
    return os.getenv("OPENAI_API_KEY") or os.getenv("OPEN_API_KEY") or ""

def _openai_chat(messages, model=None, temperature=0.2, max_tokens=400):
    """ê²½ëŸ‰ OpenAI Chat í˜¸ì¶œ (ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€)"""
    api_key = _get_openai_key()
    if not api_key:
        return None, "OPENAI_API_KEY not set"
    model = model or os.getenv("OPENAI_GPT_MODEL", "gpt-4o-mini")
    r = None
    try:
        r = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={"model": model, "messages": messages, "temperature": temperature, "max_tokens": max_tokens, "n": 1},
            timeout=25,
        )
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip(), None
    except Exception as e:
        return None, str(e)
    finally:
        # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€
        if r is not None:
            r.close()

# --- ê¸°ì‚¬ ë³¸ë¬¸/ì œëª© ì¶”ì¶œ (ê°€ë²¼ìš´ í¬ë¡¤ëŸ¬, ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€) ---
def _extract_article_text_and_title(url: str):
    """ê¸°ì‚¬ í¬ë¡¤ë§ (ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€)"""
    html = ""
    resp = None
    try:
        resp = requests.get(url, timeout=12, headers={"User-Agent": "Mozilla/5.0"})
        resp.raise_for_status()
        html = resp.text
    except Exception:
        pass
    finally:
        # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€: ì‘ë‹µ ê°ì²´ ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°
        if resp is not None:
            resp.close()

    title = ""
    text = ""
    if html:
        soup = BeautifulSoup(html, "html.parser")
        # ì œëª©
        og = soup.find("meta", property="og:title")
        if og and og.get("content"): title = og["content"].strip()
        elif soup.title: title = soup.title.get_text(strip=True)

        # ë³¸ë¬¸ (ë„¤ì´ë²„ ë“± ìš°ì„  ì‹œë„ â†’ ì¼ë°˜ article/p â†’ ì „ì²´ í…ìŠ¤íŠ¸)
        candidates = ["#dic_area", "article", "div#newsEndContents", "div#articeBody", "div#content", "div.article_body"]
        node = None
        for sel in candidates:
            node = soup.select_one(sel)
            if node: break
        if node:
            ps = node.find_all(["p", "div"])
            text = " ".join(p.get_text(" ", strip=True) for p in ps) or node.get_text(" ", strip=True)
        if not text:
            text = soup.get_text(" ", strip=True)
        text = re.sub(r"\s+", " ", text).strip()

    return title, text

# --- 35ì ì´ë‚´ ë¶ˆë¦¿ ì••ì¶• (ë°±ì—…ìš©) ---
def _short_bullets(text: str, max_lines: int = 4, max_chars: int = 35):
    if not text: return []
    parts = re.split(r'[â€¢\-\u2022\*\n\r\.!?]+', text)
    out = []
    for p in parts:
        s = re.sub(r"\s+", " ", p).strip()
        if not s: continue
        if len(s) > max_chars: s = s[:max_chars-1] + "â€¦"
        out.append(s)
        if len(out) >= max_lines: break
    return out

# --- í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì „ì²˜ë¦¬) ---
def _build_evidence_pack(full_text: str, max_sentences: int = 20) -> str:
    """
    ê¸°ì‚¬ ì „ë¬¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ ì…ë ¥ í† í° ìµœì†Œí™” + ì‘ë‹µì†ë„ í–¥ìƒ
    """
    if not full_text:
        return ""

    # ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
    sentences = re.split(r'[.!?]+', full_text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

    if not sentences:
        return full_text[:4000]

    # í•µì‹¬ í‚¤ì›Œë“œ (í¬ìŠ¤ì½” + ë¹„ì¦ˆë‹ˆìŠ¤ í•µì‹¬ì–´)
    core_keywords = [
        "í¬ìŠ¤ì½”", "POSCO", "í¬ìŠ¤ì½”ì¸í„°", "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ", "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°",
        "ê³µì¥", "ìˆ˜ì£¼", "ì–‘ì‚°", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ì‹¤ì ",
        "ì „ë§", "íˆ¬ì", "ì§€ë¶„", "í˜‘íšŒ", "ê³µê¸‰ë§", "ê³„ì•½", "MOU",
        "ìƒì‚°", "íŒë§¤", "ìˆ˜ì¶œ", "ì°©ê³µ", "ì¤€ê³µ", "ê°€ë™", "ì¦ì„¤"
    ]

    # ìˆ«ì íŒ¨í„´ (ì–µ, ì¡°, %, í†¤ ë“±)
    number_pattern = r'\d+[.,]?\d*\s*(?:ì–µ|ì¡°|ë§Œ|ì²œ|%|í†¤|MW|GW|ê±´|ê°œ|ëª…|ë‹¬ëŸ¬|\$|ì›)'

    selected = []

    # 1) ì²« ë¬¸ì¥ (ë¦¬ë“œ ë¬¸ì¥)
    if sentences:
        selected.append(sentences[0])

    # 2) ë§ˆì§€ë§‰ ë¬¸ì¥ (ê²°ë¡ /í–¥í›„ê³„íš)
    if len(sentences) > 1:
        selected.append(sentences[-1])

    # 3) í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥
    for sent in sentences[1:-1]:
        if any(kw in sent for kw in core_keywords):
            selected.append(sent)

    # 4) ìˆ«ì í¬í•¨ ë¬¸ì¥ (ì¤‘ë³µ ì œê±°)
    for sent in sentences:
        if re.search(number_pattern, sent) and sent not in selected:
            selected.append(sent)

    # 5) ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
    seen = set()
    final = []
    for s in selected:
        if s not in seen and len(s) > 15:
            seen.add(s)
            final.append(s)
            if len(final) >= max_sentences:
                break

    # ê²°ê³¼ ì¡°í•© (4000ì ì œí•œ)
    result = ". ".join(final)
    return result[:4000]

# --- ë§í¬ë¥¼ ì§ì ‘ ì½ì–´ GPTë¡œ 'ì¹´í†¡ ë³´ê³  ë©”ì‹œì§€' ìƒì„± (ê°œì„  ë²„ì „) ---
def make_kakao_report_from_url(url: str, fallback_media="", fallback_title="", fallback_summary=""):
    media = fallback_media or _publisher_from_link(url)
    title, body = _extract_article_text_and_title(url)
    title = title or fallback_title or "ì œëª© ë¯¸í™•ì¸"

    # âœ… í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ (ì „ì²˜ë¦¬ - ì†ë„ í–¥ìƒ)
    evidence = _build_evidence_pack(body or fallback_summary or "", max_sentences=20)

    # âœ… ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶œë ¥ ê°•í™”)
    sys_prompt = """ë„ˆëŠ” í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ í™ë³´ê·¸ë£¹ ì „ìš© 'ë‰´ìŠ¤ ë³´ê³  ë©”ì‹œì§€ ìƒì„± ë´‡'ì´ì•¼.
ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‰´ìŠ¤ ë§í¬ì˜ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ ë³´ê³  ë©”ì‹œì§€ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ëª©ì ì´ì•¼.

[ë³´ê³  ë©”ì‹œì§€ ì‘ì„± ê·œì¹™]
1. **í˜•ì‹**:
   - ê¸°ì‚¬ë§í¬ ì œê³µ
   - í•œì¤„ ë ìš°ê³ (enter)
   - "ë§¤ì²´ëª… : ê¸°ì‚¬ ì œëª©"
   - í•˜ë‹¨ì— í•µì‹¬ ìš”ì•½ 4~6ì¤„ (ê° ë¬¸ì¥ì€ 50~100ì ì´ë‚´, ë‹¨ë¬¸ ì¤‘ì‹¬)
2. **í†¤ì•¤ë§¤ë„ˆ**:
   - ë¹„ì¦ˆë‹ˆìŠ¤ í†¤
   - ê°„ê²°, ëª…í™•, ë¹ ë¥¸ ì •ë³´ ì „ë‹¬
   - ì¹´ì¹´ì˜¤í†¡ ë³´ê³ ìš©ìœ¼ë¡œ í•œëˆˆì— ë³´ì´ë„ë¡ êµ¬ì„±
3. **í¬í•¨ ìš”ì†Œ**:
   - ê¸°ì‚¬ ì¶œì²˜(ë§¤ì²´ëª…)
   - ê¸°ì‚¬ í•µì‹¬ ì£¼ì œ(ì œëª© ë³€í˜•í•˜ì§€ ë§ ê²ƒ)
   - ì£¼ìš” ë‚´ìš©ì€ ê° ë¬¸ë‹¨ë³„ë¡œ ìš”ì•½ì„ í•˜ê³ , ì´ë¥¼ í¬ê´„í•˜ëŠ” ì£¼ìš” ë‚´ìš© 4~7ë¸”ë¦¿ìœ¼ë¡œ ì •ë¦¬í•  ê²ƒ
   - í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ë° í¬ìŠ¤ì½”ê·¸ë£¹ ê´€ë ¨ ë‚´ìš© ê°•ì¡°
4. **ì œì™¸ ìš”ì†Œ**:
   - ë¶ˆí•„ìš”í•œ ê¸°ìëª…, ì‘ì„±ì¼ì‹œ ë“± ë³¸ë¬¸ ì™¸ ì •ë³´


ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
https://n.news.naver.com/article/215/0001235269?sid=101

ì¶œë ¥ ì˜ˆì‹œ:
https://n.news.naver.com/article/215/0001235269?sid=101

í•œêµ­ê²½ì œ TV : í¬ìŠ¤ì½”ì¸í„°, ì•Œë˜ìŠ¤ì¹´ ê°€ìŠ¤ê´€ ëš«ëŠ”ë‹¤â€¦"ë‚´ë…„ 1ë¶„ê¸° ë³¸ê³„ì•½"

- ì•Œë˜ìŠ¤ì¹´ ì•¡í™”ì²œì—°ê°€ìŠ¤(LNG) ê³µê¸‰ê³„ì•½ì„ ì´ë¥´ë©´ ë‚´ë…„ 1ë¶„ê¸° ì²´ê²°í•  ê²ƒìœ¼ë¡œ ì „ë§

- íŠ¸ëŸ¼í”„ í–‰ì •ë¶€ì˜ ì—­ì  ì‚¬ì—…ì— ìµœì´ˆë¡œ ì°¸ì—¬í•˜ëŠ” í•œêµ­ ê¸°ì—…ì¼ ë¿ë§Œ ì•„ë‹ˆë¼, ê¸€ë¡œë²Œ ê²½ìŸêµ­ ê¸°ì—…ë“¤ ì¤‘ì—ì„œë„ ê°€ì¥ ë¨¼ì € ë³¸ê³„ì•½ì„ ì²´ê²°í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ì–¸ê¸‰

- ë§ˆì´í¬ ë˜ë¦¬ë¹„ ë¯¸ ì•Œë˜ìŠ¤ì¹´ ì£¼ì§€ì‚¬ëŠ” ì•Œë˜ìŠ¤ì¹´ LNG í”„ë¡œì íŠ¸ ì°¸ì—¬ ê¸°ì—…ì— ì„¸ê¸ˆ ê°ë©´ í˜œíƒì„ ì£¼ëŠ” ë²•ì•ˆì„ ë°œì˜ í–ˆë‹¤ê³  ì™¸ì‹  ë³´ë„ ì¸ìš© ì–¸ê¸‰"""

    user_prompt = f"""ë§í¬: {url}
ë§¤ì²´ëª…: {media}
ì œëª©: {title}

[í•µì‹¬ ë‚´ìš©]
{evidence}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¹´ì¹´ì˜¤í†¡ ë³´ê³  ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜."""

    out, err = _openai_chat(
        [{"role":"system","content":sys_prompt},{"role":"user","content":user_prompt}],
        temperature=0.0,  # ì •í™•ì„± ìµœëŒ€í™”
        max_tokens=950
    )
    if out:
        return out

    # ì‹¤íŒ¨ ì‹œ ë°±ì—… í¬ë§· (ê¸°ì¡´ ìœ ì§€)
    bullets = _short_bullets(evidence or fallback_summary, 4, 35)
    if len(bullets) < 3:
        bullets = [
            "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê´€ë ¨ ì£¼ìš” ë‚´ìš© í™•ì¸ í•„ìš”",
            "ìƒì„¸ ì •ë³´ëŠ” ì›ë¬¸ ê¸°ì‚¬ ì°¸ê³ ",
            "ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„ í›„ ì¶”ê°€ ë³´ê³ "
        ]

    # ë°±ì—… ë³´ê³ ì„œë„ ìƒˆ í˜•ì‹ì— ë§ì¶¤ (URL-ì œëª© ì§ê²°, ë¸”ë¦¿ ì‚¬ì´ ë¹ˆ ì¤„, ë§ˆì¹¨í‘œ)
    bullets_formatted = '\n\n'.join(f"- {b}." for b in bullets[:4])
    backup_report = f"""{url}
{media} : {title}
{bullets_formatted}"""
    return backup_report

# --- ì¹´ìš´íŠ¸ë‹¤ìš´ ì „ìš© í”„ë˜ê·¸ë¨¼íŠ¸(ì§€ì› ì‹œ) + í´ë°± ---
def _countdown_badge_html(secs_left: int) -> str:
    return f"""
    <div style="
        padding:8px 12px; background:rgba(212,175,55,0.28);
        border-radius:10px; text-align:center;
        color:#F6D47A; font-weight:800; font-size:1.15rem;">
        {secs_left}
    </div>"""

if SUPPORTS_FRAGMENT:
    @st.fragment
    def countdown_fragment(refresh_interval: int):
        now = time.time()
        secs_left = max(0, int(st.session_state.next_refresh_at - now))
        st.markdown(_countdown_badge_html(secs_left), unsafe_allow_html=True)
        # 5ì´ˆ ê°„ê²©ìœ¼ë¡œ ë¦¬í”„ë ˆì‹œ (ì„±ëŠ¥ ìµœì í™”)
        st_autorefresh(interval=5000, key="countdown_tick")
        # 0ì´ˆ ë˜ë©´ íŠ¸ë¦¬ê±° í”Œë˜ê·¸ ì„¤ì •í•˜ê³  ì¦‰ì‹œ í˜ì´ì§€ ë¦¬ëŸ°
        if secs_left == 0 and not st.session_state.get("trigger_news_update", False):
            st.session_state.trigger_news_update = True
            st.rerun()
else:
    # êµ¬ë²„ì „ í´ë°±: ë³´ê³ ì„œ ìƒì„± ì¤‘ì´ë©´ ì¹´ìš´íŠ¸ë‹¤ìš´ ì •ì§€
    def countdown_fragment(refresh_interval: int):
        now = time.time()
        secs_left = max(0, int(st.session_state.next_refresh_at - now))
        st.markdown(_countdown_badge_html(secs_left), unsafe_allow_html=True)

        # ë³´ê³ ì„œ ìƒì„± ì¤‘ì´ ì•„ë‹ ë•Œë§Œ ìë™ ë¦¬í”„ë ˆì‹œ (5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”)
        if not any(key.startswith("report_generating_") and st.session_state.get(key, False) for key in st.session_state.keys()):
            st_autorefresh(interval=5000, key="countdown_fallback")

        # 0ì´ˆ ë˜ë©´ íŠ¸ë¦¬ê±° í”Œë˜ê·¸ ì„¤ì •í•˜ê³  ì¦‰ì‹œ í˜ì´ì§€ ë¦¬ëŸ°
        if secs_left == 0 and not st.session_state.get("trigger_news_update", False):
            st.session_state.trigger_news_update = True
            st.rerun()

def fetch_naver_news(query: str, start: int = 1, display: int = 50, sort: str = "date"):
    """ë‰´ìŠ¤ API í˜¸ì¶œ (ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€)"""
    r = None
    try:
        url = "https://openapi.naver.com/v1/search/news.json"
        params = {"query": query, "start": start, "display": display, "sort": sort}
        headers = _naver_headers()

        print(f"[DEBUG] API Request - Query: {query}, Params: {params}")
        print(f"[DEBUG] Headers present: ID={bool(headers.get('X-Naver-Client-Id'))}, Secret={bool(headers.get('X-Naver-Client-Secret'))}")

        if not headers.get("X-Naver-Client-Id") or not headers.get("X-Naver-Client-Secret"):
            print("[DEBUG] Missing API keys, returning empty result")
            return {"items": [], "error": "missing_keys"}

        print(f"[DEBUG] Starting API request...")
        r = requests.get(url, headers=headers, params=params, timeout=5)
        print(f"[DEBUG] API Response status: {r.status_code}")

        # 429 ì—ëŸ¬ (í• ë‹¹ëŸ‰ ì´ˆê³¼) ëª…ì‹œì  ì²˜ë¦¬
        if r.status_code == 429:
            error_data = r.json() if r.text else {}
            error_msg = error_data.get("errorMessage", "API quota exceeded")
            print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ (429): {error_msg}")
            return {"items": [], "error": "quota_exceeded", "error_message": error_msg}

        r.raise_for_status()
        result = r.json()
        print(f"[DEBUG] API Response items count: {len(result.get('items', []))}")
        return result

    except requests.exceptions.Timeout:
        print(f"[WARNING] Naver API timeout for query: {query}")
        return {"items": [], "error": "timeout"}
    except requests.exceptions.RequestException as e:
        print(f"[WARNING] Naver API request failed for query: {query}, error: {e}")
        if hasattr(e, 'response') and e.response is not None:
            status_code = e.response.status_code
            print(f"[WARNING] Response status: {status_code}, body: {e.response.text[:200]}")
            if status_code == 429:
                return {"items": [], "error": "quota_exceeded"}
        return {"items": [], "error": "request_failed"}
    except Exception as e:
        print(f"[WARNING] Unexpected error in fetch_naver_news: {e}")
        return {"items": [], "error": "unexpected"}
    finally:
        # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€: ì‘ë‹µ ê°ì²´ ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°
        if r is not None:
            r.close()

def crawl_naver_news(query: str, max_items: int = 200, sort: str = "date") -> pd.DataFrame:
    print(f"[DEBUG] Starting crawl_naver_news for query: {query}, max_items: {max_items}")
    items, start, total = [], 1, 0
    display = min(50, max_items)  # í•œ ë²ˆì— ìµœëŒ€ 50ê°œë¡œ ì œí•œ
    max_attempts = 2  # ìµœëŒ€ 2ë²ˆ ì‹œë„ë¡œ ì œí•œí•˜ì—¬ ë¹ ë¥¸ ì‹¤íŒ¨
    attempt_count = 0
    quota_exceeded = False

    while total < max_items and start <= 100 and attempt_count < max_attempts:  # ì‹œì‘ ìœ„ì¹˜ë„ 100ìœ¼ë¡œ ì œí•œ
        attempt_count += 1
        print(f"[DEBUG] Attempt {attempt_count} for query: {query}")

        try:
            data = fetch_naver_news(query, start=start, display=min(display, max_items - total), sort=sort)

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²´í¬
            if data.get("error") == "quota_exceeded":
                print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            arr = data.get("items", [])

            if not arr:
                print(f"[DEBUG] No items returned for query: {query}, attempt: {attempt_count}")
                break
                
            print(f"[DEBUG] Got {len(arr)} items for query: {query}")
            
            for it in arr:
                title = _clean_text(it.get("title"))
                desc = _clean_text(it.get("description"))
                link = it.get("originallink") or it.get("link") or ""
                pub = it.get("pubDate", "")
                try:
                    # âœ… GMT â†’ KST ë³€í™˜ í›„ tz ì œê±°
                    dt = pd.to_datetime(pub, utc=True).tz_convert("Asia/Seoul").tz_localize(None)
                    date_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    date_str = ""
                items.append({"ë‚ ì§œ": date_str, "ë§¤ì²´ëª…": _publisher_from_link(link),
                              "ê²€ìƒ‰í‚¤ì›Œë“œ": query, "ê¸°ì‚¬ì œëª©": title, "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½": desc, "URL": link, "sentiment": "pos"})
            
            got = len(arr)
            total += got
            if got == 0:
                break
            start += got
            
        except Exception as e:
            print(f"[WARNING] Error in crawl_naver_news attempt {attempt_count}: {e}")
            break
    
    print(f"[DEBUG] crawl_naver_news completed for {query}: {len(items)} items")
    df = pd.DataFrame(items, columns=["ë‚ ì§œ", "ë§¤ì²´ëª…", "ê²€ìƒ‰í‚¤ì›Œë“œ", "ê¸°ì‚¬ì œëª©", "ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", "URL", "sentiment"])

    # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì •ë³´ë¥¼ DataFrame ì†ì„±ìœ¼ë¡œ ì €ì¥
    if quota_exceeded:
        df.attrs['quota_exceeded'] = True
        print(f"[ERROR] API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")

    if not df.empty:
        # ìµœì‹ ìˆœ ì •ë ¬ ë¨¼ì € ìˆ˜í–‰
        df["ë‚ ì§œ_datetime"] = pd.to_datetime(df["ë‚ ì§œ"], errors="coerce")
        df = df.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
        df = df.drop("ë‚ ì§œ_datetime", axis=1)

        # ì¤‘ë³µ ì œê±° (URL ìš°ì„ , ì—†ìœ¼ë©´ ì œëª©+ë‚ ì§œ)
        key = df["URL"].where(df["URL"].astype(bool), df["ê¸°ì‚¬ì œëª©"] + "|" + df["ë‚ ì§œ"])
        df = df.loc[~key.duplicated()].reset_index(drop=True)
    return df

def load_news_db(force_refresh: bool = False) -> pd.DataFrame:
    """ë‰´ìŠ¤ DB ë¡œë“œ (GitHub ì§ì ‘ ë¡œë“œ - Streamlit Cloud ìºì‹œ ìš°íšŒ)

    Args:
        force_refresh: Trueë©´ ì¦‰ì‹œ ìµœì‹  ë°ì´í„° ë¡œë“œ (ìºì‹œ ë¬´ì‹œ)
    """
    # GitHub raw URLì—ì„œ ì§ì ‘ ë¡œë“œ (Streamlit Cloud ìºì‹œ ë¬¸ì œ í•´ê²°)
    GITHUB_RAW_URL = "https://raw.githubusercontent.com/kimwoss/Risk_management/main/data/news_monitor.csv"

    try:
        # 1ì°¨ ì‹œë„: GitHubì—ì„œ ì§ì ‘ ë¡œë“œ (ìºì‹œ ìš°íšŒ)
        # ìºì‹œ ë²„ìŠ¤íŒ…ì„ ìœ„í•´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        import time
        if force_refresh:
            # ê°•ì œ ìƒˆë¡œê³ ì¹¨: ì´ˆ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ (ì¦‰ì‹œ ìµœì‹  ë°ì´í„°)
            cache_buster = int(time.time())
        else:
            # ì¼ë°˜ ë¡œë“œ: 30ì´ˆ ë‹¨ìœ„ë¡œ ê°±ì‹  (ë¹ ë¥¸ ì—…ë°ì´íŠ¸)
            cache_buster = int(time.time() // 30)
        url_with_cache_buster = f"{GITHUB_RAW_URL}?t={cache_buster}"

        print(f"[DEBUG] GitHubì—ì„œ ì§ì ‘ ë¡œë“œ ì‹œë„: {url_with_cache_buster}")
        response = requests.get(url_with_cache_buster, timeout=10)
        response.raise_for_status()

        from io import StringIO
        df = pd.read_csv(StringIO(response.text), encoding="utf-8")
        response.close()

        # sentiment ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if "sentiment" not in df.columns:
            df["sentiment"] = "pos"

        # ë””ë²„ê·¸: ìµœì‹  ê¸°ì‚¬ ì‹œê°„ ì¶œë ¥
        if not df.empty and "ë‚ ì§œ" in df.columns:
            latest_date = df["ë‚ ì§œ"].iloc[0] if len(df) > 0 else "N/A"
            print(f"[DEBUG] âœ… GitHubì—ì„œ ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´, ìµœì‹  ê¸°ì‚¬: {latest_date}")
        return df

    except Exception as e:
        print(f"[WARNING] GitHub ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ íŒŒì¼ ì‹œë„: {e}")

        # 2ì°¨ ì‹œë„: ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ
        try:
            df = pd.read_csv(NEWS_DB_FILE, encoding="utf-8")
            # sentiment ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if "sentiment" not in df.columns:
                df["sentiment"] = "pos"
            if not df.empty and "ë‚ ì§œ" in df.columns:
                latest_date = df["ë‚ ì§œ"].iloc[0] if len(df) > 0 else "N/A"
                print(f"[DEBUG] âš ï¸ ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ: {len(df)}ê±´, ìµœì‹  ê¸°ì‚¬: {latest_date}")
            return df
        except Exception as e2:
            print(f"[ERROR] ëª¨ë“  ë¡œë“œ ì‹œë„ ì‹¤íŒ¨: {e2}")
            return pd.DataFrame(columns=["ë‚ ì§œ","ë§¤ì²´ëª…","ê²€ìƒ‰í‚¤ì›Œë“œ","ê¸°ì‚¬ì œëª©","ì£¼ìš”ê¸°ì‚¬ ìš”ì•½","URL","sentiment"])

def save_news_db(df: pd.DataFrame):
    if df.empty:
        print("[DEBUG] save_news_db skipped: empty dataframe")
        return
    # ë§¤ì²´ëª… ì •ë¦¬ (URL ê¸°ë°˜)
    if "ë§¤ì²´ëª…" in df.columns and "URL" in df.columns:
        for idx, row in df.iterrows():
            if pd.notna(row["URL"]):
                df.at[idx, "ë§¤ì²´ëª…"] = _publisher_from_link(row["URL"])

    # ë‚ ì§œ ì»¬ëŸ¼ì´ ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì •ë ¬ ìƒëµ
    # ìƒìœ„ 200ê°œ ì €ì¥ (50ê°œì—ì„œ ì¦ê°€ - ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€)
    out = df.head(200)
    out.to_csv(NEWS_DB_FILE, index=False, encoding="utf-8")
    safe_print("[DEBUG] news saved:", len(out), "rows ->", NEWS_DB_FILE)

# ----------------------------- ê³µìš© UI ìœ í‹¸ -----------------------------
def show_table(df: pd.DataFrame, label: str):
    st.markdown(f"#### {label}")
    st.dataframe(df, use_container_width=True, height=min(560, 44 + min(len(df), 12) * 38))

_PHONE_PATTERNS = [r'(?:0?1[016789])[ .-]?\d{3,4}[ .-]?\d{4}']
EMAIL_RE = re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+')

def _extract_phone(text: str) -> str:
    s = text or ""
    for p in _PHONE_PATTERNS:
        m = re.search(p, s)
        if m:
            num = re.sub(r"\D", "", m.group(0))
            if len(num) == 11 and num.startswith("010"):
                return f"010-{num[3:7]}-{num[7:]}"
            if len(num) == 10 and num.startswith("10"):
                return f"010-{num[2:6]}-{num[6:]}"
            return m.group(0)
    return ""

def parse_reporters_to_df(reporters) -> pd.DataFrame:
    if not reporters:
        return pd.DataFrame(columns=["ì´ë¦„","ì§ì±…","ì—°ë½ì²˜","ì´ë©”ì¼","ì†Œì†/íŒ€","ë¹„ê³ "])
    rows = []
    for item in reporters:
        if isinstance(item, dict):
            rows.append([
                item.get("ì´ë¦„") or item.get("name") or item.get("ê¸°ì") or "",
                item.get("ì§ì±…") or item.get("ì§ê¸‰") or item.get("role") or "",
                item.get("ì—°ë½ì²˜") or item.get("mobile") or item.get("phone") or "",
                item.get("ì´ë©”ì¼") or item.get("email") or "",
                item.get("íŒ€") or item.get("ì†Œì†") or "",
                item.get("ë¹„ê³ ") or item.get("note") or ""
            ])
            continue
        s = " ".join([str(x).strip() for x in item]) if isinstance(item, (list, tuple)) else str(item).strip()
        email_match = EMAIL_RE.search(s)
        email = email_match.group(0) if email_match else ""
        phone = _extract_phone(s)
        parts = [p.strip() for p in re.split(r"[Â·\|,\t]+", s) if p.strip()]
        name = parts[0] if parts else s
        team = ""
        m = re.search(r"(.+?)\s*\((.+?)\)", name)
        if m:
            name, team = m.group(1).strip(), m.group(2).strip()
        role = ""
        for p in parts[1:]:
            if any(k in p for k in ["íŒ€ì¥","ì°¨ì¥","ê¸°ì","ë¶€ì¥","ì—ë””í„°","ë°ìŠ¤í¬","CFO","êµ­ì¥"]):
                role = p; break
        rows.append([name, role, phone, email, team, ""])
    df = pd.DataFrame(rows, columns=["ì´ë¦„","ì§ì±…","ì—°ë½ì²˜","ì´ë©”ì¼","ì†Œì†/íŒ€","ë¹„ê³ "]).fillna("")
    if not df.empty:
        df = df.drop_duplicates(subset=["ì´ë¦„","ì—°ë½ì²˜","ì´ë©”ì¼"], keep="first").reset_index(drop=True)
    return df

def _to_people_df(lines, tag: str) -> pd.DataFrame:
    if not lines:
        return pd.DataFrame(columns=["êµ¬ë¶„","ì´ë¦„","ì§ì±…","ì—°ë½ì²˜","ì´ë©”ì¼","ì†Œì†/íŒ€","ë¹„ê³ "])
    df = parse_reporters_to_df(lines)
    if not df.empty and "ì´ë¦„" in df.columns:
        df = df[~df["ì´ë¦„"].str.fullmatch(r"\s*<.*>\s*", na=False)]
    df.insert(0, "êµ¬ë¶„", tag)
    return df

# ----------------------------- í…”ë ˆê·¸ë¨ ì•Œë¦¼ -----------------------------
def send_telegram_notification(new_articles: list):
    """
    ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ë©´ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡ (ê¸°ì‚¬ë³„ ê°œë³„ ë©”ì‹œì§€)

    - ëª¨ë“  ì‹ ê·œ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ (ê°œìˆ˜ ì œí•œ ì—†ìŒ)
    - ê° ê¸°ì‚¬ë§ˆë‹¤ ì¬ì‹œë„ ë¡œì§ í¬í•¨
    - í…”ë ˆê·¸ë¨ API Rate Limit ì¤€ìˆ˜ (ì´ˆë‹¹ ì•½ 28ê°œ)
    - ì „ì†¡ ì„±ê³µí•œ ê¸°ì‚¬ë§Œ ìºì‹œì— ì¶”ê°€í•˜ì—¬ ì¬ì „ì†¡ ë°©ì§€

    Args:
        new_articles: ìƒˆë¡œìš´ ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"title": ..., "link": ..., "date": ...}, ...]
    """
    global _sent_articles_cache

    try:
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        print(f"[DEBUG] í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œë„ - ê¸°ì‚¬ ìˆ˜: {len(new_articles) if new_articles else 0}")
        print(f"[DEBUG] ë´‡ í† í° ì¡´ì¬: {bool(bot_token)}, Chat ID ì¡´ì¬: {bool(chat_id)}")

        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì•Œë¦¼ ìŠ¤í‚µ
        if not bot_token or not chat_id:
            print("[DEBUG] âš ï¸ í…”ë ˆê·¸ë¨ ì„¤ì • ì—†ìŒ - ì•Œë¦¼ ìŠ¤í‚µ")
            print("[DEBUG] ğŸ’¡ Streamlit Cloud â†’ Settings â†’ Secretsì—ì„œ TELEGRAM_BOT_TOKENê³¼ TELEGRAM_CHAT_ID ì„¤ì • í•„ìš”")
            return

        if not new_articles:
            print("[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ - ì•Œë¦¼ ìŠ¤í‚µ")
            return

        # ì´ë¯¸ ì „ì†¡ëœ ê¸°ì‚¬ í•„í„°ë§
        with _sent_articles_lock:
            articles_to_send = []
            for article in new_articles:
                url_key = article.get("link", "")
                if url_key and url_key not in _sent_articles_cache:
                    articles_to_send.append(article)

            if not articles_to_send:
                print("[DEBUG] ëª¨ë“  ê¸°ì‚¬ê°€ ì´ë¯¸ ì „ì†¡ë¨ - ì•Œë¦¼ ìŠ¤í‚µ")
                return

            print(f"[DEBUG] ì „ì†¡ ëŒ€ìƒ: {len(articles_to_send)}ê±´ (ì¤‘ë³µ ì œì™¸: {len(new_articles) - len(articles_to_send)}ê±´)")

        # ëª¨ë“  ì‹ ê·œ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡ (ì œí•œ ì—†ìŒ)
        articles_to_notify = articles_to_send
        print(f"[DEBUG] ğŸ“¤ ì´ {len(articles_to_notify)}ê±´ì˜ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.")

        # í…”ë ˆê·¸ë¨ API URL
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

        # ê° ê¸°ì‚¬ë§ˆë‹¤ ê°œë³„ ë©”ì‹œì§€ ì „ì†¡
        success_count = 0
        for article in articles_to_notify:
            title = article.get("title", "ì œëª© ì—†ìŒ")
            link = article.get("link", "")
            date = article.get("date", "")
            press = article.get("press", "")
            keyword = article.get("keyword", "")

            # ë‹¨ë¬¸ ë©”ì‹œì§€ êµ¬ì„±
            message = f"ğŸš¨ *ìƒˆ ë‰´ìŠ¤*\n\n"

            # ê²€ìƒ‰ í‚¤ì›Œë“œ í•´ì‹œíƒœê·¸ ì¶”ê°€
            if keyword:
                # ê³µë°±ì„ ì œê±°í•˜ì—¬ í•´ì‹œíƒœê·¸ë¡œ ë³€í™˜
                hashtag = keyword.replace(" ", "")
                message += f"#{hashtag}\n"

            # ì œëª© ì•ì— [ì–¸ë¡ ì‚¬] ì¶”ê°€
            if press:
                message += f"*[{press}]* {title}\n"
            else:
                message += f"*{title}*\n"

            # ë‚ ì§œì™€ ë§í¬
            if date:
                message += f"ğŸ• {date}\n"
            if link:
                message += f"ğŸ”— {link}"

            payload = {
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": True
            }

            response = None
            # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ)
            max_retries = 3
            retry_delay = 1  # ì´ˆ

            for attempt in range(max_retries):
                try:
                    response = requests.post(url, json=payload, timeout=10)
                    if response.status_code == 200:
                        success_count += 1
                        print(f"[DEBUG] âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {title[:30]}...")

                        # ì „ì†¡ ì„±ê³µí•œ ê¸°ì‚¬ëŠ” ìºì‹œì— ì¶”ê°€
                        with _sent_articles_lock:
                            _sent_articles_cache.add(link)
                            # ìºì‹œ í¬ê¸° ì œí•œ
                            if len(_sent_articles_cache) > _MAX_SENT_CACHE:
                                # ì˜¤ë˜ëœ í•­ëª© ì œê±° (setì´ë¯€ë¡œ ì„ì˜ ì œê±°)
                                _sent_articles_cache.pop()
                        break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                    else:
                        print(f"[DEBUG] âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {response.status_code}")
                        if attempt < max_retries - 1:
                            import time
                            time.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„

                except Exception as e:
                    print(f"[DEBUG] âŒ ê°œë³„ ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    else:
                        # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•˜ë©´ ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥
                        import traceback
                        print(f"[DEBUG] ìµœì¢… ì‹¤íŒ¨ - ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                finally:
                    # ì—°ê²° ëˆ„ìˆ˜ ë°©ì§€
                    if response is not None:
                        response.close()

            # Rate Limit ë°©ì§€ (í…”ë ˆê·¸ë¨ API: ì´ˆë‹¹ 30ê°œ ì œí•œ)
            # 35ms ëŒ€ê¸° = ì´ˆë‹¹ ì•½ 28ê°œë¡œ ì•ˆì „í•œ ì†ë„ ìœ ì§€
            import time
            time.sleep(0.035)

        # ì „ì†¡ ê²°ê³¼ í†µê³„
        failed_count = len(articles_to_notify) - success_count
        if failed_count > 0:
            print(f"[DEBUG] âš ï¸ ì „ì†¡ ì‹¤íŒ¨: {failed_count}ê±´")
            print(f"[DEBUG] ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” ë‹¤ìŒ ìˆ˜ì§‘ ì‚¬ì´í´ì— ì¬ì‹œë„ë©ë‹ˆë‹¤.")

        print(f"[DEBUG] âœ… ì´ {success_count}/{len(articles_to_notify)}ê±´ ì „ì†¡ ì™„ë£Œ (ì„±ê³µë¥ : {success_count/len(articles_to_notify)*100:.1f}%)")
        print(f"[DEBUG] ì „ì†¡ ìºì‹œ í¬ê¸°: {len(_sent_articles_cache)}ê±´")

    except Exception as e:
        print(f"[DEBUG] âŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")

def detect_new_articles(old_df: pd.DataFrame, new_df: pd.DataFrame) -> list:
    """
    ê¸°ì¡´ DBì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€

    - URLì„ ìš°ì„  ì‹ë³„ìë¡œ ì‚¬ìš©
    - ìµœê·¼ 6ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ ì•Œë¦¼ ëŒ€ìƒ
    - ì •í™•í•œ ì¤‘ë³µ ì²´í¬

    Args:
        old_df: ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°
        new_df: ìƒˆë¡œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë°ì´í„°

    Returns:
        ì‹ ê·œ ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # ê¸°ì¡´ DBê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬ (ì²« ì‹¤í–‰ ìŠ¤íŒ¸ ë°©ì§€)
        if old_df.empty:
            print(f"[DEBUG] ê¸°ì¡´ DB ë¹„ì–´ìˆìŒ - ì²« ì‹¤í–‰ì´ë¯€ë¡œ ì•Œë¦¼ ìŠ¤í‚µ")
            return []

        if new_df.empty:
            return []

        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (KST)
        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST).replace(tzinfo=None)  # KST ì‹œê°„ì„ naive datetimeìœ¼ë¡œ

        # ê¸°ì¡´ DBì˜ URL ì„¸íŠ¸ ìƒì„± (ê°€ì¥ ì •í™•í•œ ì‹ë³„ì)
        old_urls = set()
        for _, row in old_df.iterrows():
            url = str(row.get("URL", "")).strip()
            if url and url != "nan" and url != "":
                old_urls.add(url)

        print(f"[DEBUG] ê¸°ì¡´ DB URL ìˆ˜: {len(old_urls)}")
        print(f"[DEBUG] ìˆ˜ì§‘ëœ ì‹ ê·œ ë°ì´í„° ìˆ˜: {len(new_df)}")

        # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
        new_articles = []
        for _, row in new_df.iterrows():
            url = str(row.get("URL", "")).strip()
            title = str(row.get("ê¸°ì‚¬ì œëª©", "")).strip()

            # URLì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
            if not url or url == "nan" or url == "":
                continue

            # URLì´ ê¸°ì¡´ DBì— ì—†ìœ¼ë©´ ì‹ ê·œ
            if url not in old_urls:
                # ë‚ ì§œ íŒŒì‹± ì‹œë„
                article_date_str = row.get("ë‚ ì§œ", "")
                try:
                    # ë‚ ì§œ í˜•ì‹: "YYYY-MM-DD HH:MM"
                    article_date = pd.to_datetime(article_date_str, errors="coerce")

                    # ë‚ ì§œê°€ ìœ íš¨í•˜ë©´ ìµœê·¼ 6ì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸
                    if pd.notna(article_date):
                        time_diff = now - article_date
                        hours_diff = time_diff.total_seconds() / 3600

                        # 6ì‹œê°„ ì´ë‚´ì˜ ê¸°ì‚¬ë§Œ ì•Œë¦¼
                        if hours_diff > 6:
                            print(f"[DEBUG] ì˜¤ë˜ëœ ê¸°ì‚¬ ìŠ¤í‚µ: {title[:30]}... ({hours_diff:.1f}ì‹œê°„ ì „)")
                            continue
                        else:
                            print(f"[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ê°ì§€: {title[:50]}... ({hours_diff:.1f}ì‹œê°„ ì „)")
                    else:
                        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ í¬í•¨ (ì•ˆì „ì¥ì¹˜)
                        print(f"[DEBUG] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ì•Œë¦¼ í¬í•¨): {title[:50]}...")

                except Exception as e:
                    print(f"[DEBUG] ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

                # URLì—ì„œ ë§¤ì²´ëª… ì¶”ì¶œ (Streamlitê³¼ ë™ì¼í•œ ë°©ì‹)
                press = _publisher_from_link(url)

                # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
                keyword = str(row.get("ê²€ìƒ‰í‚¤ì›Œë“œ", "")).strip()

                new_articles.append({
                    "title": title if title and title != "nan" else "ì œëª© ì—†ìŒ",
                    "link": url,
                    "date": article_date_str,
                    "press": press,
                    "keyword": keyword
                })

        print(f"[DEBUG] ì´ {len(new_articles)}ê±´ì˜ ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (ìµœê·¼ 6ì‹œê°„ ì´ë‚´)")
        return new_articles

    except Exception as e:
        print(f"[DEBUG] ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return []

# ----------------------------- ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ -----------------------------
def background_news_monitor():
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  í…”ë ˆê·¸ë¨ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” í•¨ìˆ˜
    ë¸Œë¼ìš°ì € ì—°ê²° ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ 3ë¶„ë§ˆë‹¤ ìë™ ì‹¤í–‰ë¨
    """
    try:
        print(f"[BACKGROUND] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # í‚¤ì›Œë“œ ì„¤ì •
        keywords = [
            "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
            "POSCO INTERNATIONAL",
            "í¬ìŠ¤ì½”ì¸í„°",
            "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
            "êµ¬ë™ëª¨í„°ì½”ì•„",
            "êµ¬ë™ëª¨í„°ì½”ì–´",
            "ë¯¸ì–€ë§ˆ LNG",
            "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜",
            "í¬ìŠ¤ì½”"
        ]
        exclude_keywords = ["í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„", "POSCO INTERNATIONAL", "í¬ìŠ¤ì½”ì¸í„°",
                           "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ", "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"]
        max_items = 30  # API ì‚¬ìš©ëŸ‰ ìµœì í™”

        # API í‚¤ ì²´í¬
        headers = _naver_headers()
        api_ok = bool(headers.get("X-Naver-Client-Id") and headers.get("X-Naver-Client-Secret"))

        if not api_ok:
            print("[BACKGROUND] API í‚¤ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # ê¸°ì¡´ DB ë¡œë“œ
        existing_db = load_news_db()

        # ë‰´ìŠ¤ ìˆ˜ì§‘
        all_news = []
        quota_exceeded = False

        for kw in keywords:
            df_kw = crawl_naver_news(kw, max_items=max_items // len(keywords), sort="date")

            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
            if df_kw.attrs.get('quota_exceeded', False):
                print(f"[BACKGROUND] âš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                quota_exceeded = True
                break

            if not df_kw.empty:
                # "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" ì •í™•í•œ ë§¤ì¹­ ê°•í™”
                if kw == "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„":
                    def should_include_posco_intl(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                        # ì •í™•íˆ "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„"ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
                        if "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" not in title and "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" not in description:
                            return False

                        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                        exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                        for exclude_word in exclude_words:
                            if exclude_word in title or exclude_word in description:
                                return False

                        return True

                    mask = df_kw.apply(should_include_posco_intl, axis=1)
                    df_kw = df_kw[mask].reset_index(drop=True)
                    if not df_kw.empty:
                        print(f"[BACKGROUND] 'í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„' ì •í™• ë§¤ì¹­ í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                # "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" ì •í™•í•œ ë§¤ì¹­ ê°•í™”
                elif kw == "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜":
                    def should_include_posco_mobility(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                        # ì •í™•íˆ "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
                        if "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" not in title and "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" not in description:
                            return False

                        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                        exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                        for exclude_word in exclude_words:
                            if exclude_word in title or exclude_word in description:
                                return False

                        return True

                    mask = df_kw.apply(should_include_posco_mobility, axis=1)
                    df_kw = df_kw[mask].reset_index(drop=True)
                    if not df_kw.empty:
                        print(f"[BACKGROUND] 'í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜' ì •í™• ë§¤ì¹­ í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                # "í¬ìŠ¤ì½”" í‚¤ì›Œë“œì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                elif kw == "í¬ìŠ¤ì½”":
                    def should_include_posco(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        title_lower = title.lower()
                        description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
                        content_lower = description.lower()

                        # ê¸°ì¡´ ì¡°ê±´: íƒ€ì´í‹€ì— "í¬ìŠ¤ì½”" í¬í•¨
                        title_has_posco = "í¬ìŠ¤ì½”" in title or "posco" in title_lower

                        # ìƒˆ ì¡°ê±´: íƒ€ì´í‹€ì— "[ë‹¨ë…]" í¬í•¨ AND ë‚´ìš©ì— "í¬ìŠ¤ì½”" í¬í•¨
                        is_exclusive_with_posco_in_content = "[ë‹¨ë…]" in title and "í¬ìŠ¤ì½”" in description

                        # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ í¬í•¨
                        if not (title_has_posco or is_exclusive_with_posco_in_content):
                            return False

                        for exclude_kw in exclude_keywords:
                            if exclude_kw.lower() in title_lower:
                                return False

                        exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                        for exclude_word in exclude_words:
                            if exclude_word in title or exclude_word in description:
                                return False

                        return True

                    mask_posco = df_kw.apply(should_include_posco, axis=1)
                    df_kw = df_kw[mask_posco].reset_index(drop=True)
                    if not df_kw.empty:
                        print(f"[BACKGROUND] 'í¬ìŠ¤ì½”' í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                else:
                    # ë‹¤ë¥¸ í‚¤ì›Œë“œëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì œëª©ì—ì„œë§Œ ë¶€ë™ì‚° ê´€ë ¨ í‚¤ì›Œë“œ ì œê±°
                    exclude_words = ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                    def should_include_general(row):
                        title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                        for exclude_word in exclude_words:
                            if exclude_word in title:
                                return False
                        return True

                    mask_general = df_kw.apply(should_include_general, axis=1)
                    df_kw = df_kw[mask_general].reset_index(drop=True)

                if not df_kw.empty:
                    all_news.append(df_kw)

        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
        if quota_exceeded:
            print(f"[BACKGROUND] âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
            print(f"[BACKGROUND] ğŸ’¡ í•´ê²° ë°©ë²•:")
            print(f"[BACKGROUND]    1. ìƒˆë¡œìš´ ë„¤ì´ë²„ ê°œë°œì ê³„ì •ìœ¼ë¡œ API í‚¤ ì¬ë°œê¸‰")
            print(f"[BACKGROUND]    2. ë§¤ì¼ ìì •(KST) ì´í›„ í• ë‹¹ëŸ‰ ì¬ì„¤ì •")
            print(f"[BACKGROUND]    3. ê¸°ì¡´ ì €ì¥ëœ ë‰´ìŠ¤ ë°ì´í„°ëŠ” ìœ ì§€ë©ë‹ˆë‹¤")
            return

        # í†µí•© ì •ë¦¬ & ì €ì¥
        df_new = pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()
        if not df_new.empty:
            df_new["ë‚ ì§œ_datetime"] = pd.to_datetime(df_new["ë‚ ì§œ"], errors="coerce")
            df_new = df_new.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
            df_new = df_new.drop("ë‚ ì§œ_datetime", axis=1)

            # ì¤‘ë³µ ì œê±°
            key = df_new["URL"].where(df_new["URL"].astype(bool), df_new["ê¸°ì‚¬ì œëª©"] + "|" + df_new["ë‚ ì§œ"])
            df_new = df_new.loc[~key.duplicated()].reset_index(drop=True)

            # ê¸°ì¡´ DBì™€ ë³‘í•©
            merged = pd.concat([df_new, existing_db], ignore_index=True) if not existing_db.empty else df_new
            merged = merged.drop_duplicates(subset=["URL", "ê¸°ì‚¬ì œëª©"], keep="first").reset_index(drop=True)
            if not merged.empty:
                merged["ë‚ ì§œ"] = pd.to_datetime(merged["ë‚ ì§œ"], errors="coerce")
                merged = merged.sort_values("ë‚ ì§œ", ascending=False, na_position="last").reset_index(drop=True)
                merged["ë‚ ì§œ"] = merged["ë‚ ì§œ"].dt.strftime("%Y-%m-%d %H:%M")

            # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€
            new_articles = detect_new_articles(existing_db, df_new)

            # DB ë¨¼ì € ì €ì¥ (race condition ë°©ì§€)
            save_news_db(merged)
            print(f"[BACKGROUND] âœ… DB ì €ì¥ ì™„ë£Œ: ì´ {len(merged)}ê±´")

            # ê¸°ì¡´ DBê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì•Œë¦¼ ì „ì†¡ (ì²« ì‹¤í–‰ ìŠ¤íŒ¸ ë°©ì§€)
            if new_articles and not existing_db.empty:
                print(f"[BACKGROUND] âœ… ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡")
                send_telegram_notification(new_articles)
            elif new_articles:
                print(f"[BACKGROUND] â­ï¸ ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ - ì²« ì‹¤í–‰ì´ë¯€ë¡œ ì•Œë¦¼ ìŠ¤í‚µ")

            print(f"[BACKGROUND] âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            print(f"[BACKGROUND] â„¹ï¸ ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"[BACKGROUND] âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"[BACKGROUND] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")

# ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì „ì—­ ë³€ìˆ˜
_scheduler = None
_scheduler_lock = threading.Lock()

# ì „ì†¡ëœ ê¸°ì‚¬ URL ì¶”ì  (ë©”ëª¨ë¦¬ ê¸°ë°˜, ìµœê·¼ 1000ê°œ)
_sent_articles_cache = set()
_sent_articles_lock = threading.Lock()
_MAX_SENT_CACHE = 1000

def start_background_scheduler():
    """
    ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•˜ëŠ” í•¨ìˆ˜
    ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë¨
    """
    global _scheduler

    # APSchedulerê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    if not SCHEDULER_AVAILABLE:
        print("[BACKGROUND] âš ï¸ APSchedulerê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("[BACKGROUND] ë¸Œë¼ìš°ì €ì—ì„œ 'ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§' ë©”ë‰´ë¥¼ ì—´ì–´ë‘ë©´ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ì´ ì‘ë™í•©ë‹ˆë‹¤.")
        return

    with _scheduler_lock:
        # ì´ë¯¸ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ìŠ¤í‚µ
        if _scheduler is not None and _scheduler.running:
            print("[BACKGROUND] ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return

        try:
            # BackgroundScheduler ìƒì„±
            _scheduler = BackgroundScheduler(timezone="Asia/Seoul")

            # 3ë¶„(180ì´ˆ)ë§ˆë‹¤ background_news_monitor ì‹¤í–‰
            _scheduler.add_job(
                background_news_monitor,
                'interval',
                seconds=180,
                id='news_monitor',
                replace_existing=True,
                max_instances=1  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
            )

            # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
            _scheduler.start()
            print(f"[BACKGROUND] âœ… ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ (3ë¶„ë§ˆë‹¤ ìë™ ì‹¤í–‰)")

            # ì•± ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ë„ ì¢…ë£Œ
            atexit.register(lambda: _scheduler.shutdown() if _scheduler else None)

        except Exception as e:
            print(f"[BACKGROUND] âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"[BACKGROUND] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")

# ----------------------------- ìŠ¤íƒ€ì¼ -----------------------------
# CSS ìºì‹œ ë¹„í™œì„±í™” - ì¦‰ì‹œ ë°˜ì˜
@st.cache_data(ttl=0)
def load_base_css():
    st.markdown("""
    <style>
      /* ì»¨í…Œì´ë„ˆ í­ + ìƒë‹¨ ì—¬ë°± (Streamlit ì •ì±… ì¤€ìˆ˜) */
      .block-container {max-width:1360px !important; padding: 24px 20px 0 !important; margin-top: 16px !important;}

      /* ë°°ê²½/í°íŠ¸ */
      @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Noto+Sans+KR:wght@400;600;700&display=swap');
      [data-testid="stAppViewContainer"]{
        background: linear-gradient(180deg,#0c0d10 0%, #0a0b0d 100%) !important;
        color:#eee; font-family:'Inter','Noto Sans KR',system-ui,Segoe UI,Roboto,Apple SD Gothic Neo,Pretendard,sans-serif;
      }
      [data-testid="stHeader"]{background:transparent; height:0;}
      section[data-testid="stSidebar"] {display:none !important;}

      /* ì¹´ë“œ */
      .card{
        background: linear-gradient(135deg, rgba(24,24,28,.65), rgba(16,16,20,.85));
        border: 1px solid rgba(255,255,255,.08);
        border-radius: 12px; padding: 24px; margin-bottom: 24px;
        box-shadow: 0 8px 32px rgba(0,0,0,.3), 0 1px 0 rgba(255,255,255,.05) inset;
        backdrop-filter: blur(10px);
      }
      .input-card{ border-color: rgba(212,175,55,.25); }
      .result-card{ border-color: rgba(189,189,189,.2); }

      /* ë²„íŠ¼ (ì œë„¤ì‹œìŠ¤ í†¤) */
      .stButton>button{
        border-radius:8px; font-weight:700; border:1px solid rgba(255,255,255,.18);
        background: linear-gradient(180deg, #2a2b2f, #1a1b1f); color:#fff;
        padding:10px 16px; letter-spacing:.01em;
      }
      /* ë¡œê³  ì˜ì—­ ìŠ¤íƒ€ì¼ */
      .nav-container a:hover {
        opacity: 0.9;
      }
      .stButton>button:hover{
        border-color: rgba(212,175,55,.6) !important;
        background: linear-gradient(135deg, rgba(32,34,40,.9), rgba(24,26,32,.95)) !important;
        box-shadow: 0 4px 20px rgba(212,175,55,.12), 0 2px 8px rgba(0,0,0,0.2) !important;
        transform: translateY(-1px) !important;
        color: #fff !important;
      }
      .stButton>button:disabled{
        color:#fff; border-color:#fff; background:linear-gradient(135deg, rgba(255,255,255,.12), rgba(255,255,255,.04));
      }

      /* ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ íŠ¹ë³„ ìŠ¤íƒ€ì¼ */
      .stDownloadButton>button{
        background: linear-gradient(135deg, #D4AF37, #B8941F) !important;
        border: 1px solid rgba(212,175,55,.4) !important;
        color: #000 !important;
        font-weight: 700 !important;
        border-radius: 8px !important;
        padding: 10px 16px !important;
        letter-spacing: 0.01em !important;
        transition: all 0.25s ease !important;
      }
      .stDownloadButton>button:hover{
        background: linear-gradient(135deg, #E6C55A, #D4AF37) !important;
        border-color: rgba(212,175,55,.8) !important;
        box-shadow: 0 4px 20px rgba(212,175,55,.25) !important;
        transform: translateY(-1px) !important;
        color: #000 !important;
      }

      /* ë°ì´í„°í”„ë ˆì„ */
      div[data-testid="stDataFrame"]{ background: rgba(255,255,255,.03) !important; border:1px solid rgba(255,255,255,.08); border-radius:10px; }
      div[data-testid="stDataFrame"] *{ color:#e7e7e7 !important; }

      /* í˜ì´ì§€ ì „í™˜ ì• ë‹ˆë©”ì´ì…˜ ê°œì„  */
      @keyframes stFadeSlide { from{opacity:0; transform:translateY(8px)} to{opacity:1; transform:none} }
      @keyframes stFadeIn { from{opacity:0} to{opacity:1} }
      [data-testid="stAppViewContainer"] .block-container{ 
        animation: stFadeSlide .25s ease-out; 
        will-change: transform, opacity;
      }
      
      /* ì…ë ¥ ìš”ì†Œ ì „í™˜ ìµœì í™” */
      .stTextInput, .stTextArea, .stSelectbox, .stButton {
        transition: all 0.2s ease;
        will-change: auto;
      }

      /* ë©”ë‰´ ì „í™˜ ì‹œ ë¶€ë“œëŸ¬ìš´ íš¨ê³¼ */
      .card {
        animation: stFadeIn 0.3s ease-out;
        will-change: opacity;
      }

      /* ëª¨ë°”ì¼ ìµœì í™” (ì „ì—­) */
      @media (max-width: 768px) {
        .card {
          padding: 16px !important;
          border-radius: 10px !important;
          margin-bottom: 16px !important;
        }
        .main-copy {
          left: 24px !important;
          padding-right: 24px !important;
        }
        .stButton>button {
          font-size: 0.9rem !important;
          padding: 10px 14px !important;
        }
        .stDownloadButton>button {
          font-size: 0.9rem !important;
          padding: 10px 14px !important;
        }
      }

      @media (max-width: 480px) {
        .card {
          padding: 12px !important;
          border-radius: 8px !important;
        }
        .main-copy {
          left: 16px !important;
          padding-right: 16px !important;
        }
        [data-testid="stAppViewContainer"] .block-container {
          padding-left: 1rem !important;
          padding-right: 1rem !important;
        }
      }
    </style>
    """, unsafe_allow_html=True)

# ----------------------------- ìì› ë¡œë“œ (ìºì‹±ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ìµœì†Œí™”) -----------------------------
@st.cache_data
def load_logo_data_uri():
    """ë¡œê³  ì´ë¯¸ì§€ ë¡œë“œ (ìºì‹œë¨)"""
    candidates = [
        os.path.join(DATA_FOLDER, "POSCO_INTERNATIONAL_Korean_Signature.svg"),
        os.path.join(DATA_FOLDER, "POSCO_INTERNATIONAL_Korean_Signature.png"),
        os.path.join(DATA_FOLDER, "logo.png"),
    ]
    for p in candidates:
        if os.path.exists(p):
            mt = mimetypes.guess_type(p)[0] or "image/png"
            with open(p, "rb") as f:
                b64 = base64.b64encode(f.read()).decode()
            return f"data:{mt};base64,{b64}"
    return ""

@st.cache_data
def load_main_background_uri():
    """ë©”ì¸ ë°°ê²½ ì´ë¯¸ì§€ ë¡œë“œ (ìºì‹œë¨)"""
    p = os.path.join(DATA_FOLDER, "Image_main.jpg")
    if os.path.exists(p):
        with open(p, "rb") as f:
            return f"data:image/jpeg;base64,{base64.b64encode(f.read()).decode()}"
    return ""

# ----------------------------- ë„¤ë¹„ê²Œì´ì…˜ -----------------------------
MENU_ITEMS = ["ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§", "ì´ìŠˆë°œìƒë³´ê³  ìƒì„±", "ì–¸ë¡ ì‚¬ ì •ë³´ ê²€ìƒ‰", "ë‹´ë‹¹ì ì •ë³´ ê²€ìƒ‰", "ê¸°ì¡´ëŒ€ì‘ì´ë ¥ ê²€ìƒ‰"]

def set_active_menu_from_url(default_label="ë©”ì¸"):
    try:
        raw = st.query_params.get("menu") or default_label
        label = urllib.parse.unquote(str(raw))
        st.session_state["top_menu"] = label
        return label
    except Exception:
        return st.session_state.get("top_menu", default_label)

def render_top_nav(active_label: str):
    logo_uri = load_logo_data_uri()
    st.markdown("""
    <style>
      /* ë„¤ë¹„ê²Œì´ì…˜ ì»¨í…Œì´ë„ˆ */
      .nav-container {
        background: linear-gradient(135deg, rgba(16,18,24,.4), rgba(12,14,20,.6));
        border: 1px solid rgba(255,255,255,.06);
        border-radius: 16px;
        padding: 16px 20px;
        margin: 8px 0 20px 0;
        box-shadow: 0 8px 32px rgba(0,0,0,.2);
        backdrop-filter: blur(12px);
      }

      /* ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ ì „ìš© ìŠ¤íƒ€ì¼ - ì œë„¤ì‹œìŠ¤ í†¤ */
      .nav-container .stButton>button {
        border-radius: 10px !important;
        font-weight: 600 !important;
        border: 1px solid rgba(255,255,255,.12) !important;
        background: linear-gradient(135deg, rgba(28,30,36,.8), rgba(20,22,28,.9)) !important;
        color: #e8e8e8 !important;
        padding: 12px 20px !important;
        letter-spacing: 0.02em !important;
        transition: all 0.25s cubic-bezier(0.4, 0, 0.2, 1) !important;
        height: 48px !important;
        min-height: 48px !important;
        font-size: 0.95rem !important;
        box-shadow: 0 2px 8px rgba(0,0,0,0.15) !important;
      }
      .nav-container .stButton>button:hover {
        border-color: rgba(212,175,55,.6) !important;
        background: linear-gradient(135deg, rgba(32,34,40,.9), rgba(24,26,32,.95)) !important;
        box-shadow: 0 4px 20px rgba(212,175,55,.12), 0 2px 8px rgba(0,0,0,0.2) !important;
        transform: translateY(-1px) !important;
        color: #fff !important;
      }
      .nav-container .stButton>button:disabled {
        color: #D4AF37 !important;
        border-color: rgba(212,175,55,.8) !important;
        background: linear-gradient(135deg, rgba(212,175,55,.15), rgba(212,175,55,.08)) !important;
        box-shadow: 0 0 0 1px rgba(212,175,55,.3) inset, 0 4px 16px rgba(212,175,55,.08) !important;
        transform: none !important;
        font-weight: 700 !important;
      }

      /* ëª¨ë°”ì¼ ìµœì í™” (768px ì´í•˜) */
      @media (max-width: 768px) {
        .nav-container .stButton>button {
          font-size: 0.75rem !important;
          padding: 8px 6px !important;
          height: auto !important;
          min-height: 42px !important;
          line-height: 1.3 !important;
          white-space: normal !important;
          word-break: keep-all !important;
        }
        .nav-container {
          padding: 12px 10px !important;
          margin: 6px 0 16px 0 !important;
        }
        .nav-container img {
          height: 36px !important;
        }
      }

      /* ë” ì‘ì€ í™”ë©´ (480px ì´í•˜) */
      @media (max-width: 480px) {
        .nav-container .stButton>button {
          font-size: 0.7rem !important;
          padding: 6px 4px !important;
          min-height: 38px !important;
        }
        .nav-container {
          padding: 10px 8px !important;
          border-radius: 12px !important;
        }
        .nav-container img {
          height: 32px !important;
        }
      }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<div class="nav-container">', unsafe_allow_html=True)
    with st.container():
        c1, c2 = st.columns([1.2, 4.0], gap="medium")
        with c1:
            if logo_uri:
                # ë¡œê³ ë¥¼ í´ë¦­ ê°€ëŠ¥í•œ HTMLë¡œ ì§ì ‘ êµ¬í˜„ (ë©”ë‰´ ë²„íŠ¼ê³¼ ë†’ì´ ì •ë ¬)
                st.markdown(f'''
                <div style="width: 100%; height: 48px; display: flex; align-items: center; justify-content: center;">
                    <a href="?home=1" style="display: block; cursor: pointer; transition: all 0.2s ease;">
                        <img src="{logo_uri}" alt="POSCO ë©”ì¸ìœ¼ë¡œ" title="ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™"
                             style="height: 42px; max-width: 100%; transition: transform 0.2s ease;"
                             onmouseover="this.style.transform='scale(1.05)'"
                             onmouseout="this.style.transform='scale(1)'">
                    </a>
                </div>
                ''', unsafe_allow_html=True)

                # URL íŒŒë¼ë¯¸í„°ë¡œ í´ë¦­ ê°ì§€
                if st.query_params.get("home") == "1":
                    st.session_state.authenticated = True
                    st.query_params.clear()
                    st.rerun()
        with c2:
            cols = st.columns(len(MENU_ITEMS))
            for i, label in enumerate(MENU_ITEMS):
                with cols[i]:
                    clicked = st.button(label, key=f"nav_{label}", use_container_width=True, disabled=(label==active_label))
                    if clicked:
                        st.query_params["menu"] = label
                        st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

# ----------------------------- ë©”ì¸ íˆì–´ë¡œ -----------------------------
def render_main_page():
    bg_uri = load_main_background_uri()
    st.markdown(f"""
    <style>
      .main-hero {{
        position:relative; width:100%; height:72vh; min-height:480px; border-radius:16px; overflow:hidden; margin:20px 0;
        box-shadow:0 20px 60px rgba(0,0,0,.4);
        background:
          linear-gradient(90deg, rgba(0,0,0,.78) 0%, rgba(0,0,0,.45) 42%, rgba(0,0,0,0) 75%),
          url('{bg_uri}') right center / contain no-repeat, #000;
      }}
      @media (max-width:900px){{
        .main-hero {{
          background:
            linear-gradient(180deg, rgba(0,0,0,.72) 0%, rgba(0,0,0,.35) 60%),
            url('{bg_uri}') center 65% / cover no-repeat, #000;
        }}
      }}
      .main-copy {{ position:absolute; left:48px; top:50%; transform:translateY(-50%); color:#fff; max-width:720px; text-shadow:0 4px 20px rgba(0,0,0,.45); }}
      .t {{ font-size:3.2rem; line-height:1.15; font-weight:300; margin:0 0 8px; letter-spacing:-.02em; }}
      .s {{ font-size:1.4rem; margin:4px 0 18px; color:rgba(255,255,255,.95); }}
      .d {{ font-size:1.05rem; color:rgba(255,255,255,.85); line-height:1.55; max-width:560px; }}
      @media (max-width:900px){{ .t{{font-size:2.2rem;}} .s{{font-size:1.1rem;}} }}
    </style>
    <section class="main-hero">
      <div class="main-copy">
        <div class="t">ìœ„ê¸°ê´€ë¦¬ì»¤ë®¤ë‹ˆì¼€ì´ì…˜</div>
        <div class="s">AI ìë™í™” ì†”ë£¨ì…˜</div>
        <div class="d">í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ì˜ ìŠ¤ë§ˆíŠ¸í•œ ì–¸ë¡ ëŒ€ì‘ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.<br/>AI ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ì‹ ì†í•˜ê³  ì •í™•í•œ ìœ„ê¸°ê´€ë¦¬ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.</div>
      </div>
    </section>
    """, unsafe_allow_html=True)

# ----------------------------- í˜ì´ì§€ë“¤ -----------------------------
def page_issue_report():
    # GPTë´‡ ë²„íŠ¼ë§Œ í‘œì‹œ
    st.markdown("""
<div style="display:flex; justify-content:flex-end; margin-bottom:16px;">
    <a href="https://chatgpt.com/g/g-68d89a8acda88191b246fd6b813160a3-pointeo-wigigwanrikeom-cinjeolhan-gaideu-ver-2"
       target="_blank" rel="noopener noreferrer"
       style="display:inline-block; padding:10px 16px; border-radius:8px; font-weight:700; text-decoration:none;
              background:linear-gradient(135deg, #D4AF37, #B8941F); border:1px solid rgba(212,175,55,.4); color:#000;">
      GPTë´‡ ì‚¬ìš©í•˜ê¸°
    </a>
</div>
""", unsafe_allow_html=True)
    col1, col2 = st.columns([1, 1])
    with col1:
        st.markdown('<div class="card input-card"><div style="font-weight:600; margin-bottom:8px;">ì´ìŠˆ ì •ë³´ ì…ë ¥</div>', unsafe_allow_html=True)
        media = st.text_input("ì–¸ë¡ ì‚¬ëª…", placeholder="ì˜ˆ: ì¡°ì„ ì¼ë³´, ë™ì•„ì¼ë³´, í•œêµ­ê²½ì œ ë“±", key="issue_media")
        reporter = st.text_input("ê¸°ìëª…", placeholder="ë‹´ë‹¹ ê¸°ìëª…ì„ ì…ë ¥í•˜ì„¸ìš”", key="issue_reporter")
        issue = st.text_area("ë°œìƒ ì´ìŠˆ", placeholder="ë°œìƒí•œ ì´ìŠˆì— ëŒ€í•´ ìƒì„¸íˆ ê¸°ìˆ í•´ì£¼ì„¸ìš”...", height=150, key="issue_content")
        gen = st.button("ì´ìŠˆë°œìƒë³´ê³  ìƒì„±", use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)
    with col2:
        if gen:
            if not media.strip():
                st.error("ì–¸ë¡ ì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            elif not reporter.strip():
                st.error("ê¸°ìëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            elif not issue.strip():
                st.error("ë°œìƒ ì´ìŠˆë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("AIê°€ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    report = generate_issue_report(media, reporter, issue)
                    st.markdown("### ìƒì„±ëœ ì´ìŠˆ ë°œìƒ ë³´ê³ ì„œ")
                    edited = st.text_area("ë³´ê³ ì„œ ë‚´ìš©(ìˆ˜ì • ê°€ëŠ¥)", value=report, height=300, key="issue_report_edit")
                    if st.button("ì €ì¥í•˜ê¸°", use_container_width=True):
                        with open("temp_issue_report.txt", "w", encoding="utf-8") as f:
                            f.write(edited)
                        st.success("ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (temp_issue_report.txt)")
                    payload = f"""í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ì–¸ë¡ ëŒ€ì‘ ì´ìŠˆ ë°œìƒ ë³´ê³ ì„œ
================================

ìƒì„±ì¼ì‹œ: {datetime.now(timezone(timedelta(hours=9))).strftime('%Y-%m-%d %H:%M:%S')}
ì–¸ë¡ ì‚¬: {media}
ê¸°ìëª…: {reporter}
ë°œìƒ ì´ìŠˆ: {issue}

ë³´ê³ ì„œ ë‚´ìš©:
{edited}
"""
                    st.download_button("ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", data=payload,
                                       file_name=f"ì´ìŠˆë°œìƒë³´ê³ ì„œ_{datetime.now(timezone(timedelta(hours=9))).strftime('%Y%m%d_%H%M%S')}.txt",
                                       mime="text/plain", use_container_width=True)
        else:
            st.markdown('<p style="color: white;">ì¢Œì¸¡ì—ì„œ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</p>', unsafe_allow_html=True)

def page_media_search():
    # ì¶œì…ë§¤ì²´ í˜„í™© ëŒ€ì‹œë³´ë“œ
    media_contacts = get_media_contacts()
    render_publisher_dashboard(media_contacts, show_live=True)

    q = st.text_input("ì–¸ë¡ ì‚¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì¡°ì„ ì¼ë³´, ì¤‘ì•™ì¼ë³´, í•œêµ­ê²½ì œ ë“±", key="media_search_query")
    
    if st.button("ğŸ” ì–¸ë¡ ì‚¬ ì •ë³´ ì¡°íšŒ", use_container_width=True):
        if q:
            with st.spinner("ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # ë””ë²„ê·¸: ë¡œë“œëœ ë°ì´í„° í™•ì¸
                master_data = load_master_data()
                media_contacts = master_data.get("media_contacts", {})
                
                
                info = search_media_info(q)
                if info:
                    st.success(f"âœ… '{info.get('name','')}' ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    c1, c2 = st.columns(2)
                    with c1:
                        st.markdown("#### ğŸ“‹ ê¸°ë³¸ ì •ë³´")
                        st.markdown(f"**ì–¸ë¡ ì‚¬ëª…**: {info.get('name','N/A')}")
                        st.markdown(f"**ë¶„ë¥˜**: {info.get('type','N/A')}")
                        st.markdown(f"**ë‹´ë‹¹ì(ç¤¾å…§)**: {info.get('contact_person','N/A')}")
                        
                        # ì—°ë½ì²˜ ì •ë³´ ì¶”ê°€
                        if info.get('main_phone', 'N/A') != 'N/A':
                            st.markdown(f"**ëŒ€í‘œì „í™”**: {info.get('main_phone','N/A')}")
                        if info.get('fax', 'N/A') != 'N/A':
                            st.markdown(f"**íŒ©ìŠ¤**: {info.get('fax','N/A')}")
                        if info.get('address', 'N/A') != 'N/A':
                            st.markdown(f"**ì£¼ì†Œ**: {info.get('address','N/A')}")
                    
                    with c2:
                        reporters = info.get("reporters", [])
                        if reporters:
                            # ì—°ë½ì²˜ ì •ë³´ë¥¼ êµ¬ë¶„ë³„ë¡œ ì •ë ¬ (DESK/ë¶€ì„œë‹´ë‹¹ìë¥¼ ìƒë‹¨ì— ë°°ì¹˜)
                            reporters_data = []
                            for reporter in reporters:
                                # ë¹ˆ ë¬¸ìì—´ì€ "-"ë¡œ í‘œì‹œ
                                reporters_data.append({
                                    "ì´ë¦„": reporter.get("ì´ë¦„", "") or "-",
                                    "ì§ì±…": reporter.get("ì§ì±…", "") or "-",
                                    "ì—°ë½ì²˜": reporter.get("ì—°ë½ì²˜", "") or "-",
                                    "ì´ë©”ì¼": reporter.get("ì´ë©”ì¼", "") or "-",
                                    "êµ¬ë¶„": reporter.get("êµ¬ë¶„", "ê¸°ì")
                                })
                            
                            if reporters_data:
                                # ì§ì±…ë³„ ê³„ì¸µ ì •ë ¬: êµ­ì¥ â†’ ë¶€ì¥ â†’ íŒ€ì¥ â†’ ì°¨ì¥ â†’ ê¸°íƒ€
                                def sort_key(reporter):
                                    position = reporter.get("ì§ì±…", "").lower()
                                    category = reporter.get("êµ¬ë¶„", "")
                                    
                                    # ì§ì±… ìš°ì„ ìˆœìœ„ ê²°ì •
                                    if "êµ­ì¥" in position:
                                        priority = 0
                                    elif "ë¶€ì¥" in position:
                                        priority = 1
                                    elif "íŒ€ì¥" in position:
                                        priority = 2
                                    elif "ì°¨ì¥" in position:
                                        priority = 3
                                    elif category == "DESK/ë¶€ì„œë‹´ë‹¹ì":
                                        priority = 1  # ë¶€ì¥ê¸‰ìœ¼ë¡œ ì²˜ë¦¬
                                    else:
                                        priority = 4  # ê¸°íƒ€
                                    
                                    return (priority, reporter.get("ì´ë¦„", ""))
                                
                                sorted_reporters = sorted(reporters_data, key=sort_key)
                                
                                # êµ¬ë¶„ í•„ë“œë¥¼ ì œì™¸í•˜ê³  DataFrame ìƒì„±
                                display_data = [{k: v for k, v in reporter.items() if k != "êµ¬ë¶„"} for reporter in sorted_reporters]
                                df_reporters = pd.DataFrame(display_data)
                                show_table(df_reporters, "ğŸ‘¥ ì¶œì…ê¸°ì ìƒì„¸ì •ë³´")
                        else:
                            st.info("ë“±ë¡ëœ ì¶œì…ê¸°ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    with st.expander("ğŸ” ìƒì„¸ ë°ì´í„° (ê°œë°œììš©)"):
                        st.json(info.get("raw_data", {}))
                else:
                    st.warning(f"âŒ '{q}' ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    with st.expander("ğŸ“‹ ë“±ë¡ëœ ì–¸ë¡ ì‚¬ ëª©ë¡ í™•ì¸"):
                        try:
                            contacts = get_media_contacts()
                            lst = list(contacts.keys())
                            cols = st.columns(3)
                            for i, name in enumerate(lst):
                                cols[i % 3].write(f"â€¢ {name}")
                        except Exception as e:
                            st.error(f"ì–¸ë¡ ì‚¬ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            st.error("ì–¸ë¡ ì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def page_contact_search():
    departments = load_master_data_fresh().get("departments", {})

    search_query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥ (ë¶€ì„œëª…, ë‹´ë‹¹ìëª…, ì—°ë½ì²˜, ì´ë©”ì¼, ë‹´ë‹¹ì´ìŠˆ):", placeholder="ì˜ˆ) ê¹€ìš°í˜„, ì‹ëŸ‰, í™ë³´ê·¸ë£¹", key="contact_search_name")
    if st.button("ğŸ” ë‹´ë‹¹ì ê²€ìƒ‰", use_container_width=True):
        rows = []
        # í™ë³´ê·¸ë£¹ì„ ë¨¼ì € ì²˜ë¦¬
        if "í™ë³´ê·¸ë£¹" in departments:
            dept = departments["í™ë³´ê·¸ë£¹"]
            if "ë‹´ë‹¹ìë“¤" in dept:
                for p in dept["ë‹´ë‹¹ìë“¤"]:
                    ë‹´ë‹¹ì´ìŠˆ_str = ", ".join(dept.get("ë‹´ë‹¹ì´ìŠˆ", []))
                    rows.append({"ë¶€ì„œëª…": "í™ë³´ê·¸ë£¹", "ì„±ëª…": p.get("ë‹´ë‹¹ì",""), "ì§ê¸‰": p.get("ì§ê¸‰",""),
                                 "ì—°ë½ì²˜": p.get("ì—°ë½ì²˜",""), "ì´ë©”ì¼": p.get("ì´ë©”ì¼",""), "ë‹´ë‹¹ì´ìŠˆ": ë‹´ë‹¹ì´ìŠˆ_str})
        # ë‚˜ë¨¸ì§€ ë¶€ì„œë“¤ ì²˜ë¦¬
        for dept_name, dept in departments.items():
            if dept_name == "í™ë³´ê·¸ë£¹":  # ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                continue
            if "ë‹´ë‹¹ìë“¤" in dept:
                for p in dept["ë‹´ë‹¹ìë“¤"]:
                    ë‹´ë‹¹ì´ìŠˆ_str = ", ".join(dept.get("ë‹´ë‹¹ì´ìŠˆ", []))
                    rows.append({"ë¶€ì„œëª…": dept_name, "ì„±ëª…": p.get("ë‹´ë‹¹ì",""), "ì§ê¸‰": p.get("ì§ê¸‰",""),
                                 "ì—°ë½ì²˜": p.get("ì—°ë½ì²˜",""), "ì´ë©”ì¼": p.get("ì´ë©”ì¼",""), "ë‹´ë‹¹ì´ìŠˆ": ë‹´ë‹¹ì´ìŠˆ_str})
            else:
                ë‹´ë‹¹ì´ìŠˆ_str = ", ".join(dept.get("ë‹´ë‹¹ì´ìŠˆ", []))
                rows.append({"ë¶€ì„œëª…": dept_name, "ì„±ëª…": dept.get("ë‹´ë‹¹ì",""), "ì§ê¸‰": dept.get("ì§ê¸‰",""),
                             "ì—°ë½ì²˜": dept.get("ì—°ë½ì²˜",""), "ì´ë©”ì¼": dept.get("ì´ë©”ì¼",""), "ë‹´ë‹¹ì´ìŠˆ": ë‹´ë‹¹ì´ìŠˆ_str})

        # í™•ì¥ëœ ê²€ìƒ‰ ë¡œì§: ë¶€ì„œëª…, ì„±ëª…, ì—°ë½ì²˜, ì´ë©”ì¼, ë‹´ë‹¹ì´ìŠˆì—ì„œ ê²€ìƒ‰
        if search_query.strip():
            filtered = []
            for r in rows:
                if (search_query.strip() in r["ë¶€ì„œëª…"] or
                    search_query.strip() in r["ì„±ëª…"] or
                    search_query.strip() in r["ì—°ë½ì²˜"] or
                    search_query.strip() in r["ì´ë©”ì¼"] or
                    search_query.strip() in r["ë‹´ë‹¹ì´ìŠˆ"]):
                    filtered.append(r)
        else:
            filtered = rows

        if filtered:
            show_table(pd.DataFrame(filtered), "ğŸ‘¥ ë‹´ë‹¹ì ê²€ìƒ‰ ê²°ê³¼")
        else:
            st.warning("âŒ í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë‹´ë‹¹ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.markdown('</div>', unsafe_allow_html=True)

    st.markdown('<div class="card">', unsafe_allow_html=True)
    rows = []
    # í™ë³´ê·¸ë£¹ì„ ë¨¼ì € ì²˜ë¦¬
    if "í™ë³´ê·¸ë£¹" in departments:
        dept = departments["í™ë³´ê·¸ë£¹"]
        if "ë‹´ë‹¹ìë“¤" in dept:
            for p in dept["ë‹´ë‹¹ìë“¤"]:
                rows.append(["í™ë³´ê·¸ë£¹", p.get("ë‹´ë‹¹ì",""), p.get("ì§ê¸‰","")])
    # ë‚˜ë¨¸ì§€ ë¶€ì„œë“¤ ì²˜ë¦¬
    for dept_name, dept in departments.items():
        if dept_name == "í™ë³´ê·¸ë£¹":  # ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
            continue
        if "ë‹´ë‹¹ìë“¤" in dept:
            for p in dept["ë‹´ë‹¹ìë“¤"]:
                rows.append([dept_name, p.get("ë‹´ë‹¹ì",""), p.get("ì§ê¸‰","")])
        else:
            rows.append([dept_name, dept.get("ë‹´ë‹¹ì",""), dept.get("ì§ê¸‰","")])
    df = pd.DataFrame(rows, columns=["ë¶€ì„œëª…","ë‹´ë‹¹ì","ì§ê¸‰"])
    show_table(df, "ğŸ”· ì „ì²´ ë¶€ì„œ ë‹´ë‹¹ì ì •ë³´")
    st.markdown('</div>', unsafe_allow_html=True)

def page_history_search():
    # 60ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ íŒŒì¼ ë³€ê²½ ì²´í¬ (ë°±ê·¸ë¼ìš´ë“œ) - ì„±ëŠ¥ ìµœì í™”
    st_autorefresh(interval=60000, key="history_autorefresh")

    # íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ìë™ ìƒˆë¡œê³ ì¹¨
    try:
        current_mtime = os.path.getmtime(MEDIA_RESPONSE_FILE)
        if 'media_response_mtime' not in st.session_state:
            st.session_state.media_response_mtime = current_mtime
        elif st.session_state.media_response_mtime != current_mtime:
            st.session_state.media_response_mtime = current_mtime
            clear_all_caches()  # ìºì‹œ í´ë¦¬ì–´
            st.toast("âœ… ì–¸ë¡ ëŒ€ì‘ë‚´ì—­ íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì–´ ìë™ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!", icon="ğŸ”„")
            st.rerun()
    except Exception:
        pass

    # ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
    df_all = load_media_response_data()

    if df_all.empty:
        st.warning("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 'data/ì–¸ë¡ ëŒ€ì‘ë‚´ì—­.csv'ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.info("""
        **CSV íŒŒì¼ í˜•ì‹:**
        ```
        ìˆœë²ˆ,ë°œìƒ ì¼ì‹œ,ë°œìƒ ìœ í˜•,í˜„ì—… ë¶€ì„œ,ë‹¨ê³„,ì´ìŠˆ ë°œìƒ ë³´ê³ ,ëŒ€ì‘ ê²°ê³¼
        1,2024-01-15,ê¸°íšê¸°ì‚¬,í™ë³´íŒ€,ê´€ì‹¬,í¬ìŠ¤ì½”ì¸í„° ê´€ë ¨ ê¸°ì‚¬,ì—°í•©ë‰´ìŠ¤
        ```
        """)
        st.markdown('</div>', unsafe_allow_html=True)
        return

    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["ë°œìƒ ì¼ì‹œ", "ë‹¨ê³„", "ë°œìƒ ìœ í˜•", "í˜„ì—… ë¶€ì„œ", "ì´ìŠˆ ë°œìƒ ë³´ê³ ", "ëŒ€ì‘ ê²°ê³¼"]
    missing = [c for c in required if c not in df_all.columns]
    if missing:
        st.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing)}")
        st.info(f"**í˜„ì¬ ì»¬ëŸ¼:** {', '.join(df_all.columns.tolist())}")
        st.markdown('</div>', unsafe_allow_html=True)
        return

    # ë‚ ì§œ íŒŒì‹±
    df_all["ë°œìƒ ì¼ì‹œ"] = pd.to_datetime(df_all["ë°œìƒ ì¼ì‹œ"], errors="coerce")
    valid_dates = df_all["ë°œìƒ ì¼ì‹œ"].dropna()

    # ë°œìƒ ìœ í˜• í‘œì¤€í™” (ìœ ì‚¬í•œ ì¹´í…Œê³ ë¦¬ í†µí•©)
    type_mapping = {
        # ê¸°ì‚¬ ê²Œì¬ í†µí•©
        'ê¸°ì‚¬ê²Œì¬': 'ê¸°ì‚¬ ê²Œì¬',
        'ê¸°ì‚¬ ê²Œì¬': 'ê¸°ì‚¬ ê²Œì¬',

        # ê¸°ì ë¬¸ì˜ í†µí•©
        'ê¸°ìë¬¸ì˜': 'ê¸°ì ë¬¸ì˜',
        'ê¸°ì ë¬¸ì˜': 'ê¸°ì ë¬¸ì˜',
        'ì–¸ë¡  ë¬¸ì˜': 'ê¸°ì ë¬¸ì˜',
        'ì–¸ë¡ ë¬¸ì˜': 'ê¸°ì ë¬¸ì˜',

        # ê¸°íšê¸°ì‚¬ í†µí•©
        'ê¸°íšê¸°ì‚¬': 'ê¸°íšê¸°ì‚¬',
        'ê¸°íšìë£Œ': 'ê¸°íšê¸°ì‚¬',
        'ê¸°íšìë£Œ ê²Œì¬': 'ê¸°íšê¸°ì‚¬',
        'ê¸°íšìë£Œê²Œì¬': 'ê¸°íšê¸°ì‚¬',

        # ì ì¬ì´ìŠˆ í†µí•©
        'ì ì¬ì´ìŠˆ': 'ì ì¬ì´ìŠˆ',
        'ì ì¬ ì´ìŠˆ': 'ì ì¬ì´ìŠˆ',

        # ë³´ë„ìë£Œ í†µí•©
        'ë³´ë„ìë£Œ': 'ë³´ë„ìë£Œ',
        'ë³´ë„ìë£Œ ê²Œì¬': 'ë³´ë„ìë£Œ',
        'ë³´ë„ìë£Œê²Œì¬': 'ë³´ë„ìë£Œ',
    }

    # ë°œìƒ ìœ í˜• ì»¬ëŸ¼ í‘œì¤€í™” ì ìš©
    if "ë°œìƒ ìœ í˜•" in df_all.columns:
        df_all["ë°œìƒ ìœ í˜•"] = df_all["ë°œìƒ ìœ í˜•"].astype(str).str.strip()
        # nan, None, ë¹ˆ ë¬¸ìì—´ ì œê±°
        df_all["ë°œìƒ ìœ í˜•"] = df_all["ë°œìƒ ìœ í˜•"].replace(['nan', 'None', '', 'NaN', 'NAN'], pd.NA)
        df_all["ë°œìƒ ìœ í˜•"] = df_all["ë°œìƒ ìœ í˜•"].replace(type_mapping)

    # 2025ë…„ ë°ì´í„° í•„í„°ë§
    df_2025 = df_all[df_all["ë°œìƒ ì¼ì‹œ"].dt.year == 2025].copy()

    # 2025ë…„ í†µê³„ ì •ë³´ í‘œì‹œ (ìƒë‹¨ì— ë°”ë¡œ í‘œì‹œ)
    stage_counts = df_2025["ë‹¨ê³„"].value_counts().to_dict()
    ê´€ì‹¬_count = stage_counts.get('ê´€ì‹¬', 0)
    ì£¼ì˜_count = stage_counts.get('ì£¼ì˜', 0)
    ìœ„ê¸°_count = stage_counts.get('ìœ„ê¸°', 0)
    ë¹„ìƒ_count = stage_counts.get('ë¹„ìƒ', 0)

    total = len(df_2025)
    ê´€ì‹¬_pct = (ê´€ì‹¬_count / total * 100) if total > 0 else 0
    ì£¼ì˜_pct = (ì£¼ì˜_count / total * 100) if total > 0 else 0
    ìœ„ê¸°_pct = (ìœ„ê¸°_count / total * 100) if total > 0 else 0
    ë¹„ìƒ_pct = (ë¹„ìƒ_count / total * 100) if total > 0 else 0

    # ì „ë¬¸ì ì¸ ëŒ€ì‹œë³´ë“œ ì¹´ë“œ ë Œë”ë§
    render_status_dashboard(
        total=total,
        status_counts={
            'ê´€ì‹¬': ê´€ì‹¬_count,
            'ì£¼ì˜': ì£¼ì˜_count,
            'ìœ„ê¸°': ìœ„ê¸°_count,
            'ë¹„ìƒ': ë¹„ìƒ_count
        },
        year=2025,
        show_live=True
    )

    st.markdown("---")

    # ê²€ìƒ‰ í•„í„°
    years = sorted(valid_dates.dt.year.unique().tolist()) if not valid_dates.empty else []
    # ë°œìƒ ìœ í˜• ì˜µì…˜ ìƒì„± (nan, None, ë¹ˆ ë¬¸ìì—´ ì œì™¸)
    type_options = ["ì „ì²´"] + sorted([
        t for t in df_all["ë°œìƒ ìœ í˜•"].dropna().unique().tolist()
        if t and str(t).lower() not in ['nan', 'none', '']
    ])

    st.markdown("### ğŸ” ê²€ìƒ‰ ì¡°ê±´")
    with st.container():
        col_period, col_stage, col_type = st.columns([1, 1, 1])
        with col_period:
            period_mode = st.selectbox("ê¸°ê°„", ["ì „ì²´", "ì—°ë„", "ì—°ì›”"], index=0, key="hist_period")
            sel_year = sel_month = None
            if period_mode == "ì—°ë„" and years:
                sel_year = st.selectbox("ì—°ë„ ì„ íƒ", options=years, index=len(years)-1, key="hist_year")
            elif period_mode == "ì—°ì›”" and years:
                sel_year = st.selectbox("ì—°ë„ ì„ íƒ", options=years, index=len(years)-1, key="hist_year2")
                months = sorted(valid_dates[valid_dates.dt.year == sel_year].dt.month.unique().tolist())
                sel_month = st.selectbox("ì›” ì„ íƒ", options=months if months else [], key="hist_month")
        with col_stage:
            stage_option = st.selectbox("ë‹¨ê³„", ["ì „ì²´", "ê´€ì‹¬", "ì£¼ì˜", "ìœ„ê¸°", "ë¹„ìƒ"], index=0, key="hist_stage")
        with col_type:
            type_option = st.selectbox("ë°œìƒ ìœ í˜•", type_options, index=0, key="hist_type")

    col_kw, col_btn = st.columns([4, 1])
    with col_kw:
        keyword = st.text_input("ê²€ìƒ‰ì–´", value="", placeholder="ì˜ˆ) í¬ìŠ¤ì½”, ì‹¤ì ë°œí‘œ, IR (ì—¬ëŸ¬ ë‹¨ì–´ ê³µë°± â†’ AND)", key="history_search_keyword")
    with col_btn:
        st.markdown('<div style="height: 1.6rem;"></div>', unsafe_allow_html=True)
        do_search = st.button("ğŸ” ê²€ìƒ‰", use_container_width=True, type="primary")

    # ê²€ìƒ‰ ì‹¤í–‰
    if do_search:
        result_df = df_all.copy()

        # ê¸°ê°„ í•„í„°
        if period_mode == "ì—°ë„" and sel_year is not None:
            result_df = result_df[result_df["ë°œìƒ ì¼ì‹œ"].dt.year == sel_year]
        elif period_mode == "ì—°ì›”" and sel_year is not None and sel_month is not None:
            result_df = result_df[(result_df["ë°œìƒ ì¼ì‹œ"].dt.year == sel_year) &
                                  (result_df["ë°œìƒ ì¼ì‹œ"].dt.month == int(sel_month))]

        # ë‹¨ê³„ í•„í„°
        if stage_option != "ì „ì²´":
            result_df = result_df[result_df["ë‹¨ê³„"].astype(str).str.contains(stage_option, case=False, na=False)]

        # ë°œìƒ ìœ í˜• í•„í„°
        if type_option != "ì „ì²´":
            result_df = result_df[result_df["ë°œìƒ ìœ í˜•"].astype(str).str.contains(type_option, case=False, na=False)]

        # í‚¤ì›Œë“œ ê²€ìƒ‰ (AND ì¡°ê±´)
        if keyword.strip():
            terms = [t for t in keyword.split() if t.strip()]
            target_cols = ["ë°œìƒ ìœ í˜•", "í˜„ì—… ë¶€ì„œ", "ì´ìŠˆ ë°œìƒ ë³´ê³ ", "ëŒ€ì‘ ê²°ê³¼"]
            for t in terms:
                mask_any = result_df[target_cols].astype(str).apply(
                    lambda col: col.str.contains(t, case=False, na=False)
                ).any(axis=1)
                result_df = result_df[mask_any]

        # ê²°ê³¼ í‘œì‹œ
        if not result_df.empty:
            try:
                result_df = result_df.sort_values("ë°œìƒ ì¼ì‹œ", ascending=False)
            except Exception:
                pass

            st.markdown("---")
            st.markdown(f"### ğŸ“ˆ ê²€ìƒ‰ ê²°ê³¼: ì´ **{len(result_df):,}ê±´**")

            # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ (ìˆœë²ˆ ì œì™¸, ë‚ ì§œ í¬ë§·íŒ…)
            display_df = result_df.copy()
            if "ë°œìƒ ì¼ì‹œ" in display_df.columns:
                display_df["ë°œìƒ ì¼ì‹œ"] = display_df["ë°œìƒ ì¼ì‹œ"].dt.strftime("%Y-%m-%d").fillna("")

            # ìˆœë²ˆ ì»¬ëŸ¼ ì œì™¸
            display_cols = [c for c in display_df.columns if c != "ìˆœë²ˆ"]
            display_df = display_df[display_cols]

            # ì»¬ëŸ¼ëª… í•œê¸€í™”
            display_df = display_df.rename(columns={
                "ë°œìƒ ì¼ì‹œ": "ğŸ“… ë°œìƒì¼ì‹œ",
                "ë°œìƒ ìœ í˜•": "ğŸ“‘ ìœ í˜•",
                "í˜„ì—… ë¶€ì„œ": "ğŸ¢ ë¶€ì„œ",
                "ë‹¨ê³„": "âš ï¸ ë‹¨ê³„",
                "ì´ìŠˆ ë°œìƒ ë³´ê³ ": "ğŸ“° ì´ìŠˆ ë‚´ìš©",
                "ëŒ€ì‘ ê²°ê³¼": "âœ… ëŒ€ì‘ ê²°ê³¼"
            })

            show_table(display_df, "")
        else:
            st.warning("âŒ ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")

def page_news_monitor():
    # ===== ê¸°ë³¸ íŒŒë¼ë¯¸í„° =====
    keywords = [
        "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„",
        "POSCO INTERNATIONAL",
        "í¬ìŠ¤ì½”ì¸í„°",
        "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ",
        "êµ¬ë™ëª¨í„°ì½”ì•„",
        "êµ¬ë™ëª¨í„°ì½”ì–´",
        "ë¯¸ì–€ë§ˆ LNG",
        "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜",
        "í¬ìŠ¤ì½”"  # ì¼ë°˜ í¬ìŠ¤ì½” ê¸°ì‚¬ (ê¸°ì¡´ í‚¤ì›Œë“œ ì œì™¸ í•„í„°ë§ ì ìš©)
    ]
    # í¬ìŠ¤ì½” ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œì™¸í•  í‚¤ì›Œë“œ (ì¤‘ë³µ ë°©ì§€)
    exclude_keywords = ["í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„", "POSCO INTERNATIONAL", "í¬ìŠ¤ì½”ì¸í„°",
                       "ì‚¼ì²™ë¸”ë£¨íŒŒì›Œ", "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"]

    refresh_interval = 180  # 180ì´ˆ ì¹´ìš´íŠ¸ë‹¤ìš´ (3ë¶„) - ë¹ ë¥¸ ì—…ë°ì´íŠ¸
    max_items = 100  # í‚¤ì›Œë“œë‹¹ ì•½ 11ê°œ ìˆ˜ì§‘ (í•„í„°ë§ í›„ ì¶©ë¶„í•œ ê¸°ì‚¬ í™•ë³´)

    # ===== ì„¸ì…˜ ìƒíƒœ ê¸°ë³¸ê°’ =====
    now = time.time()
    if "next_refresh_at" not in st.session_state:
        st.session_state.next_refresh_at = now + refresh_interval
    if "last_news_fetch" not in st.session_state:
        st.session_state.last_news_fetch = 0.0   # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°
    if "initial_loaded" not in st.session_state:
        st.session_state.initial_loaded = False  # ì²« ë Œë” ì´í›„ True
    if "trigger_news_update" not in st.session_state:
        st.session_state.trigger_news_update = False

    # ===== ë‹¹ì¼ ë‰´ìŠ¤ í˜„í™© ëŒ€ì‹œë³´ë“œ (ìµœìƒë‹¨ ë°°ì¹˜) =====
    # ì„¸ì…˜ì— ìµœì‹  ìˆ˜ì§‘ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (ì¦‰ì‹œ ë°˜ì˜)
    db_for_dashboard = st.session_state.get('news_display_data', load_news_db())
    render_news_dashboard(db_for_dashboard, show_live=True)

    # ===== ìƒë‹¨ UI (ì¹´ìš´íŠ¸ë‹¤ìš´/ìƒíƒœ/ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨) =====
    c_count, c_status, c_btn = st.columns([1, 2.5, 1])
    with c_btn:
        manual_refresh = st.button("ğŸ”„ ì§€ê¸ˆ ìƒˆë¡œê³ ì¹¨", use_container_width=True)
    with c_status:
        status = st.empty()

    # ì¹´ìš´íŠ¸ë‹¤ìš´ í”„ë˜ê·¸ë¨¼íŠ¸ (1ì´ˆ ë‹¨ìœ„ ì—…ë°ì´íŠ¸)
    with c_count:
        countdown_fragment(refresh_interval)

    # ìµœì†Œ ê°„ê²©ì˜ êµ¬ë¶„ì„ 
    st.markdown('<div style="margin: 8px 0;"></div>', unsafe_allow_html=True)

    # ===== ìƒˆë¡œê³ ì¹¨ ë°©ì‹ ê²°ì • =====
    # ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨: Naver API ì§ì ‘ í˜¸ì¶œ (ì‹¤ì‹œê°„ ìµœì‹  ë‰´ìŠ¤)
    # ìë™ ìƒˆë¡œê³ ì¹¨/ì´ˆê¸° ë¡œë“œ: Naver API í˜¸ì¶œ (ìµœì‹  ë°ì´í„°)
    if manual_refresh:
        # ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨: Naver API ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
        should_fetch = True
        st.session_state.trigger_news_update = True

        # ë³´ê³ ì„œ ì´ˆê¸°í™”
        report_keys = [key for key in st.session_state.keys() if key.startswith('report_state_')]
        for key in report_keys:
            del st.session_state[key]
        if report_keys:
            print(f"[DEBUG] ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨: {len(report_keys)}ê°œ ë³´ê³ ì„œ ì´ˆê¸°í™”")

        # íƒ€ì´ë¨¸ ë¦¬ì…‹
        st.session_state.next_refresh_at = time.time() + refresh_interval
    else:
        # ìë™ ìƒˆë¡œê³ ì¹¨ ë˜ëŠ” ì´ˆê¸° ë¡œë“œ: Naver API í˜¸ì¶œ
        should_fetch = st.session_state.trigger_news_update or (not st.session_state.initial_loaded)

        # ìë™ ìƒˆë¡œê³ ì¹¨ ì‹œ ë³´ê³ ì„œ ì´ˆê¸°í™”
        if st.session_state.trigger_news_update:
            report_keys = [key for key in st.session_state.keys() if key.startswith('report_state_')]
            for key in report_keys:
                del st.session_state[key]
            if report_keys:
                print(f"[DEBUG] ìë™ ìƒˆë¡œê³ ì¹¨: {len(report_keys)}ê°œ ë³´ê³ ì„œ ì´ˆê¸°í™”")

    # ===== ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§ =====
    if should_fetch:
        # ìˆ˜ì§‘ ì§ì „ ìƒíƒœ ë©”ì‹œì§€
        status.info("ğŸ”„ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘â€¦")

        # API í‚¤ ìœ íš¨ì„± ì²´í¬
        headers = _naver_headers()
        api_ok = bool(headers.get("X-Naver-Client-Id") and headers.get("X-Naver-Client-Secret"))

        # ì´ˆê¸°ì—” DBê°€ ìˆìœ¼ë©´ ë¨¼ì € ë³´ì—¬ì£¼ê³ (ëŠê¹€ ì—†ëŠ” í™”ë©´), ë°±ê·¸ë¼ìš´ë“œì²˜ëŸ¼ ë°”ë¡œ ìˆ˜ì§‘ ì‹œë„
        existing_db = load_news_db()

        try:
            all_news = []
            quota_exceeded = False

            if api_ok:
                # í‚¤ì›Œë“œë³„ ìµœì‹ ìˆœ ìˆ˜ì§‘
                for kw in keywords:
                    df_kw = crawl_naver_news(kw, max_items=max_items // len(keywords), sort="date")

                    # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì²´í¬
                    if df_kw.attrs.get('quota_exceeded', False):
                        print(f"[DEBUG] âš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€ - ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ë‹¨")
                        quota_exceeded = True
                        break

                    if not df_kw.empty:
                        # "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" ì •í™•í•œ ë§¤ì¹­ ê°•í™”
                        if kw == "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„":
                            def should_include_posco_intl(row):
                                title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                                description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                                # ì •í™•íˆ "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„"ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
                                if "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" not in title and "í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„" not in description:
                                    return False

                                # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                                exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                                for exclude_word in exclude_words:
                                    if exclude_word in title or exclude_word in description:
                                        return False

                                return True

                            mask = df_kw.apply(should_include_posco_intl, axis=1)
                            df_kw = df_kw[mask].reset_index(drop=True)
                            if not df_kw.empty:
                                print(f"[DEBUG] 'í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„' ì •í™• ë§¤ì¹­ í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                        # "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" ì •í™•í•œ ë§¤ì¹­ ê°•í™”
                        elif kw == "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜":
                            def should_include_posco_mobility(row):
                                title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                                description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))

                                # ì •í™•íˆ "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜"ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
                                if "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" not in title and "í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜" not in description:
                                    return False

                                # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                                exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                                for exclude_word in exclude_words:
                                    if exclude_word in title or exclude_word in description:
                                        return False

                                return True

                            mask = df_kw.apply(should_include_posco_mobility, axis=1)
                            df_kw = df_kw[mask].reset_index(drop=True)
                            if not df_kw.empty:
                                print(f"[DEBUG] 'í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹°ì†”ë£¨ì…˜' ì •í™• ë§¤ì¹­ í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                        # "í¬ìŠ¤ì½”" í‚¤ì›Œë“œì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                        elif kw == "í¬ìŠ¤ì½”":
                            def should_include_posco(row):
                                title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                                title_lower = title.lower()
                                description = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))  # ë‚´ìš© í•„ë“œ
                                content_lower = description.lower()

                                # ê¸°ì¡´ ì¡°ê±´: íƒ€ì´í‹€ì— "í¬ìŠ¤ì½”" í¬í•¨
                                title_has_posco = "í¬ìŠ¤ì½”" in title or "posco" in title_lower

                                # ìƒˆ ì¡°ê±´: íƒ€ì´í‹€ì— "[ë‹¨ë…]" í¬í•¨ AND ë‚´ìš©ì— "í¬ìŠ¤ì½”" í¬í•¨
                                is_exclusive_with_posco_in_content = "[ë‹¨ë…]" in title and "í¬ìŠ¤ì½”" in description

                                # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ í¬í•¨ (1ë‹¨ê³„)
                                if not (title_has_posco or is_exclusive_with_posco_in_content):
                                    return False

                                # 2ë‹¨ê³„: ì œëª©ì— ì œì™¸ í‚¤ì›Œë“œ(í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ë“±)ê°€ ì—†ëŠ”ê°€?
                                for exclude_kw in exclude_keywords:
                                    if exclude_kw.lower() in title_lower:
                                        return False

                                # 3ë‹¨ê³„: ì œëª© ë˜ëŠ” ë‚´ìš©ì— ë¶€ë™ì‚° í‚¤ì›Œë“œê°€ ì—†ëŠ”ê°€?
                                exclude_words = ["ì²­ì•½", "ë¶„ì–‘", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                                for exclude_word in exclude_words:
                                    if exclude_word in title or exclude_word in description:
                                        return False

                                return True

                            # í¬ìŠ¤ì½” ì „ìš© í•„í„°ë§ ì ìš©
                            mask_posco = df_kw.apply(should_include_posco, axis=1)
                            df_kw = df_kw[mask_posco].reset_index(drop=True)
                            if not df_kw.empty:
                                print(f"[DEBUG] 'í¬ìŠ¤ì½”' í•„í„°ë§ ì™„ë£Œ: {len(df_kw)}ê±´ ì¶”ê°€")

                        else:
                            # ë‹¤ë¥¸ í‚¤ì›Œë“œëŠ” ê¸°ì¡´ì²˜ëŸ¼ ì œëª©ì—ì„œë§Œ ë¶€ë™ì‚° ê´€ë ¨ í‚¤ì›Œë“œ ì œê±°
                            exclude_words = ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼", "ì¬ê±´ì¶•", "ì •ë¹„êµ¬ì—­"]
                            def should_include_general(row):
                                title = str(row.get("ê¸°ì‚¬ì œëª©", ""))
                                for exclude_word in exclude_words:
                                    if exclude_word in title:
                                        return False
                                return True

                            mask_general = df_kw.apply(should_include_general, axis=1)
                            df_kw = df_kw[mask_general].reset_index(drop=True)

                        if not df_kw.empty:
                            all_news.append(df_kw)

                # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
                if quota_exceeded:
                    status.error("âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼ (ì¼ì¼ 25,000íšŒ ì œí•œ)\n\n"
                                "ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                                "1. ìƒˆë¡œìš´ ë„¤ì´ë²„ ê°œë°œì ê³„ì •ìœ¼ë¡œ API í‚¤ ì¬ë°œê¸‰\n"
                                "2. ë§¤ì¼ ìì •(KST) ì´í›„ í• ë‹¹ëŸ‰ ì¬ì„¤ì •\n"
                                "3. ê¸°ì¡´ ì €ì¥ëœ ë‰´ìŠ¤ ë°ì´í„°ëŠ” ìœ ì§€ë©ë‹ˆë‹¤")
                    # í”Œë˜ê·¸ ë¦¬ì…‹
                    st.session_state.trigger_news_update = False
                    st.session_state.next_refresh_at = time.time() + refresh_interval
                    st.session_state.initial_loaded = True
                else:
                    # í†µí•© ì •ë¦¬ & ì €ì¥
                    df_new = pd.concat(all_news, ignore_index=True) if all_news else pd.DataFrame()
                    if not df_new.empty:
                        df_new["ë‚ ì§œ_datetime"] = pd.to_datetime(df_new["ë‚ ì§œ"], errors="coerce")
                        df_new = df_new.sort_values("ë‚ ì§œ_datetime", ascending=False, na_position="last").reset_index(drop=True)
                        df_new = df_new.drop("ë‚ ì§œ_datetime", axis=1)

                        # ì¤‘ë³µ ì œê±° (URL ìš°ì„ , ì—†ìœ¼ë©´ ì œëª©+ë‚ ì§œ)
                        key = df_new["URL"].where(df_new["URL"].astype(bool), df_new["ê¸°ì‚¬ì œëª©"] + "|" + df_new["ë‚ ì§œ"])
                        df_new = df_new.loc[~key.duplicated()].reset_index(drop=True)

                        # ê¸°ì¡´ DBì™€ ë³‘í•©í•´ ìµœì‹ ìˆœ ì •ë ¬
                        merged = pd.concat([df_new, existing_db], ignore_index=True) if not existing_db.empty else df_new
                        merged = merged.drop_duplicates(subset=["URL", "ê¸°ì‚¬ì œëª©"], keep="first").reset_index(drop=True)
                        if not merged.empty:
                            merged["ë‚ ì§œ"] = pd.to_datetime(merged["ë‚ ì§œ"], errors="coerce")
                            merged = merged.sort_values("ë‚ ì§œ", ascending=False, na_position="last").reset_index(drop=True)
                            merged["ë‚ ì§œ"] = merged["ë‚ ì§œ"].dt.strftime("%Y-%m-%d %H:%M")

                        # ì‹ ê·œ ê¸°ì‚¬ ê°ì§€ (ì°¸ê³ ìš©)
                        new_articles = detect_new_articles(existing_db, df_new)

                        # ğŸ”’ Streamlitì€ ì½ê¸° ì „ìš© ëª¨ë“œ - DB ì €ì¥ ë¹„í™œì„±í™”
                        # DB ì €ì¥ê³¼ í…”ë ˆê·¸ë¨ ì•Œë¦¼ì€ GitHub Actionsì—ì„œë§Œ ë‹´ë‹¹
                        # save_news_db(merged)  # ë¹„í™œì„±í™”

                        # ì„¸ì…˜ ìƒíƒœì—ë§Œ ì €ì¥ (UI í‘œì‹œìš©)
                        st.session_state.news_display_data = merged

                        # ğŸ”’ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ë¹„í™œì„±í™” - GitHub Actions ì „ìš©
                        # send_telegram_notification(new_articles)  # ë¹„í™œì„±í™”

                        if new_articles:
                            print(f"[STREAMLIT] ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ ê°ì§€ (í…”ë ˆê·¸ë¨ì€ GitHub Actionsì—ì„œ ë°œì†¡)")
                        st.session_state.last_news_fetch = now

                        # ìƒíƒœ ë©”ì‹œì§€ì— ì‹ ê·œ ê¸°ì‚¬ ìˆ˜ í‘œì‹œ
                        if new_articles:
                            status.success(f"âœ… ê¸°ì‚¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì‹ ê·œ {len(new_articles)}ê±´ (ì´ {len(merged)}ê±´ ì €ì¥)")
                        else:
                            status.success(f"âœ… ê¸°ì‚¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ! í˜„ì¬ ì €ì¥ëœ ê±´ìˆ˜: {len(merged)}")
                    else:
                        # ê²°ê³¼ ì—†ìŒì´ì–´ë„ ì¡°ìš©íˆ ë‹¤ìŒ ë¼ìš´ë“œ(180ì´ˆ ë’¤)ë¡œ ë„˜ì–´ê°
                        status.info("â„¹ï¸ ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ì–´ìš”. ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ë‹¤ì‹œ ì‹œë„í• ê²Œ.")
            else:
                # API í‚¤ ì—†ìœ¼ë©´ ê·¸ëƒ¥ DBë§Œ ìœ ì§€ í‘œì‹œ
                if existing_db.empty:
                    status.warning("âš ï¸ API í‚¤ê°€ ì—†ê³ , ì €ì¥ëœ ë°ì´í„°ë„ ì—†ì–´ìš”. í‚¤ ì„¤ì • í›„ ë‹¤ì‹œ ì‹œë„í•´ì¤˜.")
                else:
                    status.info("â„¹ï¸ API í‚¤ê°€ ì—†ì–´ ì €ì¥ëœ ë°ì´í„°ë§Œ í‘œì‹œ ì¤‘ì´ì—ìš”.")

        except Exception as e:
            # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê°„ë‹¨íˆë§Œ (ì´ˆê¸° ë°ì´í„° ë¶ˆê°€ ë¬¸êµ¬ëŠ” ì œê±°)
            status.error(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

        # í”Œë˜ê·¸ ë¨¼ì € ë¦¬ì…‹í•˜ê³  ë‹¤ìŒ ë¼ìš´ë“œ íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹ 
        st.session_state.trigger_news_update = False
        st.session_state.next_refresh_at = time.time() + refresh_interval
        st.session_state.initial_loaded = True

    # ===== í™”ë©´ í‘œì‹œ (ì €ì¥ëœ ìµœì‹  ë°ì´í„° ê¸°ì¤€) =====
    st.markdown("---")
    # ì„¸ì…˜ì— ìµœì‹  ìˆ˜ì§‘ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (ì¦‰ì‹œ ë°˜ì˜)
    db = st.session_state.get('news_display_data', load_news_db())

    # ğŸ” ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
    if not db.empty and "ë‚ ì§œ" in db.columns:
        latest_article = db["ë‚ ì§œ"].iloc[0] if len(db) > 0 else "N/A"
        load_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        st.markdown(f'<div style="background: #1e1e1e; padding: 8px; border-radius: 4px; margin-bottom: 12px; font-size: 12px; color: #888;">'
                   f'ğŸ“Š DB ë¡œë“œ: {load_time} | ì´ {len(db)}ê±´ | ìµœì‹  ê¸°ì‚¬: {latest_article}</div>',
                   unsafe_allow_html=True)

    if db.empty:
        st.markdown('<p style="color: white;">ğŸ“° ì €ì¥ëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
        return

    # í‚¤ì›Œë“œ í•„í„° & ì •ë ¬
    pattern = "|".join(keywords)
    df_show = db[db["ê²€ìƒ‰í‚¤ì›Œë“œ"].astype(str).str.contains(pattern, case=False, na=False)].copy()
    if df_show.empty:
        st.markdown('<p style="color: white;">ğŸ“° POSCO ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.</p>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
        return

    df_show = df_show.sort_values(by="ë‚ ì§œ", ascending=False, na_position="last").reset_index(drop=True).head(50)
    if "URL" in df_show.columns:
        df_show["ë§¤ì²´ëª…"] = df_show["URL"].apply(_publisher_from_link)
    if "ë§¤ì²´ëª…" in df_show.columns:
        df_show["ë§¤ì²´ëª…"] = df_show.apply(
            lambda row: _publisher_from_link(row["URL"]) if pd.notna(row["URL"]) else row["ë§¤ì²´ëª…"], axis=1
        )

    ch1, ch2 = st.columns([3, 1])
    with ch1:
        st.markdown(f"**í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„ ê´€ë ¨ ê¸°ì‚¬: ìµœì‹  {len(df_show)}ê±´**")
    with ch2:
        st.download_button(
            "â¬‡ CSV ë‹¤ìš´ë¡œë“œ",
            df_show.to_csv(index=False).encode("utf-8"),
            file_name=f"posco_news_{datetime.now(timezone(timedelta(hours=9))).strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv",
            use_container_width=True
        )

    st.markdown('<p style="color: white; font-weight: 600; margin-bottom: 8px;">í‘œì‹œ ë°©ì‹</p>', unsafe_allow_html=True)
    
    # ë¼ë””ì˜¤ ë²„íŠ¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ í•˜ì–€ìƒ‰ìœ¼ë¡œ ë³€ê²½
    st.markdown("""
    <style>
    div[data-testid="stRadio"] > div {
        color: white !important;
    }
    div[data-testid="stRadio"] label {
        color: white !important;
    }
    div[data-testid="stRadio"] label > div {
        color: white !important;
    }
    div[data-testid="stRadio"] span {
        color: white !important;
    }
    div[data-testid="stRadio"] p {
        color: white !important;
    }
    .stRadio > label > div[data-testid="stMarkdownContainer"] > p {
        color: white !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    view = st.radio("", ["ì¹´ë“œí˜• ë·°", "í…Œì´ë¸” ë·°"], index=0, horizontal=True, key="news_view", label_visibility="collapsed")

    if view == "ì¹´ë“œí˜• ë·°":
        st.markdown("""
<style>
  /* ì»´íŒ©íŠ¸í•œ ë‰´ìŠ¤ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
  .news-card{
    background: #1E1E1E;
    border: 1px solid #2A2A2A;
    border-radius: 8px;
    padding: 16px 20px;
    margin-bottom: 12px;
    transition: all 0.2s ease;
    position: relative;
  }
  .news-card:hover{
    background: #252525;
    border-color: #3A3A3A;
  }

  /* ìƒë‹¨: ì¶œì²˜ íƒœê·¸ì™€ ë‚ ì§œ */
  .news-header{
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 10px;
  }
  .news-left{
    display: flex;
    align-items: center;
    gap: 6px;
  }
  .news-media{
    background: rgba(212,175,55,.2);
    color: #D4AF37;
    padding: 2px 8px;
    border-radius: 4px;
    font-size: 11px;
    font-weight: 600;
  }
  .news-key{
    background: rgba(135,206,235,.12);
    color: #87CEEB;
    padding: 2px 8px;
    border-radius: 4px;
    font-size: 11px;
    font-weight: 500;
  }
  .news-date{
    color: #AAAAAA;
    font-size: 12px;
    font-weight: 400;
  }

  /* ì¤‘ê°„: ì œëª©ê³¼ ìš”ì•½ */
  .news-title{
    color: #FFFFFF;
    font-size: 16px;
    font-weight: 600;
    line-height: 1.4;
    margin: 0 0 8px 0;
    word-break: break-word;
  }
  .news-summary{
    color: #CCCCCC;
    font-size: 13px;
    line-height: 1.5;
    margin: 0 0 12px 0;
    overflow: hidden;
    text-overflow: ellipsis;
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
  }

  /* í•˜ë‹¨: ë§í¬ì™€ ë²„íŠ¼ ì¢Œì¸¡ ì •ë ¬ */
  .news-footer{
    display: flex;
    justify-content: flex-start;
    align-items: center;
    gap: 8px;
  }
  .news-link a{
    color: #55b7ff;
    text-decoration: none;
    font-size: 13px;
    font-weight: 500;
    transition: all 0.2s ease;
    word-break: break-all;
  }
  .news-link a:hover{
    text-decoration: underline;
  }

  /* ë³´ê³ ì„œ ìƒì„± ë²„íŠ¼ - ì‘ê³  ì»´íŒ©íŠ¸í•˜ê²Œ */
  button[kind="secondary"] {
    height: auto !important;
    min-height: auto !important;
    padding: 4px 10px !important;
    font-size: 11px !important;
    font-weight: 500 !important;
    border-radius: 4px !important;
    transition: all 0.2s ease !important;
    background-color: rgba(255,255,255,0.08) !important;
    border: 1px solid rgba(255,255,255,0.12) !important;
  }
  button[kind="secondary"]:hover {
    background-color: #D4AF37 !important;
    border-color: #D4AF37 !important;
    color: #1E1E1E !important;
  }

  /* ìƒì„±ëœ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ */
  .report-container {
    background: rgba(255,255,255,0.05);
    border: 1px solid rgba(255,255,255,0.1);
    border-radius: 8px;
    padding: 15px;
    margin: 10px 0;
    color: #e0e0e0;
    font-family: monospace;
    white-space: pre-wrap;
    line-height: 1.5;
    font-size: 13px;
  }
</style>
""", unsafe_allow_html=True)

        for i, (_, row) in enumerate(df_show.iterrows()):
            title   = str(row.get("ê¸°ì‚¬ì œëª©", "")).strip('"')
            summary = str(row.get("ì£¼ìš”ê¸°ì‚¬ ìš”ì•½", ""))
            summary = (summary[:150] + "...") if len(summary) > 150 else summary
            media   = str(row.get("ë§¤ì²´ëª…", ""))
            keyword = str(row.get("ê²€ìƒ‰í‚¤ì›Œë“œ", ""))
            url = str(row.get("URL", ""))
            dt = str(row.get("ë‚ ì§œ", ""))
            sentiment = str(row.get("sentiment", "pos"))
            if " " in dt:
                d, t = dt.split(" ", 1)
                formatted_dt = f"ğŸ“… {d}  ğŸ• {t}"
            else:
                formatted_dt = f"ğŸ“… {dt}"

            # ê°ì„± dot ì„¤ì •
            if sentiment == "neg":
                sentiment_dot = '<span style="display:inline-block;width:8px;height:8px;border-radius:50%;background:#ef4444;margin-right:6px;vertical-align:middle;"></span>'
            else:
                sentiment_dot = '<span style="display:inline-block;width:8px;height:8px;border-radius:50%;background:#22c55e;margin-right:6px;vertical-align:middle;"></span>'

            # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ì•ˆì „í•œ ì œëª© ìƒì„±
            safe_name = re.sub(r'[^\wê°€-í£\s]', '', title)[:30]

            # ì»¨í…Œì´ë„ˆë¡œ ì¹´ë“œ ì „ì²´ ê°ì‹¸ê¸°
            with st.container():
                # ë‰´ìŠ¤ ì¹´ë“œ ë Œë”ë§
                st.markdown(f"""
                <div class="news-card">
                  <!-- ìƒë‹¨: ì¶œì²˜ íƒœê·¸ì™€ ë‚ ì§œ -->
                  <div class="news-header">
                    <div class="news-left">
                      {sentiment_dot}<span class="news-media">{media}</span>
                      <span class="news-key">#{keyword}</span>
                    </div>
                    <span class="news-date">{formatted_dt}</span>
                  </div>

                  <!-- ì œëª© (í•œ ì¤„, ë§ì¤„ì„) -->
                  <div class="news-title">{title}</div>

                  <!-- ìš”ì•½ (ìµœëŒ€ 2ì¤„) -->
                  <div class="news-summary">{summary}</div>

                  <!-- í•˜ë‹¨: ë§í¬ì™€ ë²„íŠ¼ ìš°ì¸¡ ì •ë ¬ -->
                  <div class="news-footer">
                    <div class="news-link">
                      <a href="{url}" target="_blank">ğŸ”— {url}</a>
                    </div>
                """, unsafe_allow_html=True)

                # ë³´ê³ ì„œ ìƒì„± ë²„íŠ¼
                report_key = f"report_btn_{i}"
                report_state_key = f"report_state_{i}"

                # ë³´ê³ ì„œ ìƒíƒœ ì´ˆê¸°í™”
                if report_state_key not in st.session_state:
                    st.session_state[report_state_key] = {"generated": False, "content": ""}

                # ë²„íŠ¼ë§Œ ë°°ì¹˜ (ìš°ì¸¡ ì •ë ¬)
                if st.button("ğŸ“„ ë³´ê³ ì„œ ìƒì„±", key=report_key, type="secondary"):
                    with st.spinner("ê¸°ì‚¬ ìš”ì•½ ìƒì„± ì¤‘..."):
                        try:
                            report_txt = make_kakao_report_from_url(
                                url, fallback_media=media, fallback_title=title, fallback_summary=summary
                            )
                            st.session_state[report_state_key]["generated"] = True
                            st.session_state[report_state_key]["content"] = report_txt
                            st.rerun()
                        except Exception as e:
                            backup_report = f"{url}\n\n{media} : \"{title}\"\n- í•µì‹¬ ìš”ì•½ì€ ì›ë¬¸ ì°¸ê³ \n- ìƒì„¸ ë‚´ìš©ì€ ë§í¬ í™•ì¸ í•„ìš”"
                            st.session_state[report_state_key]["generated"] = True
                            st.session_state[report_state_key]["content"] = backup_report
                            st.rerun()

                # ì¹´ë“œ ë‹«ê¸°
                st.markdown("""
                  </div>
                </div>
                """, unsafe_allow_html=True)
            
            # ë³´ê³ ì„œê°€ ìƒì„±ëœ ê²½ìš° í•˜ë‹¨ì— í‘œì‹œ
            if st.session_state[report_state_key]["generated"]:
                st.code(
                    st.session_state[report_state_key]["content"],
                    language=None
                )

    else:
        df_table = df_show[["ë‚ ì§œ","ë§¤ì²´ëª…","ê²€ìƒ‰í‚¤ì›Œë“œ","ê¸°ì‚¬ì œëª©","ì£¼ìš”ê¸°ì‚¬ ìš”ì•½","URL"]].rename(columns={
            "ë‚ ì§œ":"ğŸ“… ë°œí–‰ì¼ì‹œ","ë§¤ì²´ëª…":"ğŸ“° ì–¸ë¡ ì‚¬","ê²€ìƒ‰í‚¤ì›Œë“œ":"ğŸ” í‚¤ì›Œë“œ","ê¸°ì‚¬ì œëª©":"ğŸ“° ì œëª©","ì£¼ìš”ê¸°ì‚¬ ìš”ì•½":"ğŸ“ ìš”ì•½","URL":"ğŸ”— ë§í¬"
        })
        st.dataframe(
            df_table,
            use_container_width=True,
            height=min(700, 44 + max(len(df_table), 12)*35),
            column_config={
                "ğŸ”— ë§í¬": st.column_config.LinkColumn("ğŸ”— ë§í¬", help="ê¸°ì‚¬ ì›ë¬¸ ë§í¬", display_text="ê¸°ì‚¬ë³´ê¸°"),
                "ğŸ“ ìš”ì•½": st.column_config.TextColumn("ğŸ“ ìš”ì•½", help="ê¸°ì‚¬ ìš”ì•½", max_chars=100)
            }
        )

# ----------------------------- ë©”ì¸ ë£¨í‹´ -----------------------------
def main():
    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„í™œì„±í™” (GitHub Actionsì—ì„œ ìë™ ì•Œë¦¼ ì²˜ë¦¬)
    # if "background_scheduler_started" not in st.session_state:
    #     start_background_scheduler()
    #     st.session_state["background_scheduler_started"] = True

    # ì¸ì¦ ì²´í¬ - ì¸ì¦ë˜ì§€ ì•Šì€ ê²½ìš° ë¡œê·¸ì¸ í˜ì´ì§€ í‘œì‹œ
    if not check_authentication():
        show_login_page()
        return

    load_base_css()
    if "data_loaded" not in st.session_state:
        st.session_state["data_loaded"] = True

    active = set_active_menu_from_url()
    # ë©”ë‰´ ë³€ê²½ ê°ì§€ ë° ì…ë ¥ ìƒíƒœ ì´ˆê¸°í™” (ê°œì„ ë¨)
    if "current_menu" not in st.session_state:
        st.session_state.current_menu = active
    elif st.session_state.current_menu != active:
        # ë©”ë‰´ê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œ ëª¨ë“  ì…ë ¥ ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì •ë¦¬ (ìµœì í™”ë¨)
        prefixes = ('issue_', 'media_search_', 'contact_search_', 'history_search_',
                   'news_view', 'widget_', 'text_input_', 'text_area_', 'selectbox_')
        keys_to_clear = [k for k in st.session_state.keys() if k.startswith(prefixes)]

        # ì‚­ì œí•  í‚¤ê°€ ìˆì„ ë•Œë§Œ ì‚­ì œ ë° rerun (ì„±ëŠ¥ ìµœì í™”)
        if keys_to_clear:
            for key in keys_to_clear:
                st.session_state.pop(key, None)  # KeyError ë°©ì§€

            # ë©”ë‰´ ìƒíƒœ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
            st.session_state.current_menu = active
            st.rerun()
        else:
            # ì‚­ì œí•  í‚¤ê°€ ì—†ìœ¼ë©´ ë©”ë‰´ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (rerun ìƒëµ)
            st.session_state.current_menu = active
    
    render_top_nav(active)

    if active == "ë©”ì¸":
        render_main_page()
    elif active == "ì´ìŠˆë°œìƒë³´ê³  ìƒì„±":
        page_issue_report()
    elif active == "ì–¸ë¡ ì‚¬ ì •ë³´ ê²€ìƒ‰":
        page_media_search()
    elif active == "ë‹´ë‹¹ì ì •ë³´ ê²€ìƒ‰":
        page_contact_search()
    elif active == "ê¸°ì¡´ëŒ€ì‘ì´ë ¥ ê²€ìƒ‰":
        page_history_search()
    elif active == "ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§":
        page_news_monitor()
    else:
        # ì˜ëª»ëœ íŒŒë¼ë¯¸í„°ë©´ ë©”ì¸ìœ¼ë¡œ ë³´ëƒ„
        st.query_params["menu"] = "ë©”ì¸"
        st.rerun()

    # ë²„ì „ ì •ë³´ í‘œì‹œ (í•˜ë‹¨, ì‘ê²Œ)
    st.markdown(
        f'<div style="text-align: center; color: rgba(255,255,255,0.3); font-size: 11px; margin-top: 40px; padding-bottom: 20px;">'
        f'v2.0.1 | 2025-11-28 | ë¡œê·¸ì¸/íƒ€ì´ë¨¸ ìˆ˜ì •'
        f'</div>',
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()